[
  {
    "objectID": "Bayesian-Stats.html",
    "href": "Bayesian-Stats.html",
    "title": "5  Bayesian Statistics",
    "section": "",
    "text": "5.0.1 What is a prior really?\nBayesianism has been on the to do list for awhile since well they make the prettiest plots. More importantly we get more coherent measures of uncertainty and are closer to how we actually think of the world. There are lots of broad applications for Bayesianism in the private industry. The betting market for sports and how we think about teams is effectively Bayesian. We incorporate a lot of information about the observable parts of a team like schedule, roster talent, coaching, and guesses about the probablity that the roster will stay healthy. These are all priors we have about things that relate to the success of the team going into the season. As the season progresses and we get data our prior will update. In the case of the Saints this year our prior updated to make them a good team after one game moving our guess about the number of games they would win this year to probably more than their projected win total. With the second win this may have not moved our prior much at all. However, as attrition set in then our prior shifted a little bit back toward the pre-season total so on and so forth.\nBayesianism a formalization of this process by encoding our beliefs in probability distributions. Our prior is really just our belief about the distribution and plausible range of values for that variable before ever seeing the data. So if we were to set a prior for a coin we would set the prior as somthing kind of loosy goosy as this\nSo in this case we are just stating that the probability of heads is about 50 percent over a thousand trials assuming a fair coin. However, lets say that we know the coin is biased in a known way that makes it come up heads 61% of the time. We could then set our prior that it willl come up heads as this\nThis is not neccessarily all that unique from a standard null hypothesis which we covered in the opening “chapter” of this book. It is the combination of the prior and the posterior which makes the Bayesian machine go brrr.\nThe posterior can we operationalized along these lines\n\\[\nPosterior = \\frac{\\text{Probability of Data} \\times Prior}{\\text{Mean probability of Data}}\n\\]\nWhere we are averaging the probability of the prior. If we derive this out more formally we will see that the posterior is actually proportional to the prodct of the prior and the probability of the data. We can overcome a bad prior given an infinite amount of data, but this process will be inefficient and critically we will never reach infinite data. Priors in Bayesian inference are important computationally as well as substantively.\nSetting a prior is one of the hardest things in Bayesian statistics and the subject a large and rich part of the literature. I think one really important thing to adjudicate is what a prior really is. That way we can reinforce the importance of setting a good one, what to check, and how to check it.\nAccording to Gelman et al. (2013) we can conceptualize the prior distribution along two different lines. There is the population interpretation. This is a little frequentisty but is definitely helpful. Lets say that we have some pseudo population of parameters that the candidate parameter \\(\\beta\\) is drawn from. For simplicity sake lets draw \\(\\beta\\) from as standard normal where we overlay the overall distribution\nlibrary(tidyverse)\n\nsim_data = \\(beta_val = 0, beta_variance = 1, n = 100){\n  \n  sim_dat = tibble(beta_values = rnorm(n, mean = beta_val, sd = beta_variance),\n                   beta_mean = rep(beta_val, length.out = n),\n                   beta_variance = rep(beta_variance, length.out = n))\n\n  return(sim_dat)\n    \n}\n\npop_sims = map(c(5, 10, 15), \\(x) sim_data(beta_val = x)) |&gt;\n    list_rbind() \n\nwide_version = pop_sims |&gt; \n    mutate(id = row_number()) |&gt;\n    pivot_wider(names_from = beta_mean, values_from = beta_values, names_glue = 'mean_{beta_mean}') \n\nplot_dat = wide_version |&gt;\n    mutate(pop_total = pop_sims$beta_values) |&gt;\n    pivot_longer(cols = mean_5:mean_15) |&gt;\n    mutate(nice_labs = as.factor(str_to_title(str_replace(name, '_', \" \"))), \n           nice_labs  = fct_relevel(nice_labs, 'Mean 5', 'Mean 10', 'Mean 15'))\n\n\nggplot(plot_dat) +\ngeom_density(mapping = aes(x = value, y = after_stat(ncount),   fill = nice_labs), \n             stat = \"bin\",  size = 0.5,\n             alpha = 0.7) + \ngeom_density(mapping = aes(x = pop_total, y = after_stat(ncount)), \n                 alpha = 0.9,\n                 color = \"gray30\", size = 0.6, \n             stat = \"bin\",\n             direction = \"mid\") +\n   facet_wrap(vars(nice_labs)) +\n   MetBrewer::scale_fill_met_d(name = 'Lakota') +\n   theme_minimal() +\n   labs(fill = NULL, y = 'Scaled Count') +\n   theme(legend.position = 'none')\nSo in the population interpretation of priors we have kind of weird population by design. Depending on what ‘draw’ we get our prior could be N(5,1) or it could be N(10,1) where plausible values center around 5 or 10 with a standard deviation of 1. In this setting we are thinking about given the data what is a reasonable set of values that we would expect to see. In this setting we are kind of explicitly using a bit of frequentist logic to bridge the gap. However, instead of assuming that all values between 5-15 are equally plausible we are expliciltly stating that the mass of the distribution will be around some value and the amount of variation that we will have. This could be useful if we have a huge amount of experiments or results banked and we can imagine our plausible beta values as drawn from some distribution of the experiments.\nIn the state of knowledge interpretation of priors we are still using our subject matter expertise of the phenomena, but there may not be a good population to ground our priors on. Say we are launching a completly new product or expanding our product into a new market. We may have some idea about what we are likely to see but we don’t have the same reference population that we can draw from, but we have at least some idea of what our expectation should be.\nAn important thing to note about Bayes Rule is that we can think of Bayes rules along these lines\n\\[\nPosterior = \\frac{\\overbrace{P(A)}^{\\text{Prior}} \\times \\overbrace{\\text{Probability of Data}}^{Likelihood}}{\\text{Average Probability of Data}}\n\\]\nWhat this means is that effectively the posterior is a bit of a compromise between our data and our beliefs as the size of the data get bigger our prior will have less of an influence on our posterior distribution, but it will never completely evaporate.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "1  Data Viz with Matplot",
    "section": "",
    "text": "1.1 Initializing a plot\nIt is time to learn the grammar of matplot. Since it is the most popular library and you are going to use data viz a fair amount. In some ways matplot lib is like ggplot in a lot of other ways it very much is not. A better mental mapping in some cases is how you would do things in base R.\nimport polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\n# Load dataset into a Polars DataFrame\npengs = pl.from_pandas(load_penguins()).drop_nulls()\nInstead of simply mapping what you know onto this you should learn it for what it is. There are three layers to matplotlib that makes it quirky. Most of this is going to be taken from scientific visualization with matplotlib. In this case I am directly ripping this script from the book.\nCode\nfrom matplotlib.ticker import AutoMinorLocator, MultipleLocator, FuncFormatter\n\nnp.random.seed(123)\n\nX = np.linspace(0.5, 3.5, 100)\nY1 = 3 + np.cos(X)\nY2 = 1 + np.cos(1 + X / 0.75) / 2\nY3 = np.random.uniform(Y1, Y2, len(X))\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(1, 1, 1, aspect=1)\n\n\ndef minor_tick(x, pos):\n    if not x % 1.0:\n        return \"\"\n    return \"%.2f\" % x\n\n\nax.xaxis.set_major_locator(MultipleLocator(1.000))\nax.xaxis.set_minor_locator(AutoMinorLocator(4))\nax.yaxis.set_major_locator(MultipleLocator(1.000))\nax.yaxis.set_minor_locator(AutoMinorLocator(4))\nax.xaxis.set_minor_formatter(FuncFormatter(minor_tick))\n\nax.set_xlim(0, 4)\n\n\n(0.0, 4.0)\n\n\nCode\nax.set_ylim(0, 4)\n\n\n(0.0, 4.0)\n\n\nCode\n\nax.tick_params(which=\"major\", width=1.0)\nax.tick_params(which=\"major\", length=10)\nax.tick_params(which=\"minor\", width=1.0, labelsize=10)\nax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n\nax.grid(linestyle=\"--\", linewidth=0.5, color=\".25\", zorder=-10)\n\nax.plot(X, Y1, c=(0.25, 0.25, 1.00), lw=2, label=\"Blue signal\", zorder=10)\nax.plot(X, Y2, c=(1.00, 0.25, 0.25), lw=2, label=\"Red signal\")\nax.plot(X, Y3, linewidth=0, marker=\"o\", markerfacecolor=\"w\", markeredgecolor=\"k\")\n\nax.set_title(\"Anatomy of a figure. Part of Figure\", fontsize=20, verticalalignment=\"bottom\")\nax.set_xlabel(\"X axis label. Part of Axis\")\nax.set_ylabel(\"Y axis label. Part of Axis\")\n\nax.legend()\n\n\ndef circle(x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n\n    circle = Circle(\n        (x, y),\n        radius,\n        clip_on=False,\n        zorder=10,\n        linewidth=1,\n        edgecolor=\"black\",\n        facecolor=(0, 0, 0, 0.0125),\n        path_effects=[withStroke(linewidth=5, foreground=\"w\")],\n    )\n    ax.add_artist(circle)\n\n\ndef text(x, y, text):\n    ax.text(\n        x,\n        y,\n        text,\n        backgroundcolor=\"white\",\n        # fontname=\"Yanone Kaffeesatz\", fontsize=\"large\",\n        ha=\"center\",\n        va=\"top\",\n        weight=\"regular\",\n        color=\"#000099\",\n    )\n\n\n# Minor tick\ncircle(0.50, -0.10)\ntext(0.50, -0.32, \"Minor tick label\")\n\n# Major tick\ncircle(-0.03, 4.00)\ntext(0.03, 3.80, \"Major tick\")\n\n# Minor tick\ncircle(0.00, 3.50)\ntext(0.00, 3.30, \"Minor tick\")\n\n# Major tick label\ncircle(-0.15, 3.00)\ntext(-0.15, 2.80, \"Major tick label\")\n\n# X Label\ncircle(1.80, -0.27)\ntext(1.80, -0.45, \"X axis label\")\n\n# Y Label\ncircle(-0.27, 1.80)\ntext(-0.27, 1.6, \"Y axis label\")\n\n# Title\ncircle(1.60, 4.13)\ntext(1.60, 3.93, \"Title\")\n\n# Blue plot\ncircle(1.75, 2.80)\ntext(1.75, 2.60, \"Line\\n(line plot)\")\n\n# Red plot\ncircle(1.20, 0.60)\ntext(1.20, 0.40, \"Line\\n(line plot)\")\n\n# Scatter plot\ncircle(3.20, 1.75)\ntext(3.20, 1.55, \"Markers\\n(scatter plot)\")\n\n# Grid\ncircle(3.00, 3.00)\ntext(3.00, 2.80, \"Grid\")\n\n# Legend\ncircle(3.70, 3.80)\ntext(3.70, 3.60, \"Legend\")\n\n# Axes\ncircle(0.5, 0.5)\ntext(0.5, 0.3, \"Axes\")\n\n# Figure\ncircle(-0.3, 0.65)\ntext(-0.3, 0.45, \"Figure\")\n\ncolor = \"#000099\"\nax.annotate(\n    \"Spines\",\n    xy=(4.0, 0.35),\n    xytext=(3.3, 0.5),\n    color=color,\n    weight=\"regular\",  # fontsize=\"large\", fontname=\"Yanone Kaffeesatz\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3\", color=color),\n)\n\nax.annotate(\n    \"\",\n    xy=(3.15, 0.0),\n    xytext=(3.45, 0.45),\n    color=color,\n    weight=\"regular\",  # fontsize=\"large\", fontname=\"Yanone Kaffeesatz\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3\", color=color),\n)\nIt is important to note that there are several areas to that differ from ggplot. The logic differs in a lot of ways so there is no clean 1:1 translation of matplotlib and ggplot. So it worth adjudicating some of these differences",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Viz with Matplot</span>"
    ]
  },
  {
    "objectID": "data-viz.html#initializing-a-plot",
    "href": "data-viz.html#initializing-a-plot",
    "title": "1  Data Viz with Matplot",
    "section": "",
    "text": "Figure: In way this is just bringing the plot into existince. Kind of like when you just do ggplot. This also includes the titles, fontface of the titles\nAxes: This corresponds to the area where the data will be rendered. Sometimes refered to as the subplot. This controls the spines basically the area that is around the plot. This also includes the ticks and the actual plotting of the data.\nAxis: For some reason this is separated but these are really just the equivelent of the scales argument in ggplot",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Viz with Matplot</span>"
    ]
  },
  {
    "objectID": "data-viz.html#plotting",
    "href": "data-viz.html#plotting",
    "title": "1  Data Viz with Matplot",
    "section": "1.2 Plotting",
    "text": "1.2 Plotting\nTo initialize a plot we can just do\n\n\nfig,ax = plt.subplots(figsize=(10,6))\n\n\n\n\n\n\n\n\nfig is just a matplotlib object. We don’t neccessarily ‘add’ anything in the same way we do with ggplot. This kind of boils down to differences in R and Python. While ggplot is its own class in R and geoms have to be registered as class methods. Spirtually these all follow the functional programming paradigm. While matplotlib follows a more object oriented paradigm.\nSo instead of adding layers like below\n\nlibrary(ggplot2)\n\npengs = palmerpenguins::penguins\n\nggplot(pengs, aes(x = body_mass_g)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nWe are modifying each of the underlying objects using class methods.\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\n\n\n\n\n\n\n\n\nNotice that one of the key differences in the two plots is that when making the plot we are operating on the axis class which has the plotting ‘geometries’. Whereas in ggplot geom_histogram is a function that will create the bins, plot the data, then go and grab the labels, and then place them on the appropraite axis. So in matplot lib we would need to do somthing to this effect.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\n\n\n\n\n\n\n\n\nOne thing that comes up frequently if you are doing things itertively is that if we wanted to modidfy a plot matplot has the tendency of overlaying things on top of plots that you have previously created. We actually have to close out the plot since we interact with each of the indivual object itertively especially when working in a script.\nWe can clear/close a plot doing one of two things\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\nfig.clf()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\nplt.close(fig)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Viz with Matplot</span>"
    ]
  },
  {
    "objectID": "data-viz.html#other-elements-of-aes",
    "href": "data-viz.html#other-elements-of-aes",
    "title": "1  Data Viz with Matplot",
    "section": "1.3 Other elements of aes",
    "text": "1.3 Other elements of aes\nLets take a very simple ggplot.\n\nggplot(pengs, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n    geom_point()\n\n\n\n\n\n\n\n\naes is kind of just a function. So when aes is not empty it will look through the unique values of the column and then loop over the values and plot them. Whereas in matplotlib we have to explicitly do this. Seaborn is a bit of a higher level api. Which will abstract some of this work away for you. In matplotlib we have to do this ourselves\n\n\nCode\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y=spec_data[\"body_mass_g\"],\n        c=color,\n        label=species,\n    )\n\nax.set_xlabel(\"Flipper length mm\")\nax.set_ylabel(\"Body Mass g\")\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n\n\n\n\n\n\n\n\nThis would for any number of plot types\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.hist(\n        x=spec_data[\"flipper_length_mm\"],\n        color=color,\n        label=species,\n        alpha=0.5,\n        edgecolor=\"white\",\n    )\n\nax.legend()\nax.set_xlabel(\"Flipper Length mm\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Viz with Matplot</span>"
    ]
  },
  {
    "objectID": "data-viz.html#more-complex-plots",
    "href": "data-viz.html#more-complex-plots",
    "title": "1  Data Viz with Matplot",
    "section": "1.4 More ‘complex plots’",
    "text": "1.4 More ‘complex plots’\nMatplot has a lot of nice things when we start to need more bespoke transformations to the plot areas. One of the things that it lacks when compared to seaborne or ggplot. So lets take a simple plot that we tend to make a lot in demos or just as a data exploration\n\nggplot(pengs, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n    geom_point() +\n    geom_smooth()\n\n\n\n\n\n\n\n\nThe points we have already covered how to make but there are no built in class methods to matplot lib for a lowess smoother. Which kind of feels like a silly choice. To do this we first need to define a function. We then have to do the same thing\n\n\nCode\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n\ndef bootstrap_lowess(x, y, frac=0.75, it=3, n_boot=100, alpha=0.05):\n    \"\"\"\n    Bootstraps LOWESS smoothing to estimate confidence intervals.\n\n    Parameters:\n    - x, y: Input data (must be numpy arrays)\n    - frac: Smoothing span (same as ggplot default)\n    - it: Number of robustifying iterations\n    - n_boot: Number of bootstrap resamples\n    - alpha: Confidence level (default 95%)\n\n    Returns:\n    - x_sorted: Sorted x values\n    - ysm: Smoothed LOWESS estimate\n    - y_lower, y_upper: Confidence interval bounds\n    \"\"\"\n    # Sort x and y to avoid issues\n    sorted_indices = np.argsort(x)\n    x_sorted, y_sorted = x[sorted_indices], y[sorted_indices]\n\n    # Compute the original LOWESS estimate\n    smoothed = lowess(y_sorted, x_sorted, frac=frac, it=it)\n    ysm = smoothed[:, 1]  # Extract smoothed values\n\n    # Bootstrap resampling\n    y_boot = np.zeros((n_boot, len(x_sorted)))\n    for i in range(n_boot):\n        resample_idx = np.random.choice(\n            len(x_sorted), len(x_sorted), replace=True)\n        y_resampled = y_sorted[resample_idx]\n        x_resampled = x_sorted[resample_idx]\n        smoothed_boot = lowess(y_resampled, x_resampled, frac=frac, it=it)\n        y_boot[i, :] = np.interp(\n            x_sorted, smoothed_boot[:, 0], smoothed_boot[:, 1])\n\n    # Compute confidence intervals\n    lower_bound = np.percentile(y_boot, 100 * (alpha / 2), axis=0)\n    upper_bound = np.percentile(y_boot, 100 * (1 - alpha / 2), axis=0)\n\n    return x_sorted, ysm, lower_bound, upper_bound\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    x = spec_data[\"flipper_length_mm\"].to_numpy()\n    y = spec_data[\"body_mass_g\"].to_numpy()\n\n    # Sort data to avoid issues in plot and LOWESS\n    sorted_indices = np.argsort(x)\n    x_sorted, y_sorted = x[sorted_indices], y[sorted_indices]\n    x_sorted, ysm, y_lower, y_upper = bootstrap_lowess(\n        x, y, frac=0.75, it=3, n_boot=100\n    )\n\n    # Scatter plot\n    ax.scatter(x, y, c=color, label=species, alpha=0.6)\n\n    # Smoothed LOWESS line\n    ax.plot(x_sorted, ysm, color=color, linewidth=2)\n\n    # Confidence interval shading\n    ax.fill_between(x_sorted, y_lower, y_upper, color=color, alpha=0.2)\n\n\nax.set_xlabel(\"Flipper length mm\")\nax.set_ylabel(\"Body Mass g\")\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n\n\n\n\n\n\n\nSimilarly we would need to define a kernel density estimator however it is going to be a whole lot easier to let seaborn take the wheel in some cases\n\nimport seaborn as sns \n\nfig,ax = plt.subplots()\n\nsns.kdeplot(pengs, x = 'flipper_length_mm', fill = 'species', hue = 'species')\n\n\n\n\n\n\n\n\nIn the same way ols and lowess plots get a bit easier and you can maintain some of the flexibility\n\n\nfig, ax = plt.subplots(figsize = (10,6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    sns.regplot(spec_data, x = 'flipper_length_mm', y = 'body_mass_g', lowess = True, scatter = True,\n    ax = ax, color = color, label = species)\n\nax.set_xlabel('Flipper Length(mm)')\nax.set_ylabel('Body Mass(g)')\nax.legend()\n\n\n\n\n\n\n\n\nSimilarly you could do the same thing for the kernel density estimator if you are truly wedded to the matplot lib default style then you could just loop over like this\n\nfig, ax = plt.subplots(figsize = (10,6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    sns.kdeplot(spec_data, x = 'flipper_length_mm',\n    ax = ax, fill = True, color = color, label = species)\n\nax.set_xlabel('Flipper Length(mm)')\nax.legend()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Viz with Matplot</span>"
    ]
  },
  {
    "objectID": "data-viz.html#small-multiples",
    "href": "data-viz.html#small-multiples",
    "title": "1  Data Viz with Matplot",
    "section": "1.5 Small Multiples",
    "text": "1.5 Small Multiples\nSometimes we need small multiples to show make things look nice or not make our plots to busy. In ggplot this is fairly trivial because we don’t neccessarily need super fine tuned control over the layout. Sometimes we just need two multiples like this.\n\npengs = na.omit(pengs)\n\nggplot(pengs, aes(x = body_mass_g, fill = species)) +\n    geom_histogram() +\n    facet_wrap(vars(sex))\n\n\n\n\n\n\n\n\nIn matplotlib this is where our figure object comes in handy. We are effectively going to tell the figures that we need another plot in the same way that we use par in base R plots. One of the things that really makes matplotlib standout from ggplot is just how finetuned you can make the plots.\n\n\n\npengs = pengs.filter(pl.col('sex').is_not_null())\nsexs = pengs.unique(subset = 'sex')['sex']\n\nfig, axes = plt.subplots(nrows=1, ncols=len(sexs), figsize=(10,5))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\n\n\n\n\nfor i, sex in enumerate(sexs):\n    ax = axes[i]\n    ax.set_title(f\"Sex: {sex}\")\n    \n    for color, species_name in zip(colors, species):\n        # Filter data for the current species and sex\n        spec_data = pengs.filter((pl.col('species') == species_name) & (pl.col('sex') == sex))\n        ax.hist(spec_data['body_mass_g'], bins=30, color=color,\n                edgecolor='white', alpha=0.7, label=species_name)\n    \n    ax.set_xlabel('Body Mass (g)')\n    ax.legend()\n\n\n\n\n\n\n\n\nSimilarly we can do the same thing with our loess plots\n\npengs = pengs.filter(pl.col('sex').is_not_null())\nsexs = pengs.unique(subset = 'sex')['sex']\n\nfig, axes = plt.subplots(nrows=1, ncols=len(sexs), figsize=(10,5))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor i, sex in enumerate(sexs):\n    ax = axes[i]\n    ax.set_title(f\"Sex: {sex}\")\n    \n    for color, species_name in zip(colors, species):\n        # Filter data for the current species and sex\n        spec_data = pengs.filter((pl.col('species') == species_name) & (pl.col('sex') == sex))\n        sns.regplot(spec_data, x = 'flipper_length_mm', y = 'body_mass_g', lowess = True, scatter = True,\n        ax = ax, color = color, label = species_name)\n    \n    ax.set_xlabel('Body Mass (g)')\n    ax.legend()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Viz with Matplot</span>"
    ]
  },
  {
    "objectID": "data-viz.html#customizing-appearences",
    "href": "data-viz.html#customizing-appearences",
    "title": "1  Data Viz with Matplot",
    "section": "1.6 Customizing Appearences",
    "text": "1.6 Customizing Appearences\nI think generally nobody that does data visualization like the defaults. So adding themes or customizing themes are important. Lets just work with a very simple plot and just keep making it nice.\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y = spec_data['body_mass_g'],\n        color=color,\n        label=species,\n        alpha=0.5\n    )\n\nax.legend()\nax.set_xlabel(\"Flipper Length mm\")    \n\n\n\n\n\n\n\n\nI have a little bit of a bespoke ggplot theme which is really just aligning the axis labels and customizing the fontface to be Assitant.\n\n\nplt.rcParams.update({\n    'font.family': 'Assistant'\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y = spec_data['body_mass_g'],\n        color=color,\n        label=species,\n        alpha=0.5\n    )\n\nax.legend(frameon = False, loc = 'center left', bbox_to_anchor=(1, 0.5))\nax.set_xlabel(\"Flipper Length mm\", loc = 'left', labelpad = 15)   \nax.set_ylabel('Body Mass(g)', labelpad = 15, loc = 'top')\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.tick_params(axis = 'x', which = 'both', length = 0)\nax.tick_params(axis = 'y', which = 'both', length = 0)\nax.grid(True, linestyle = 'solid', which = 'major', linewidth = 0.4)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Viz with Matplot</span>"
    ]
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "2  SQL",
    "section": "",
    "text": "2.1 Setup\nIn this chapter we will briefly go over the basics of SQL and also do some SQL practice since I have a very limited way to practice.\nSince I am going on the non-academic job market it is high time I learned SQL. I have tried lots of amazing resources but find it hard for me to navigate between notes and various and learning SQL since they are just familiar enough to trip me up and lots of them send you off to various editors. This blog post will serve as my notes and hopefully as a resource for not myself. The general idea is I am just going to work through R4DS and the various dplyr verbs. Then move onto some more advanced SQL stuff like window functions and what not.\nFor the majority of this palmerpenguins dataset not because you really need to use SQL for a dataset this small but copying over the nyc-taxi dataset is incredibly annoying for blogging purposes.\nlibrary(DBI)\nlibrary(arrow)\nlibrary(tinytable)\nlibrary(dbplyr)\nlibrary(tidyverse)\n\npengs = palmerpenguins::penguins\n\ncon =  src_memdb()\n\npengs = copy_to(con, pengs,\n       overwrite = TRUE)\nWe are going to go back and forth using dbplyr and SQL to query the dataset. What impressed me throughout this process was how seamless dbplyr works with dplyr verbs work. With the exception of some string functions it can work as a drop in replacement for SQL. What really helped throughout this process was writing out my queries and using show_query.\npengs |&gt;\n    select(species) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT `species`\nFROM `pengs`\nWhich will give us a SQL query. Obviously this is a pretty simple query but as we get more and more complex this is going to be helpful. For the most part show_query outputs the right query but can be a little bit difficult to debug because of the limitations of showing things in the R console.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#select",
    "href": "SQL.html#select",
    "title": "2  SQL",
    "section": "3.1 Select",
    "text": "3.1 Select\nOne convention in SQL which I don’t really get but is a thing is that functions are defined using all caps. Luckily for us the SQL and dplyr versions are pretty much the same one is just shouty. If we wanted all the columns like we may when we are importing the dataset for the first time we are just going to do SELECT * FROM taxis. There is really not like a perfect equivalent in R except for maybe head. But even then it is not a perfect one to one.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#r",
    "href": "SQL.html#r",
    "title": "2  SQL",
    "section": "3.2 R",
    "text": "3.2 R\n\nhead(pengs)\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#sql",
    "href": "SQL.html#sql",
    "title": "2  SQL",
    "section": "3.3 SQL",
    "text": "3.3 SQL\n\ntbl(con, sql(\"SELECT * FROM pengs\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#filter",
    "href": "SQL.html#filter",
    "title": "2  SQL",
    "section": "3.4 Filter",
    "text": "3.4 Filter\nThe first major difference syntactically between dplyr and SQL is with filter statements aka WHERE statements in SQL. So let’s say we want only penguins that are Adelie penguins.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species == 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nBecomes.\n\n\n\ntbl(con,sql( \"\n    SELECT * from pengs\n    WHERE species = 'Adelie'\n\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\nSome flavors of SQL make you end lines with ‘;’\nAs dplyr users will notice the way we specified the equality position uses the = instead of ==. This is going to come up a lot. The same thing goes for negation operations.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species != 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\ntbl(con, sql(\"SELECT * from pengs \n             WHERE NOT species = 'Adelie'\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nIf we want multiple conditions in our where statements instead of | or &/, we actually just use the words or and and\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species == 'Chinstrap' | species == 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nbecomes\n\n\n\ntbl(con, sql(\"SELECT * from pengs \n            WHERE species = 'Adelie' OR species = 'Chinstrap'\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nYou could easily sub in AND but that feels a bit excessive to continue this process for each possible combination. One thing that I do all the time is use sets to subset my data.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species %in% c('Chinstrap', \"Gentoo\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nBecomes\n\n\n\ntbl(con, sql(\"SELECT * from pengs\n            WHERE species IN ('Chinstrap', 'Gentoo')\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nin this case we define a set in a similar way. If we wanted to negate this statement all we would do is\n\ntbl(con, sql(\"SELECT * from pengs\n            WHERE NOT species IN ('Chinstrap', 'Gentoo')\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nLets say we want to find penguins that are less than the average body mass in R this is fairly straightforward\n\npengs |&gt;\n    filter(body_mass_g &lt; mean(body_mass_g, na.rm = TRUE))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nHowever when we do this in some flavor of SQL it is not as straightforward. These are aggregation functions that where can’t handle because thats not its job. So if we did\n\ntbl(con, \"SELECT * from pengs WHERE body_mass_g &lt; AVG(body_mass_g)\")\n\nError in `db_query_fields.DBIConnection()`:\n! Can't query fields.\nℹ Using SQL: SELECT * FROM `SELECT * from pengs WHERE body_mass_g &lt;\n  AVG(body_mass_g)` AS `q01` WHERE (0 = 1)\nCaused by error:\n! no such table: SELECT * from pengs WHERE body_mass_g &lt; AVG(body_mass_g)\n\n\nWe get an error. If we wanted to use aggregation functions we have to change how we do this\n\npengs |&gt;\n    filter(body_mass_g &lt; mean(body_mass_g, na.rm = TRUE)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `island`,\n  `bill_length_mm`,\n  `bill_depth_mm`,\n  `flipper_length_mm`,\n  `body_mass_g`,\n  `sex`,\n  `year`\nFROM (\n  SELECT `pengs`.*, AVG(`body_mass_g`) OVER () AS `col01`\n  FROM `pengs`\n) AS `q01`\nWHERE (`body_mass_g` &lt; `col01`)\n\n\nWhat is this OVER thing? OVER in SQL is a window function. There is a more technical way to explain this but heuristically when we pass AVG to WHERE we are effectively doing this. So there is not really anything to compare it too.\n\npengs |&gt;\n    summarise(mean(body_mass_g, na.rm = TRUE))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.47.1 [:memory:]\n  `mean(body_mass_g, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             4202.\n\n\nIf we wanted to filter penguins that are less than the average body mass we have to prevent this aggregation process by creating a column and then creating a less than statement like this\n\ntbl(con, sql(\"SELECT * FROM(\n              SELECT pengs .*, AVG(body_mass_g) OVER () AS avg\n               FROM pengs)\n              WHERE (body_mass_g &lt; avg)\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg &lt;dbl&gt;\n\n\nIt is a little clunky but the tl;dr is that we basically have two FROM statements so if we wanted all penguins between the minimum and the average we could do\n\nRSQL\n\n\n\npalmerpenguins::penguins |&gt;\n    filter(between(body_mass_g, left = min(body_mass_g, na.rm = TRUE), right = mean(body_mass_g, na.rm = TRUE)))\n\n# A tibble: 193 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ 183 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\ntbl(con, sql(\"SELECT * FROM(\n             SELECT pengs .*, AVG(body_mass_g) OVER() AS avg, MIN(body_mass_g) OVER() AS min\n            FROM pengs)\n            WHERE body_mass_g BETWEEN min AND avg\"))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg &lt;dbl&gt;, min &lt;int&gt;\n\n\n\n\n\nIf you notice in all our examples, we have lots and lots of missing values. This is one of the most common tasks in like any data science task. Let’s say that we can safely ignore the missing valus. In R we have a lot of options whether we are using filter or drop_na from tidyr. However, in SQL missing values are usually represented by NULL\n\ntbl(con, sql(\"SELECt * FROM pengs \n                WHERE NOT sex IS NULL\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#rename",
    "href": "SQL.html#rename",
    "title": "2  SQL",
    "section": "3.5 Rename",
    "text": "3.5 Rename\nThe AS function is kind the work horse for the next few sections. The naming convention differs a little bit so instead of new_name = old_name we do SELECT old_name as new_name\n\ntbl(con, sql(\"SELECT species AS kinds_of_penguins\n          FROM pengs\"))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.47.1 [:memory:]\n   kinds_of_penguins\n   &lt;chr&gt;            \n 1 Adelie           \n 2 Adelie           \n 3 Adelie           \n 4 Adelie           \n 5 Adelie           \n 6 Adelie           \n 7 Adelie           \n 8 Adelie           \n 9 Adelie           \n10 Adelie           \n# ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#mutate",
    "href": "SQL.html#mutate",
    "title": "2  SQL",
    "section": "3.6 Mutate",
    "text": "3.6 Mutate\nAs lots of things go we need to be able to create our own variables. So to do this in R we do this\n\npengs |&gt;\n    mutate(sqr_body_mass = body_mass_g^2)\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, sqr_body_mass &lt;dbl&gt;\n\n\nIn SQL to get the equivalent statement we use SELECT transformation AS new_var_name when we need to do things that are not in the dataset. So we basically need to define the column before we do anything.\n\ntbl(con, sql(\"SELECT pengs .*, POWER(body_mass_g,2) AS sqr_body_mass\n            FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, sqr_body_mass &lt;dbl&gt;\n\n\nSo if we needed wanted to make a ratio of bill depth to bill length we would do\n\ntbl(con, sql(\"SELECT pengs .*, bill_depth_mm/bill_length_mm AS ratio \n            FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, ratio &lt;dbl&gt;\n\n\nA very important thing we do all the time is generate indicator variables for treatment status gender etc. Oddly enough if we peep the output of show query we see a familiar face!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#r-7",
    "href": "SQL.html#r-7",
    "title": "2  SQL",
    "section": "3.7 R",
    "text": "3.7 R\n\npengs |&gt;\n    mutate(male = ifelse(sex == 'Male', 1, 0)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `pengs`.*,\n  CASE WHEN (`sex` = 'Male') THEN 1.0 WHEN NOT (`sex` = 'Male') THEN 0.0 END AS `male`\nFROM `pengs`\n\n\nSo to make an indicator variable we would just do\n\ntbl(con, sql(\"SELECT pengs.*, CASE WHEN (sex = 'male') THEN 1.0 WHEN not (sex = 'male') THEN 0.0 END AS male\n             FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, male &lt;dbl&gt;\n\n\nLet’s combine our window functions with our friend case_when\n\ntbl(con, sql(\"SELECT * FROM(SELECT pengs .*,\n           AVG(body_mass_g) AS avg, MIN(body_mass_g) AS min, MAX(body_mass_g) AS max,\n            CASE WHEN (body_mass_g = min) THEN 'This penguins is small' WHEN (body_mass_g = avg) THEN 'This is an average sized penguin' WHEN (body_mass_g = max) THEN 'this is a really big penguin' END AS note \n            FROM pengs)\"))\n\nI will spare you the long output of the error message. But needless to say this was wrong. If we translate what I was trying to do into dplyr we get this\n\npengs |&gt;\n    mutate(note = case_when(\n            body_mass_g == min(body_mass_g) ~ 'This is a small peng',\n            body_mass_g == mean(body_mass_g) ~ 'Average sized peng',\n            body_mass_g == max(body_mass_g) ~ 'Big sized peng',\n             .default = 'Penguin is some size')) |&gt;\n        show_query()\n\n&lt;SQL&gt;\nSELECT\n  `pengs`.*,\n  CASE\nWHEN (`body_mass_g` = MIN(`body_mass_g`) OVER `win1`) THEN 'This is a small peng'\nWHEN (`body_mass_g` = AVG(`body_mass_g`) OVER `win1`) THEN 'Average sized peng'\nWHEN (`body_mass_g` = MAX(`body_mass_g`) OVER `win1`) THEN 'Big sized peng'\nELSE 'Penguin is some size'\nEND AS `note`\nFROM `pengs`\nWINDOW `win1` AS ()\n\n\nSo it looks like we need to change the window function\n\ncheck = tbl(con, sql(\"SELECT pengs .*,\n              CASE\n            WHEN (body_mass_g &gt;= MIN(body_mass_g) OVER win1) THEN 'this is a small penguin'\n            WHEN (body_mass_g = AVG(body_mass_g) OVER win1) THEN 'this is an average sized penguin'\n            WHEN (body_mass_g = MAX(body_mass_g) OVER win1) THEN 'this is a big penguin'\n            ELSE 'This penguin is not big, small or average'\n            END AS note\n            FROM pengs \n            WINDOW win1 AS ()\")) |&gt;\n                collect()\n\nLets look at this a little closer to make sure this worked. We would probably want to make this a little more robust. So lets go ahead and define a range.\n\ntbl(con, sql(\"SELECT pengs .*,\n              CASE\n            WHEN (body_mass_g &gt;= MIN(body_mass_g) OR body_mass_g &lt; AVG(body_mass_g)  OVER win1) THEN 'this is a small penguin'\n            WHEN (body_mass_g &gt;= AVG(body_mass_g) OR body_mass_g &lt; MAX(body_mass_G) OVER win1) THEN 'this is an average sized penguin'\n            WHEN (body_mass_g &gt;= MAX(body_mass_g) OVER win1) THEN 'this is a big penguin'\n            ELSE 'This penguin is not big, small or average'\n            END AS note\n            FROM pengs \n            WINDOW win1 AS ()\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            46.9          16.6               192        2700\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, note &lt;chr&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#group-by-and-summarize",
    "href": "SQL.html#group-by-and-summarize",
    "title": "2  SQL",
    "section": "3.8 Group by and summarize",
    "text": "3.8 Group by and summarize\nAs established earlier we can use SQL to summarize like this.\n\ntbl(con, sql('SELECT AVG(bill_depth_mm) AS avg\n           FROM pengs'))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.47.1 [:memory:]\n    avg\n  &lt;dbl&gt;\n1  17.2\n\n\nBut the actual practical utility is somewhat limited. Often we want group specific differences. Oddly enough I expected this to be a window function thing, but we actually delay computing of the mean by different groups to the end. I guess this makes sense if we are dealing with big data\n\ntbl(con, sql(\"SELECT species, AVG(body_mass_g) AS avg_body_mass\n            FROM pengs\n            GROUP BY species\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  species   avg_body_mass\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Adelie            3701.\n2 Chinstrap         3733.\n3 Gentoo            5076.\n\n\nSo if we wanted to count of the species we would do something along this line\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total\n            FROM pengs \n            GROUP BY species\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  species   total\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nFor multiple grouping variables we would define the grouping variables the same way as we would in dplyr\n\ntbl(con, sql(\"SELECT species, sex, COUNT(species) AS total\n            FROM pengs \n            GROUP BY species, sex\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.47.1 [:memory:]\n  species   sex    total\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    &lt;NA&gt;       6\n2 Adelie    female    73\n3 Adelie    male      73\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    &lt;NA&gt;       5\n7 Gentoo    female    58\n8 Gentoo    male      61\n\n\nThe same would go for multiple summary functions\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total, AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS median_bill_depth\n             FROM pengs \n             GROUP BY sex\"))\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.47.1 [:memory:]\n  species total avg_bill_depth median_bill_depth\n  &lt;chr&gt;   &lt;int&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie     11           16.6              17.1\n2 Adelie    165           16.4              17  \n3 Adelie    168           17.9              18.4\n\n\nAfter aggregation we may only be interesed in a certain value in this aggregated set. However, if we do this we will get an error because WHERE will only look for rows in unaggregated data. Effectively SQL splits filter into a grouped and ungrouped version.\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total, AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS median_bill_depth\n             FROM pengs \n             GROUP BY sex\n             WHERE avg_bill_depth &gt; 600\"))\n\nError in `db_query_fields.DBIConnection()`:\n! Can't query fields.\nℹ Using SQL: SELECT * FROM ( SELECT species, COUNT(species) AS total,\n  AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS\n  median_bill_depth FROM pengs GROUP BY sex WHERE avg_bill_depth &gt; 600 ) AS\n  `q01` WHERE (0 = 1)\nCaused by error:\n! near \"WHERE\": syntax error\n\n\nSo if we wanted to filter a grouped data frame we have to use HAVING\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total, AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS median_bill_depth\n             FROM pengs \n             GROUP BY sex\n             HAVING avg_bill_depth &gt; 10\")) |&gt;\n                collect()\n\n# A tibble: 3 × 4\n  species total avg_bill_depth median_bill_depth\n  &lt;chr&gt;   &lt;int&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie     11           16.6              17.1\n2 Adelie    165           16.4              17  \n3 Adelie    168           17.9              18.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#joinsappending-rows",
    "href": "SQL.html#joinsappending-rows",
    "title": "2  SQL",
    "section": "3.9 Joins/Appending Rows",
    "text": "3.9 Joins/Appending Rows\nIn the real world it is rare that we will have all our data in one place. Companies keep information in lots of different places because well it would be bad if we kept credit card information with all the necessary components to make a purchase. Instead of having to figure out three different things malicious actors would just need to access one database. Replacing entire data tables can also skyrocket costs. So instead, it is more efficient to simply insert rows.\n\n3.9.1 Apppending Rows\nTo kind of mimic this we are just going to slice this data frame roughly in half. While not entirely realistic the general process will be similar enough\n\n\nCode\npengs_top = palmerpenguins::penguins |&gt;\n    slice(1:172)\n\npengs_bottom = palmerpenguins::penguins |&gt;\n    slice(173:344)\n\ncon2 = src_memdb()\n\ncon3 = src_memdb()\n\npengs_top = copy_to(con2, pengs_top)\n\npengs_bottom = copy_to(con3, pengs_bottom)\n\n\nFor whatever reason show_query is not working with this so we are going to have to consult the interwebs. The SQL equivalent of bind_rows is UNION.\n\ntbl(con2, sql(\"SELECT * FROM pengs_top\n             UNION ALL \n             SELECT * FROM pengs_bottom\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nOne of the key things in this query is ALL which is somewhat new to me. Basically the ALL tells SQL that we don’t really care about duplicates so just add the rows regardless. So if we wanted to exclude duplicates we would do something like this\n\n\nCode\ntbl(con2, sql(\"SELECt * FROM pengs_top \n              UNION \n              SELECT * FROM pengs_top\")) |&gt;\n                collect() |&gt;\n                nrow()\n\n\n[1] 172\n\n\nCode\ntbl(con2,sql(\"SELECT * FROM pengs_top\") ) |&gt;\n    collect() |&gt;\n    nrow()\n\n\n[1] 172\n\n\n\n\n3.9.2 Joins\nLuckily for us the join syntax from dplyr is pretty directly taken SQL so lefts create some dummy data to join.\n\n\nCode\nnational_data &lt;- tribble(\n  ~state, ~year, ~unemployment, ~inflation, ~population,\n  \"GA\",   2018,  5,             2,          100,\n  \"GA\",   2019,  5.3,           1.8,        200,\n  \"GA\",   2020,  5.2,           2.5,        300,\n  \"NC\",   2018,  6.1,           1.8,        350,\n  \"NC\",   2019,  5.9,           1.6,        375,\n  \"NC\",   2020,  5.3,           1.8,        400,\n  \"CO\",   2018,  4.7,           2.7,        200,\n  \"CO\",   2019,  4.4,           2.6,        300,\n  \"CO\",   2020,  5.1,           2.5,        400\n)\n\nnational_libraries &lt;- tribble(\n  ~state, ~year, ~libraries, ~schools,\n  \"CO\",   2018,  230,        470,\n  \"CO\",   2019,  240,        440,\n  \"CO\",   2020,  270,        510,\n  \"NC\",   2018,  200,        610,\n  \"NC\",   2019,  210,        590,\n  \"NC\",   2020,  220,        530,\n)\n\ncon3 = src_memdb()\n\ncon4 = src_memdb()\n\nnational_data = copy_to(con4, national_data, overwrite = TRUE)\n\nnational_libraries = copy_to(con3, national_libraries, overwrite = TRUE)\n\n\nSo we have some fake national level data that we would like to join in to the dataset. We could do something like this but what we notice is that it is going to decide the join keys for us and probably create some headaches for us later on. To solve this we need to use our keys if we expose the underlying logic\n\nnational_data |&gt;\n    left_join(national_libraries, join_by(state, year)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT `national_data`.*, `libraries`, `schools`\nFROM `national_data`\nLEFT JOIN `national_libraries`\n  ON (\n    `national_data`.`state` = `national_libraries`.`state` AND\n    `national_data`.`year` = `national_libraries`.`year`\n  )\n\n\nWe will notice that join_by is shorthand for equality joins. What changes is that instead of left_key = right_key we have to specify what is coming from what table using .\n\ndb_con = con4$con\n\nquery = \"SELECT *\n             FROM national_data\n             LEFT JOIN national_libraries\n             ON (\n             national_data.state = national_libraries.state AND\n             national_data.year = national_libraries.year\n             )\n             \"\n\ndbGetQuery(db_con, sql(query))       \n\n  state year unemployment inflation population state year libraries schools\n1    GA 2018          5.0       2.0        100  &lt;NA&gt;   NA        NA      NA\n2    GA 2019          5.3       1.8        200  &lt;NA&gt;   NA        NA      NA\n3    GA 2020          5.2       2.5        300  &lt;NA&gt;   NA        NA      NA\n4    NC 2018          6.1       1.8        350    NC 2018       200     610\n5    NC 2019          5.9       1.6        375    NC 2019       210     590\n6    NC 2020          5.3       1.8        400    NC 2020       220     530\n7    CO 2018          4.7       2.7        200    CO 2018       230     470\n8    CO 2019          4.4       2.6        300    CO 2019       240     440\n9    CO 2020          5.1       2.5        400    CO 2020       270     510\n\n\n\n\nFor whatever reason with SQLite gets a little grumpy with the join syntax.\nIf we wanted to do various other joins like inner and anti joins we would do a similar thing.\n\nquery = \"SELECT * \n        FROM national_data\n    INNER JOIN national_libraries \n    ON(\n    national_data.state = national_libraries.state AND\n    national_data.year = national_libraries.year\n    )\n\"\n\ndbGetQuery(db_con, sql(query))\n\n  state year unemployment inflation population state year libraries schools\n1    CO 2018          4.7       2.7        200    CO 2018       230     470\n2    CO 2019          4.4       2.6        300    CO 2019       240     440\n3    CO 2020          5.1       2.5        400    CO 2020       270     510\n4    NC 2018          6.1       1.8        350    NC 2018       200     610\n5    NC 2019          5.9       1.6        375    NC 2019       210     590\n6    NC 2020          5.3       1.8        400    NC 2020       220     530\n\n\n\n\n3.9.3 Inequality joins\nConfession I have never really understood how inequality joins work in regular dplyr but I am sure at some point I am going to need them and now when the stakes are so low is a good time to do it. So lets just take the data from the dplyr 1.1.0 announcement to do this since we know what the output should be.\n\ncompanies &lt;- tibble(\n  id = c(\"A\", \"B\", \"B\"),\n  since = c(1973, 2009, 2022),\n  name = c(\"Patagonia\", \"RStudio\", \"Posit\")\n)\n\ntransactions &lt;- tibble(\n  company = c(\"A\", \"A\", \"B\", \"B\"),\n  year = c(2019, 2020, 2021, 2023),\n  revenue = c(50, 4, 10, 12)\n)\n\ncompanies = copy_to(con3, companies, overwrite = TRUE)\n\ntransactions = copy_to(con4, transactions, overwrite = TRUE)\n\ndb_con = con3$con\n\nSo the main idea of an inequality join is that we can join by a key in this case company but only keep records from a certain date. The blog post kind of equates it with a filter/WHERE that happens during the join phase. So we would see something like this\n\ntransactions |&gt;\n  inner_join(companies, join_by(company == id, year &gt;= since)) \n\n# Source:   SQL [?? x 5]\n# Database: sqlite 3.47.1 [:memory:]\n  company  year revenue since name     \n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1 A        2019      50  1973 Patagonia\n2 A        2020       4  1973 Patagonia\n3 B        2021      10  2009 RStudio  \n4 B        2023      12  2009 RStudio  \n5 B        2023      12  2022 Posit    \n\n\nInstead of two equality statements we would effectively use the same syntax just swapping out the = with &gt;=\n\nquery = \"\n      SELECT * FROM transactions\n      INNER JOIN companies \n      ON(\n      transactions.company = companies.id AND\n      transactions.year &gt;= companies.since\n      )\n\n\"\n\ndbGetQuery(db_con, sql(query))\n\n  company year revenue id since      name\n1       A 2019      50  A  1973 Patagonia\n2       A 2020       4  A  1973 Patagonia\n3       B 2021      10  B  2009   RStudio\n4       B 2023      12  B  2009   RStudio\n5       B 2023      12  B  2022     Posit",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#pivots",
    "href": "SQL.html#pivots",
    "title": "2  SQL",
    "section": "4.1 Pivots",
    "text": "4.1 Pivots\nIn tidyverse parlance we use pivots to change the “shape of the data.” If you are unfamiliar with this idea consider the religion and income data below. You will notice that we have a column for each income bracket or what is sometimes called “wide” data. This may be useful for some question but generally if we want to plot things or do things it will be easier if they are “long” data.\n\n\nCode\ncon5 = src_memdb()\n\nrelig = copy_to(con5, relig_income, overwrite = TRUE)\n\nhead(relig_income, n = 2)\n\n\n# A tibble: 2 × 11\n  religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Agnostic      27        34        60        81        76       137        122\n2 Atheist       12        27        37        52        35        70         73\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\nTo make our data “long” we use pivot_longer and to make data “wide” we use pivot_wider each has their own quirks but the general idea is that we have to tell these functions where to put the old names/where to get the new names and where to put the old values/where to get the new values. So if we wanted to make our data longer we would do something like this.\n\nlong = relig_income |&gt;\n    pivot_longer(-religion,\n                names_to = 'income_bracket',\n                values_to = 'income')\n\nhead(long, n = 2)\n\n# A tibble: 2 × 3\n  religion income_bracket income\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;\n1 Agnostic &lt;$10k              27\n2 Agnostic $10-20k            34\n\n\nIf we wanted to make this wide again all we would do is reverse this with pivot_wider\n\nwide = long |&gt;\n    pivot_wider(names_from = income_bracket, values_from = income)\n\n\n\nThere are ton of additional functionality that will not be covered like dealing with not uniquely identified columns.\nTo get a sense of how to do this let’s consult our old friend show_query\n\nrelig |&gt;\n    pivot_longer(-religion,\n                names_to = 'income_bracket',\n                values_to = 'income') |&gt;\n                    show_query()\n\nWe are not going to actually show the results because it is quite the query. The summary of what is happening is that SQLite doesn’t have a perfect equivalent of pivot_longer. Basically, what you need to do is to keep appending smaller and smaller data frames to each other until you get to a long data frame. In other flavors of SQL this process is a lot more humane with explicit PIVOT and UNPIVOT but I am not in one of those flavors. To spare myself a bit I am just going to do two columns\n\ntbl(con5, sql(\"\n    SELECT religion, '&lt;$10k' AS income_bracket, '&lt;$10k' AS income\n    FROM relig_income \n\n    UNION ALL\n\n    SELECT religion, '$10-20k' AS income_bracket, '$10-20k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$20-30k' AS income_bracket, '$20-30k' AS income\n    FROM relig_income\n\n    \n\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.47.1 [:memory:]\n   religion                income_bracket income\n   &lt;chr&gt;                   &lt;chr&gt;          &lt;chr&gt; \n 1 Agnostic                &lt;$10k          &lt;$10k \n 2 Atheist                 &lt;$10k          &lt;$10k \n 3 Buddhist                &lt;$10k          &lt;$10k \n 4 Catholic                &lt;$10k          &lt;$10k \n 5 Don’t know/refused      &lt;$10k          &lt;$10k \n 6 Evangelical Prot        &lt;$10k          &lt;$10k \n 7 Hindu                   &lt;$10k          &lt;$10k \n 8 Historically Black Prot &lt;$10k          &lt;$10k \n 9 Jehovah's Witness       &lt;$10k          &lt;$10k \n10 Jewish                  &lt;$10k          &lt;$10k \n# ℹ more rows\n\n\nI am a little scared to see what this looks for pivot_wider but we should at least give it a go.\n\nlong = relig |&gt;\n    pivot_longer(-religion,\n                 names_to = 'income_bracket',\n                 values_to = 'income')\n\nlong |&gt;\n    pivot_wider(names_from = income_bracket, values_from = income) |&gt;\n    show_query()\n\nOkay again this is a little unwieldy to show. Basically what happens is that we are creating a big case_when condition and then from there we are going to use the same binding trick and then group the data. So lets go ahead and copy and paste some of this.\n\n\nCode\nquery = \"\nSELECT\n    religion,\n    MAX(CASE WHEN (income_bracket = '&lt;$10k') THEN income END) AS '&lt;$10K',\n    MAX(CASE WHEN (income_bracket = '$10-20k') THEN income END) AS '$10-20k',\n    MAX(CASE WHEN (income_bracket = '$20-30k') THEN income END) AS '$20-30k'\nFROM (\n    SELECT religion, '&lt;$10k' AS income_bracket, '&lt;$10k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$10-20k' AS income_bracket, '$10-20k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$20-30k' AS income_bracket, '$20-30k' AS income\n    FROM relig_income\n) AS wide_religion\nGROUP BY religion\n\"\n\ntbl(con5, sql(query))\n\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.47.1 [:memory:]\n   religion                `&lt;$10K` `$10-20k` `$20-30k`\n   &lt;chr&gt;                   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n 1 Agnostic                &lt;$10k   $10-20k   $20-30k  \n 2 Atheist                 &lt;$10k   $10-20k   $20-30k  \n 3 Buddhist                &lt;$10k   $10-20k   $20-30k  \n 4 Catholic                &lt;$10k   $10-20k   $20-30k  \n 5 Don’t know/refused      &lt;$10k   $10-20k   $20-30k  \n 6 Evangelical Prot        &lt;$10k   $10-20k   $20-30k  \n 7 Hindu                   &lt;$10k   $10-20k   $20-30k  \n 8 Historically Black Prot &lt;$10k   $10-20k   $20-30k  \n 9 Jehovah's Witness       &lt;$10k   $10-20k   $20-30k  \n10 Jewish                  &lt;$10k   $10-20k   $20-30k  \n# ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#unnesta-brief-aside",
    "href": "SQL.html#unnesta-brief-aside",
    "title": "2  SQL",
    "section": "4.2 Unnest/a brief aside",
    "text": "4.2 Unnest/a brief aside\nSo one thing that you come across from time to time in R and python data wrangling are list columns. These happen for a variety of reasons and are pretty innocuous to handle.\n\nlist_starwars = starwars |&gt;\n    select(name, films)\n\n list_starwars |&gt;\n    unnest_longer(films)\n\n# A tibble: 173 × 2\n   name           films                  \n   &lt;chr&gt;          &lt;chr&gt;                  \n 1 Luke Skywalker A New Hope             \n 2 Luke Skywalker The Empire Strikes Back\n 3 Luke Skywalker Return of the Jedi     \n 4 Luke Skywalker Revenge of the Sith    \n 5 Luke Skywalker The Force Awakens      \n 6 C-3PO          A New Hope             \n 7 C-3PO          The Empire Strikes Back\n 8 C-3PO          Return of the Jedi     \n 9 C-3PO          The Phantom Menace     \n10 C-3PO          Attack of the Clones   \n# ℹ 163 more rows\n\n\nHowever, per this Stack overflow answer and the linked question this is not really a thing or like really not advised. Even when you try to copy the starwars dataset to a database you get an error.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#misc",
    "href": "SQL.html#misc",
    "title": "2  SQL",
    "section": "4.3 Misc",
    "text": "4.3 Misc",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#ranking",
    "href": "SQL.html#ranking",
    "title": "2  SQL",
    "section": "4.4 Ranking",
    "text": "4.4 Ranking\nThere are lots of different ways to rank things in R if we want to return the min/max you can do\n\npengs |&gt;\n    slice_max(bill_length_mm, n = 3)\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nThere are also various ranking functions.\n\nexample = tribble(~id, ~col1,\n                   1, 1,\n                   2, 2,\n                   3, 2,\n                   4, 3,\n                   5, 4)\n\nexample |&gt;\n    mutate(rank_one = dense_rank(col1),\n           rank_two = min_rank(col1))\n\n# A tibble: 5 × 4\n     id  col1 rank_one rank_two\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     1     1        1        1\n2     2     2        2        2\n3     3     2        2        2\n4     4     3        3        4\n5     5     4        4        5\n\n\nLike our dplyr join functions the dense_rank and min_rank function actually takes inspiration from SQL. So in our example where the two functions differ is how they handle ties. So in dense_rank and min_rank both id 2 and 3 get assigned the same rank where they differ is dense_rank will assign id 4 the rank of 3 and min_rank will assign id 4 the rank of 4.\nSo how would we do this in SQL\n\ncon7 = src_memdb()\n\nteam_rankings = copy_to(con7, example)\n\nteam_rankings |&gt;\n    mutate(rank_one = dense_rank(col1)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `example`.*,\n  CASE\nWHEN (NOT((`col1` IS NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((`col1` IS NULL)) THEN 1 ELSE 0 END) ORDER BY `col1`)\nEND AS `rank_one`\nFROM `example`\n\n\nThis is deceptively a bit more complex. So lets break it down.\n\ntbl(con7, sql(\"\nSELECT\nexample .*,\n    CASE \nWHEN (NOT((col1 is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((col1 is NULL)) THEN 1 ELSE 0 END) ORDER BY col1)\nEND AS rank_one\nFROM example\n\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.47.1 [:memory:]\n     id  col1 rank_one\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;\n1     1     1        1\n2     2     2        2\n3     3     2        2\n4     4     3        3\n5     5     4        4\n\n\nSo basically the PARTITION BY bit is used to divide the data into groups before we rank them. The CASE WHEN handles when we have missing values. Then the window function is applying dense rank over these partions. This was a somewhat silly example so lets do something a bit more realistic. Lets say we actually want to rank the penguins by average bill length and then return the penguins in the top 3.\n\ntbl(con, sql(\n    \"\n    SELECT\n    ranked_pengs .*,\n    CASE\n    WHEN (NOT((avg_bill_length is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((avg_bill_length is NULL)) THEN 1 ELSE 0 END) ORDER BY avg_bill_length)\n    END AS rank\n    FROM( \n     SELECT pengs .*, AVG(bill_length_mm) OVER () AS avg_bill_length\n     FROM pengs)\n     AS ranked_pengs \n     LIMIT 3\n    \"\n))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.47.1 [:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg_bill_length &lt;dbl&gt;, rank &lt;int&gt;\n\n\nWe could also do this by groups by just inserting a group by statement before the limit bit\n\ntbl(con, sql(\n    \"\n    SELECT\n    ranked_pengs .*,\n    CASE\n    WHEN (NOT((avg_bill_length is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((avg_bill_length is NULL)) THEN 1 ELSE 0 END) ORDER BY avg_bill_length)\n    END AS rank\n    FROM( \n     SELECT pengs .*, AVG(bill_length_mm) OVER () AS avg_bill_length\n     FROM pengs)\n     AS ranked_pengs \n     GROUP BY species\n     LIMIT 3\n    \"\n))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Torgersen           39.1          18.7               181        3750\n2 Chinstrap Dream               46.5          17.9               192        3500\n3 Gentoo    Biscoe              46.1          13.2               211        4500\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg_bill_length &lt;dbl&gt;, rank &lt;int&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#distinct-values",
    "href": "SQL.html#distinct-values",
    "title": "2  SQL",
    "section": "4.5 Distinct Values",
    "text": "4.5 Distinct Values\nDuplicates are a fact of life but depending on your question or what information you are trying to show repeated records may not be desirable. We handle these with the same function but kind of like mutate we have to let select handle these. If we wanted one row per column without having to specify every column in our dataset than we could do something like this\n\ntbl(con, sql(\"SELECT *\n            FROM(\n            SELECT pengs .*,\n            ROW_NUMBER() OVER (PARTITION BY species ORDER BY species) AS id \n            FROM PENGS) AS small_pengs\n            WHERE id = 1\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Torgersen           39.1          18.7               181        3750\n2 Chinstrap Dream               46.5          17.9               192        3500\n3 Gentoo    Biscoe              46.1          13.2               211        4500\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, id &lt;int&gt;\n\n\nHowever if we have a slightly less complex query than we can feed distinct multiple columns\n\ntbl(con, sql(\"SELECT DISTINCT species, island\n            FROM pengs\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island   \n  &lt;chr&gt;     &lt;chr&gt;    \n1 Adelie    Torgersen\n2 Adelie    Biscoe   \n3 Adelie    Dream    \n4 Gentoo    Biscoe   \n5 Chinstrap Dream",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#sql-concepts-translated",
    "href": "SQL.html#sql-concepts-translated",
    "title": "2  SQL",
    "section": "4.6 SQL Concepts translated",
    "text": "4.6 SQL Concepts translated\n\n4.6.1 CTES and Subqueries\nHere we are going to go over the general ideas of more advanged SQL. These depart fairly significantly from dplyr in a lot of ways. Both SQL and dplyr are flexible and you can get them to do a lot. However, since we are working with things that are on the company computer that are really big the SQL parser will enforce some optimization/when you do something to the data it is no longer available. We saw this a little bit with WHERE and HAVING but this was never made explicit.\nSQL shifts away from my mental model of how to manipulate data. We never explicitly assign anything and in dplyr, data.table, polars etc have an explicit logic of chaining built into it with not a lot of rules baked into the order of execution as long as the column is available. Whereas SQL enforces execution because it is going to optimize our query for us. This requires a little sacrafice in flexibility in how we write the code to make the optimization process easier.\nWhat this means practically is that in a lot of cases as a dplyr or coder I will assign intermediate objects to check on the health of the data cleaning pipeline as to not explicitly overwrite results. Polars kind of more clearly reflects where we aren’t allowed to use columns we just created we have to separate them into another operation. Effectively we can kind of think of CTE’s and subqueries as creating intermediate objects where we do some data cleaning and then do filter or do something else to it. So think of something to the effect of the below code.\n\npenguins = palmerpenguins::penguins\n\ncte_examp = penguins |&gt;\n    mutate(sqr_mass = body_mass_g^2,\n           sqr_length = bill_length_mm^2)\n\ncte_filt = cte_examp |&gt;\n    filter(sqr_mass &gt; 60 | sqr_length &gt; 20)\n\nsubquery_examp = penguins |&gt;\n    mutate(sqr_mass = body_mass_g^2,\n           sqr_length = bill_length_mm^2) |&gt;\nfilter(sqr_mass &gt; 60 | sqr_length &gt; 20)\n\nHowever, often times we may need to do something akin to assigning intermediate operations to solve our data processing needs. SQL has built in ways to do this but they definitely look a little wonky. We have Common Table Expressions and Subqueries\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Subquery\n                Common Table Expression\n              \n        \n        \n        \n                \n                  Inner queries that allow us to filter and perform calculations to be used later. Only used in the FROM Clause\n                  Effectively a table we can refer back to using FROM small_table\n                \n        \n      \n    \n\n\n\n\n\n4.6.2 CTE\nSo lets say we have a table where we are interested in ranking the top selling artist within each genre based on revenue per member.\n\nartists_con = dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\nartistis_data = copy_to(artists_con, artists_data)\n\nSo in dplyr we would just do something like this\n\nartists_data = tibble(\n  artist_id = c(103, 104, 105, 109, 110),\n  artist_name = c(\"Taylor Swift\", \"BTS\", \"Adele\", \"Blackpink\", \"Maroon 5\"),\n  genre = c(\"Pop\", \"K-Pop\", \"Pop\", \"K-Pop\", \"Pop\"),\n  concert_revenue = c(700000, 800000, 600000, 450000, 550000),\n  year_of_formation = c(2004, 2013, 2006, 2016, 1994),\n  country = c(\"United States\", \"South Korea\", \"United Kingdom\", \"South Korea\", \"United States\"),\n  number_of_members = c(1, 7, 1, 4, 5),\n  album_released = c(9, 7, 3, 5, 7),\n  label = c(\"Republic Records\", \"Big Hit Music\", \"Columbia Records\", \"YG Entertainment\", \"Interscope Records\"),\n  stringsAsFactors = FALSE\n)\n\nartists_data |&gt;\n    mutate(rev_per_member = concert_revenue / number_of_members) |&gt;\n    # forcing rank to reverse the order its ranking htins\n    mutate(rank = rank(-rev_per_member, ties.method = 'first'), .by = genre) |&gt;\n    filter(rank == 1) |&gt;\n    group_by(genre) |&gt;\n    arrange(desc(rev_per_member), .by_group = TRUE)\n\n# A tibble: 2 × 12\n# Groups:   genre [2]\n  artist_id artist_name  genre concert_revenue year_of_formation country      \n      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;        \n1       104 BTS          K-Pop          800000              2013 South Korea  \n2       103 Taylor Swift Pop            700000              2004 United States\n# ℹ 6 more variables: number_of_members &lt;dbl&gt;, album_released &lt;dbl&gt;,\n#   label &lt;chr&gt;, stringsAsFactors &lt;lgl&gt;, rev_per_member &lt;dbl&gt;, rank &lt;int&gt;\n\n\nFor a CTE we would do something like this\n\ntbl(artists_con, sql(\n    \"\n    WITH ranked_concerts_cte AS(\n    SELECT \n    artist_name,\n    concert_revenue,\n    genre,\n    number_of_members,\n    (concert_revenue / number_of_members) AS revenue_per_member,\n    RANK() OVER (\n    PARTITION BY genre\n    ORDER BY (concert_revenue / number_of_members) DESC) AS ranked_concerts\n    FROM artists_data)\n\n    SELECT\n  artist_name,\n  concert_revenue,\n  genre,\n  number_of_members,\n  revenue_per_member\nFROM ranked_concerts_cte\nWHERE ranked_concerts = 1\nORDER BY revenue_per_member DESC\n    \"\n))\n\nIn effect we are going through kind of just making an intermediate object in order to use it in a more familiar looking query. Here the logic is a bit obscured but imagine we are pulling millions of columns of with millions of rows from a big table. Here we are just goint to make our life a bit easier by just creating an intermediate object to query from\n\n\n4.6.3 Subquery\nIn this case we can shorten the steps by writing a subquery where we are using our FROM statement do the heavy lifting. So instead of making a smaller table we can use our from statement to do the processing like this\n\ntbl(artists_con, sql(\n    \"\n    SELECT \n        artist_name,\n        concert_revenue,\n        genre,\n        number_of_members,\n        revenue_per_member\n    FROM (\n        SELECT\n            artist_name, \n            concert_revenue, \n            genre,\n            number_of_members,\n            (concert_revenue / number_of_members) AS revenue_per_member,\n            RANK() OVER (\n                PARTITION BY genre\n                ORDER BY (concert_revenue / number_of_members) DESC\n            ) AS ranked_concerts\n        FROM artists_data\n    ) AS sub\n    WHERE ranked_concerts = 1\n    ORDER BY revenue_per_member DESC\n    \"\n))\n\n\n\n4.6.4 Inner Joins\nInner joins come up a lot in SQL interview questions. Part of the reason is that they have a lot of utility in data cleaning generally but especially when we have to kind of contort ourselves around our tools. Take two different problems\nSuppose we want to know which manager makes less than their direct report from this table.\n\nemployees = tribble(\n~employee_id,   ~name,  ~salary,    ~department_id, ~manager_id,\n1,  \"Emma Thompson\",    3800,   1,  6,\n2,  \"Daniel Rodriguez\", 2230,   1,  7,\n3,  'Olivia Smith',    7000,    1,   8,\n4,  'Noah Johnson',   6800, 2,  9,\n5,  'Sophia Martinez',  1750,   1,  11,\n6,  'Liam Brown',   13000,   3, NA,\n7,  'Ava Garcia',  12500,   3,  NA,\n8,  'William Davis',    6800,   2,  NA,\n10, 'James Anderson',   4000,   1,  11) \n\ntt(employees)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                employee_id\n                name\n                salary\n                department_id\n                manager_id\n              \n        \n        \n        \n                \n                  1\n                  Emma Thompson\n                  3800\n                  1\n                  6\n                \n                \n                  2\n                  Daniel Rodriguez\n                  2230\n                  1\n                  7\n                \n                \n                  3\n                  Olivia Smith\n                  7000\n                  1\n                  8\n                \n                \n                  4\n                  Noah Johnson\n                  6800\n                  2\n                  9\n                \n                \n                  5\n                  Sophia Martinez\n                  1750\n                  1\n                  11\n                \n                \n                  6\n                  Liam Brown\n                  13000\n                  3\n                  NA\n                \n                \n                  7\n                  Ava Garcia\n                  12500\n                  3\n                  NA\n                \n                \n                  8\n                  William Davis\n                  6800\n                  2\n                  NA\n                \n                \n                  10\n                  James Anderson\n                  4000\n                  1\n                  11\n                \n        \n      \n    \n\n\n\nI think the data in the original data lemur question is a bit messed up since it should indicate that Olivia Smith is in the same department as her manager William Davis.\n\nemployees_fixed = employees |&gt;\n    mutate(department_id = ifelse(name == 'Olivia Smith', 2, department_id)) \n\nSince we know managers don’t have a manager id and the manager id is denoted by their employee id we can just create a separate tibble and then join on the manager id column\n\nmanagers = employees_fixed |&gt;\n    filter(is.na(manager_id)) |&gt;\n    select(-manager_id) |&gt;\n    rename(manager_id = employee_id, \n          manager_salary = salary) \n\n\nemployees_fixed |&gt;\n    filter(!is.na(manager_id)) |&gt;\n    left_join(managers, join_by(manager_id)) |&gt;\n    filter(salary &gt; manager_salary)\n\n# A tibble: 1 × 8\n  employee_id name.x     salary department_id.x manager_id name.y manager_salary\n        &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1           3 Olivia Sm…   7000               2          8 Willi…           6800\n# ℹ 1 more variable: department_id.y &lt;dbl&gt;\n\n\nWhich gives us the correct answer. In SQL we would have to do something a bit different. We have to join the table to itself to create the same effect.\n\ninner_join_con = src_memdb()\n\nemployees_fixed_db = copy_to(inner_join_con, employees_fixed)\n\ntbl(inner_join_con, sql(\n    \"\n    SELECT \n    mgr.employee_id AS manager_id,\n    mgr.name AS manager_name,\n    mgr.salary AS manager_salary,\n    emp.employee_id AS employee_id,\n    emp.salary AS salary,\n    emp.name AS employee_name\n    FROM employees_fixed AS mgr \n    INNER JOIN employees_fixed AS emp\n    ON mgr.employee_id = emp.manager_id\n    WHERE emp.salary &gt; mgr.salary\n    \"\n))\n\n# Source:   SQL [?? x 6]\n# Database: sqlite 3.47.1 [:memory:]\n  manager_id manager_name  manager_salary employee_id salary employee_name\n       &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        \n1          8 William Davis           6800           3   7000 Olivia Smith \n\n\nEffectively we are using select to create a mini table and then from there we would just reference the\n\n\n4.6.5 Window Functions\nWindow functions are kid of just grouped mutates in dplyr. We define the windows using OVER and PARTITION BY. We are diving up an existing table into smaller windows that that let us run various aggregation functions or define groups. There are three parts to a window function\n\nOVER which tells SQL we are going to do a window function\nPARTION BY which tells SQL what to do the grouping by\nThen our operation So effectively we can think of it as this\n\n\npenguins |&gt;\n    mutate(avg_pengs = mean(body_mass_g, na.rm = TRUE), .by = species)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, avg_pengs &lt;dbl&gt;\n\n\nSo what we would do in dplyr would be something like this. In SQL we would do something broadly similar just in the SELECT part of the query so this would become\n\nemployees_fixed2 = employees |&gt;\n    mutate(department_id = ifelse(name == 'Olivia Smith', 2, department_id), \n           is_manager = ifelse(is.na(manager_id), TRUE, FALSE)) \n\nwindow_con = src_memdb()\n\nemployees_fixed_db = copy_to(window_con, employees_fixed2)\n\ntbl(window_con, sql(\n    \"\n    SELECT \n    name,\n    AVG(salary) OVER(PARTITION BY is_manager) AS avg_salary\n    FROM employees_fixed2\n    \"\n))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  name             avg_salary\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Emma Thompson         4263.\n2 Daniel Rodriguez      4263.\n3 Olivia Smith          4263.\n4 Noah Johnson          4263.\n5 Sophia Martinez       4263.\n6 James Anderson        4263.\n7 Liam Brown           10767.\n8 Ava Garcia           10767.\n9 William Davis        10767.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#sql-practice",
    "href": "SQL.html#sql-practice",
    "title": "2  SQL",
    "section": "4.7 SQL practice",
    "text": "4.7 SQL practice\n\n4.7.1 Histogram of Tweets\nAssume you’re given a table Twitter tweet data, write a query to obtain a histogram of tweets posted per user in 2022. Output the tweet count per user as the bucket and the number of Twitter users who fall into that bucket.\nIn other words, group the users by the number of tweets they posted in 2022 and count the number of users in each group.\nThis question is a little bit weird since I didn’t quite understand what is going on. Basically we need to do something like this.\n\nfake_user_data = tibble(\n    tweet_id = c(12321,12312123,1234231231,1232342341,1234234231212),\n    user_id = c(111,111,111,254,148),\n    tweet_date = c(mdy_hms('12/30/2021 00:00:00'), mdy_hms('01/01/2022 00:00:00'), mdy_hms('02/14/2022 00:00:00'),\n                   mdy_hms('03/01/2022 00:00:00'), mdy_hms('03/23/2022 00:00:00'))\n)\n\nfake_user_data |&gt;\n    filter(year(tweet_date) == 2022) |&gt;\n    group_by(user_id) |&gt;\n    summarise(tweet_count_per_user = n(), .groups = 'drop') |&gt;\n    summarise(tweet_bucket = n(), .by = tweet_count_per_user) |&gt;\n    rename(users_num = tweet_count_per_user)\n\n# A tibble: 2 × 2\n  users_num tweet_bucket\n      &lt;int&gt;        &lt;int&gt;\n1         2            1\n2         1            2\n\n\nSo kind of something that I don’t do a lot. The question is effectively just testing if you can use subqueries. So this mess becomes. A query that looks like this\n\ntwitter_bucket = dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\n\nfake_tweet_post = copy_to(twitter_bucket, fake_user_data, overwrite = TRUE)\n\ntwitter_bucket &lt;- tbl(twitter_bucket, sql(\"\n    SELECT \n        tweet_count_per_user AS tweet_bucket, \n        COUNT(user_id) AS users_num\n    FROM (\n        SELECT \n            user_id, \n            COUNT(tweet_id) AS tweet_count_per_user\n        FROM fake_user_data\n        WHERE EXTRACT(YEAR FROM tweet_date) = 2022\n        GROUP BY user_id\n    ) AS total_tweets\n    GROUP BY tweet_count_per_user\n\"))\n\ntwitter_bucket |&gt;\n    collect()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#getting-qualified-candidates",
    "href": "SQL.html#getting-qualified-candidates",
    "title": "2  SQL",
    "section": "4.8 Getting qualified candidates",
    "text": "4.8 Getting qualified candidates\nGiven a table of candidates and their skills, you’re tasked with finding the candidates best suited for an open Data Science job. You want to find candidates who are proficient in Python, Tableau, and PostgreSQL.\nWrite a query to list the candidates who possess all of the required skills for the job. Sort the output by candidate ID in ascending order.\nSo one thing that is interesting is that we have SQL has this having count function that makes life a little easier. Basically WHERE clauses can’t be done sequentially so instead of doing somethign like this\n\ncanidates = tribble(\n~candidate_id, ~skill,\n123,    'Python',\n123,    'Tableau',\n123,    'PostgreSQL',\n234,    'R',\n234,    'PowerBI',\n234,    'SQL Server',\n345,    'Python',\n345,    'Tableau')\n\n\ncanidates |&gt;\n    filter(skill %in% c('Python', 'PostgreSQL', 'Tableau')) |&gt;\n    group_by(candidate_id) |&gt;\n    summarise(total = n(), .groups = 'drop') |&gt;\n    filter(total == 3)  |&gt;\n    select(candidate_id)\n\n# A tibble: 1 × 1\n  candidate_id\n         &lt;dbl&gt;\n1          123\n\n\nso this becomes\n\nlibrary(DBI)\nlibrary(RPostgres)\n\ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\n\ncandidates_data = copy_to(con, canidates)\n\n\nfind_qual = tbl(con, sql(\"\nSELECT candidate_id \nFROM canidates\nWHERE skill IN ('Python', 'Tableau', 'PostgreSQL')\nGROUP BY candidate_id\nHAVING COUNT(skill) = 3\n\"))\n\nfind_qual |&gt;\n    collect()\n\nOne thing that can be kind of annoying is that when we have something like a left join we need to be pretty explicit about what we are grabbing things from and what we are referencing from\n\npages_con &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\npages = tribble(\n~page_id, ~page_name,\n20001,  'SQL Solutions',\n20045,  'Brain Exercises',\n20701,  'Tips for Data Analysts',\n31111,  'Postgres Crash Course',\n32728,  'Break the thread'\n\n)\n\npages = copy_to(pages_con, pages)\n\npage_likes_con &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password =  Sys.getenv('PASSWORD'))\n\npage_likes = tribble(\n~user_id, ~page_id, ~liked_date,\n111,    20001,  '04/08/2022 00:00:00',\n121,    20045,  '03/12/2022 00:00:00',\n156,    20001,  '07/25/2022 00:00:00',\n255,    20045,  '07/19/2022 00:00:00',\n125,    20001,  '07/19/2022 00:00:00',\n144,    31111,  '06/21/2022 00:00:00',\n125,    31111,  '07/04/2022 00:00:00') |&gt;\nmutate(liked_date = mdy_hms(liked_date))\n\npage_likes = copy_to(page_likes_con, page_likes, overwrite = TRUE, temporary = FALSE)\n\ntbl(pages_con, sql(\n    '\n    SELECT pages.page_id\n    FROM pages\n    LEFT JOIN page_likes ON(\n    pages.page_id = page_likes.page_id)\n    WHERE liked_date is NULL\n    ORDER BY page_id ASC\n    '\n)) |&gt;\n    collect()\n\nTesla is investigating production bottlenecks and they need your help to extract the relevant data. Write a query to determine which parts have begun the assembly process but are not yet finished.\nAssume you’re given the table on user viewership categorised by device type where the three types are laptop, tablet, and phone.\nWrite a query that calculates the total viewership for laptops and mobile devices where mobile is defined as the sum of tablet and phone viewership. Output the total viewership for laptops as laptop_reviews and the total viewership for mobile devices as mobile_views\n\ncon_nyt = dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password =  Sys.getenv('PASSWORD'))\n\nviewership = tribble(\n    ~user_id    ,~device_type   , ~view_time,\n123,    'tablet',   '01/02/2022 00:00:00',\n125,    'laptop',   '01/07/2022 00:00:00',\n128,    'laptop',   '02/09/2022 00:00:00',\n129,    'phone',    '02/09/2022 00:00:00',\n145,    'tablet',   '02/24/2022 00:00:00'\n)\n\nviewership = copy_to(con_nyt, viewership)\n\n\ntbl(con_nyt, sql(\n    \"\n    SELECT\n  SUM(laptop_views) AS laptop_views_count,\n  SUM(mobile_views) AS mobile_views_count\nFROM (\n  SELECT *,\n         CASE WHEN device_type = 'laptop' THEN 1 ELSE 0 END AS laptop_views, \n         CASE WHEN device_type IN ('tablet', 'phone') THEN 1 ELSE 0 END AS mobile_views\n  FROM viewership\n) AS viwership\n    \"\n)) |&gt;\n    collect()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "3  Baby Stats",
    "section": "",
    "text": "3.1 Probability\nWe have our general sense of probabilities where we are just the number of times things occur. This is kind of helpful.\nimport numpy as np\nfrom scipy import stats as stats\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nimport polars as pl\n\nnumbs = np.array([1, 3, 4, 3])\n\n1 / numbs.sum()\n\nnp.float64(0.09090909090909091)\nHowever, for the most part in the real world we don’t neccessarily care about the probability of a single event happening unconditionally. Conditioanl probability is genereallly a little weird but not totally different than just counting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baby Stats</span>"
    ]
  },
  {
    "objectID": "probability.html#conditional-probability",
    "href": "probability.html#conditional-probability",
    "title": "3  Baby Stats",
    "section": "3.2 Conditional Probability",
    "text": "3.2 Conditional Probability\nThere are two ways we generally think of basic condiitonal probability Frequentistly and Bayesianly. For the most part these are fairly similar. The key difference is how we incorporate what we know about the world.\n\n3.2.1 Freqeuentist\nIn Frequentism people often say that we don’t impose any priors on the data. This is not true in any real sense because we impose a flat prior. So the prior is kind of incorporated for us. Typically we see conditional probabilities expressed in two ways\n\\[\n\\text{P(A|B)} = \\frac{\\text{Probability of A and B Hapenning}}{\\text{Probability of B happening}} \\\\\\\n\\text{P(A|B)} = \\frac{\\text{P(A)} \\cap \\text{P(B)}}{\\text{P(B)}}\n\\]\nThis is a bit hard to visualize so lets take an example\n\nsenators.glimpse()\n\nRows: 6\nColumns: 3\n$ party  &lt;str&gt; 'Democrats', 'Republican', 'Independents', 'Democrats', 'Republican', 'Independents'\n$ gender &lt;str&gt; 'Men', 'Men', 'Men', 'Woman', 'Woman', 'Woman'\n$ total  &lt;i64&gt; 33, 40, 2, 15, 9, 1\n\n\n\nSo if we wanted to calculate the conditional probability of drawing a female democratic senator we would really just do this.\n\nwomand_democrat = senators.filter((pl.col('gender') == 'Woman') & (pl.col('party') == 'Democrats'))['total']\n\ndemocrat = senators.filter(pl.col('party') == 'Democrats').with_columns(total = pl.col('total').sum())['total'][0]\n\ndenom = 100\n\n(womand_democrat/100)/(democrat/100)\n\n\nshape: (1,)\n\n\n\ntotal\n\n\nf64\n\n\n\n\n0.3125\n\n\n\n\n\n\nSo this will roughly get us that the odds of drawing a female democratic senator is 0.31.\n\n\n3.2.2 Bayesianism\nBayesians view conditional probability in a slightly different way. But as Richard McCelreath argues that isn’t entirely true in this canned example. Typically we define Bayes Rule as something along these lines.\n\\[\\begin{align}\n\n\\text{P(A|B)} = \\frac{\\text{P(A and B)} \\times \\text{P(B)}}{P(A)}\n\n\\end{align}\\]\nHowever as Rich points out there is nothing uniquely Bayesian about this example. We could theoretically rewrite this to just plug in the numbers into the classic frequentist conditional probability statement. The canoncial example is some sort of testing framework. In statistical rethinking they use “Vampirism”\n\nprob_vampire_positive = 0.95 \n\nprob_positive_mortal = 0.01\n\npr_vamp = 0.001 \n\npr_positive = prob_vampire_positive * pr_vamp + prob_positive_mortal * (1-pr_vamp)\n\nprob_vampire_positive * pr_vamp /  pr_positive\n\n0.08683729433272395\n\n\n\n\n3.2.3 Integrating what we know and what we learn\nThe richness of Bayesian statistics come from using our expertise and updating them in light of data. Informally we can think of a prior as something we know about the data or data generating process before actually seeing. If we know that height in American males is about 5’9 we can write our prior as\n\\[\n\\begin{align}\n\\text{Male Height} \\sim N(\\mu, \\sigma) \\\\\n                        N(70.8, 1)\n\\end{align}\n\\]\nwhere we assume height is normally distributed around 70.8 and a variance of an inch. We can make this prior more restrictive by changing the standard deviation to a smaller value or greedier by increasing it. This works for all kinds of data. Say we want to formalize our prior on the Chiefs getting a questionable call. We could write the prior as a binomial distribution\n\\[\n\\begin{align}\n\\text{Chiefs Get Questionable call} \\sim Binomial(n, \\pi) \\\\\\\n                                         Binomial(n, .6)\n\\end{align}\n\\]\nA posterior is really just when our prior meets our data. Our posterior distribution contains every unique combination of data, likelihood, parameters, and our prior.\nWhat makes Bayesianism interesting is when we start updating the posterior distribution. Under the hood we need integral calculus to do this but hand deriving that either on paper or from scratch. Really what we are kind of doing when we run this through a MCMC is that we are sampling from our posterior distribution and counting the frequencies that something occurs. I am going to use the counting water example\n\n\nCode\ndef calculate_n_ways_possible(observations: str, n_water: int, resolution: int = 4):\n    \"\"\"\n    Calculate the number of ways to observing water ('W') given the toss of a globe\n    with `resolution` number of sides and `n_water` faces.\n    \n    Note: this method results in numerical precision issues (due to the product) when the\n    resolution of 16 or so, depending on your system.\n    \"\"\"\n    assert n_water &lt;= resolution\n    \n    # Convert observation string to an array\n    observations = np.array(list(observations.upper()))\n    \n    # Create n-sided globe with possible outcomes\n    possible = np.array(list(\"L\" * (resolution - n_water)) + list(\"W\" * n_water))\n    \n    # Tally up ways to obtain each observation given the possible outcomes\n    # Here we use brute-force, but we could also use the analytical solution below\n    ways = []\n    for obs in observations:\n        ways.append((possible == obs).sum())\n    \n    p_water = n_water / resolution\n    # perform product in log space for numerical precision\n    n_ways = np.round(np.exp(np.sum(np.log(ways)))).astype(int)\n    return n_ways, p_water\n\n\ndef run_globe_tossing_simulation(observations, resolution, current_n_possible_ways=None):\n    \"\"\"Simulate the number of ways you can observe water ('W') for a globe of `resolution`\n    sides, varying the proportion of the globe that is covered by water.\n    \"\"\"\n    # For Bayesian updates\n    current_n_possible_ways = current_n_possible_ways if current_n_possible_ways is not None else np.array([])\n    \n    print(f\"Observations: '{observations}'\")\n    p_water = np.array([])\n    for n_W in range(0, resolution + 1):\n        n_L = resolution - n_W\n        globe_sides = \"W\" * n_W + \"L\" * n_L\n        n_possible_ways, p_water_ = calculate_n_ways_possible(observations, n_water=n_W, resolution=resolution)\n        print(f\"({n_W+1}) {globe_sides} p(W) = {p_water_:1.2}\\t\\t{n_possible_ways} Ways to Produce\")\n\n        p_water = np.append(p_water, p_water_)\n        current_n_possible_ways = np.append(current_n_possible_ways, n_possible_ways)\n\n    return current_n_possible_ways, p_water\n\nfrom pprint import pprint\nnp.random.seed(1)\ndef simulate_globe_toss(p: float = 0.7, N: int = 9) -&gt; list[str]:\n    \"\"\"Simulate N globe tosses with a specific/known proportion\n    p: float\n        The propotion of water\n    N: int\n        Number of globe tosses\n    \"\"\"\n    return np.random.choice(list(\"WL\"),  size=N, p=np.array([p, 1-p]), replace=True)\n\n\n\nRESOLUTION = 4\nobservations = \"WLW\"\nn_possible_ways, p_water = run_globe_tossing_simulation(observations, resolution=RESOLUTION)\n\nObservations: 'WLW'\n(1) LLLL p(W) = 0.0     0 Ways to Produce\n(2) WLLL p(W) = 0.25        3 Ways to Produce\n(3) WWLL p(W) = 0.5     8 Ways to Produce\n(4) WWWL p(W) = 0.75        9 Ways to Produce\n(5) WWWW p(W) = 1.0     0 Ways to Produce\n\n\nThis will spit out the number of ways we can produce various draws. Now lets simulate the number of globe tosses wher we are just adding tosses.\n\n\nCode\nfrom scipy.special import factorial\n\ndef beta_posterior(n_W: int, n_L: int, p: float) -&gt; float:\n    \"\"\"Calculates the beta posterior over proportions `p` given a set of\n    `N_W` water and `N_L` land observations\n    \"\"\"\n    return factorial(n_W + n_L + 1) / (factorial(n_W) * factorial(n_L)) * p ** n_W * (1-p) ** n_L\n\n\ndef plot_beta_posterior_from_observations(observations: str, resolution: int = 50, **plot_kwargs) -&gt; None:\n    \"\"\"Calculates and plots the beta posterior for a string of observations\"\"\"\n    n_W = len(observations.replace(\"L\", \"\"))\n    n_L = len(observations) - n_W\n    proportions = np.linspace(0, 1, resolution)\n        \n    probs = beta_posterior(n_W, n_L, proportions)\n    plt.plot(proportions, probs, **plot_kwargs)\n    plt.yticks([])\n    plt.title(observations)\n    \n\n# Tossing the globe\nobservations = \"WLWWWLWLW\"\nfig, axs = plt.subplots(3, 3, figsize=(8, 8))\nfor ii in range(9):\n    ax = axs[ii // 3][ii % 3]\n    plt.sca(ax)\n    # Plot previous\n    if ii &gt; 0:\n        plot_beta_posterior_from_observations(observations[:ii], color='k', linestyle='--')\n    else:\n        # First observation, no previous data\n        plot_beta_posterior_from_observations('', color='k', linestyle='--')\n        \n    color = 'C1' if observations[ii] == 'W' else 'C0'\n    plot_beta_posterior_from_observations(observations[:ii+1], color=color, linewidth=4, alpha=.5)\n    \n    if not ii % 3:\n        plt.ylabel(\"posterior probability\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baby Stats</span>"
    ]
  },
  {
    "objectID": "probability.html#counting",
    "href": "probability.html#counting",
    "title": "3  Baby Stats",
    "section": "3.3 Counting",
    "text": "3.3 Counting\nOne component of interviews is that we should have some intuition on counting. Like not 1,2,3 etc but how many possible combinations of things can there be aka permutations and combinations. It has been awhile since you have had to do this so it is worth going over. Permutations care about about the unique order that things can be paired in. While combinations are order agnostic. Lets say that we are trying to figure out the number of possible ways that we can seat guests at various tables. That would be a combination. For your own sake lets number all the seats. A permutation would care about the number of unique ways we can arrange people while taking the seat numbers available. What is underriding this whole thing is factorials and orders. So the formulas look something like this. Where n is the number of items and k is the number of items to arrange\n\\[\\begin{align}\n\\text{permutation} = \\frac{n!}{(n-k)!} \\\\\n\n\\text{combination} = \\frac{n!}{k!(n-k)!}\n\n\\end{align}\n\n\\]\nfactorials are just a simplfying way of writing out something like this. Where we are progresively multiplying 1 by 1 to 2 * 1 then so on and so forth.\n\ndef factorial(n):\n\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\nprint(factorial(n = 5))\n\n120\n\n\nLets say we wanted to figure out the number of possible table combinations. Lets say we have 20 people and 5 tables. So we would do something like this.\n\nimport math \n\nmath.factorial(20)//(math.factorial(5) * math.factorial(20 - 5))\n\n15504\n\n\nHowever this is not really how wedding seating works. Order matters and who sits next to who matters. So we need to take that into account. In instead of multiplying by the factorial of k we do something like this with permutations\n\nmath.factorial(20)//(factorial(20-5))\n\n1860480",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baby Stats</span>"
    ]
  },
  {
    "objectID": "probability.html#the-centeral-limit-theorum",
    "href": "probability.html#the-centeral-limit-theorum",
    "title": "3  Baby Stats",
    "section": "3.4 The Centeral Limit Theorum",
    "text": "3.4 The Centeral Limit Theorum\nShe is genuinely one of the most coolest concepts in statistics. Bascially no matter the distribution of the variablee as we take more and more samples of the data as N increases we are going to converge to a normal distribution. This is really powerful concept in frequentist statistics because we are making inferences about the population using samples.\nEffectively we can take any distribution that we want and take an infinite number of samples from it and it will be a normal distribution. Lets take our trusty dusty old uniform distribution\n\npop = np.random.uniform(0,100, size = 5000)\n\n\n\nfig, ax = plt.subplots(figsize = (12,8))\n\nax.hist(pop, edgecolor = 'white')\n\nplt.show()\n\n\n\n\n\n\n\n\nThen lets compare what happens when we take increasingly larger sample sizes.\n\nsamp_sizes = [10, 100, 500, 1000]\n\nsamps = {}\n\nfor i in samp_sizes:\n    sample_means = [\n        np.random.choice(pop, size=samp, replace=True) for samp in samp_sizes\n    ]\n    samps[f\"sample_size{i}\"] = sample_means\n\n\ndf = (\n    pl.DataFrame(samps)\n    .unpivot(value_name=\"vals\", variable_name=\"sample_size\")\n    .explode(\"vals\")\n)\n\n(ggplot(df, aes(x=\"vals\"))\n  + geom_histogram(bins = 30) \n  + facet_wrap(\"sample_size\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baby Stats</span>"
    ]
  },
  {
    "objectID": "probability.html#explain-the-difference-between-probability-and-likelihood",
    "href": "probability.html#explain-the-difference-between-probability-and-likelihood",
    "title": "3  Baby Stats",
    "section": "3.5 Explain the Difference Between Probability and Likelihood",
    "text": "3.5 Explain the Difference Between Probability and Likelihood\nProbability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses.\nProbability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5.\nThe likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baby Stats</span>"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "4  Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "4.1 Transformations\nSince the bulk of your work will be building machnine learning models it is probably going to be important that you get way more comfortable with machine learning in python. You are a bit of a unicorn in the sense that you will do anything but learn pandas. So you are going to have to make sure that this isn’t new information.\nSince R is kind of your native language the way Python does things is weird to you so a simple linear model like this\nbecomes this (monster) in python where you now also have to tell it that you need the constant. Which is frankly crazy.\nLike an intersting cultural difference between these two is how we do things after estimation. For R apply functions to it since R is a more functionally oriented language. However if we access the object we created we have a whole host of class methods for this task. So if we wanted to predict what would happen at specified values we would do something to the effect of\nMultiple regression works somewhat similarly. Unfortunately it takes this hideous form\nWhat starts to get interesting is that what if we need we want to fit everything in one go? In R this is pretty simple\nWhereas in python you need to do something like this\nor\nWe can start to do things like add transformations like this\nx = boston.drop(columns = 'medv')\n\ny = boston['medv']\n\nx['sqr_lstat'] = x['lstat'] **2\n\ny['medv'] = np.log(y['medv'])\n\nx = sm.add_constant(x) \n\nsm.OLS(y,x).fit().summary()\nFor interactions we do something like this\nx = boston[['age', 'lstat']]\n\nx['lstat:age'] = x['age'] * x['lstat']\n\nx = sm.add_constant(x) \n\ny = boston['medv']\n\n\nsm.OLS(y, x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.556\n\n\nModel:\nOLS\nAdj. R-squared:\n0.553\n\n\nMethod:\nLeast Squares\nF-statistic:\n209.3\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n4.86e-88\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-1635.0\n\n\nNo. Observations:\n506\nAIC:\n3278.\n\n\nDf Residuals:\n502\nBIC:\n3295.\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n36.0885\n1.470\n24.553\n0.000\n33.201\n38.976\n\n\nage\n-0.0007\n0.020\n-0.036\n0.971\n-0.040\n0.038\n\n\nlstat\n-1.3921\n0.167\n-8.313\n0.000\n-1.721\n-1.063\n\n\nlstat:age\n0.0042\n0.002\n2.244\n0.025\n0.001\n0.008\n\n\n\n\n\n\n\n\nOmnibus:\n135.601\nDurbin-Watson:\n0.965\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n296.955\n\n\nSkew:\n1.417\nProb(JB):\n3.29e-65\n\n\nKurtosis:\n5.461\nCond. No.\n6.88e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.88e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\nFor qualitative variables we need to switch to a new dataset. There is a lot of interesting information in multicategory variables. One thing that we have to keep in mind when using qualitative variables normally is that we have a reference category. This may not always be straightforward to infer and we are throwing information away that is interesting. One hot encoding or breaking out the qualitative variable to indicatior variables is a nice way to do this,\ncarseats = pl.read_csv('data/Carseats.csv')\n\ncarseats.select(pl.col('ShelveLoc').unique()).head()\n\n\nshape: (3, 1)\n\n\n\nShelveLoc\n\n\nstr\n\n\n\n\n\"Bad\"\n\n\n\"Medium\"\n\n\n\"Good\"\n\n\n\n\n\ncars_small = carseats.select(pl.col('Sales', 'CompPrice', 'Income', 'ShelveLoc')).to_dummies(cs.string())\n\nx = cars_small.select(pl.exclude('Sales')).to_pandas()\n\ny = cars_small.to_pandas()['Sales']\n\nx = sm.add_constant(x)\n\nsm.OLS(y, x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nSales\nR-squared:\n0.352\n\n\nModel:\nOLS\nAdj. R-squared:\n0.345\n\n\nMethod:\nLeast Squares\nF-statistic:\n53.63\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n4.38e-36\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-895.60\n\n\nNo. Observations:\n400\nAIC:\n1801.\n\n\nDf Residuals:\n395\nBIC:\n1821.\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n3.8101\n0.755\n5.046\n0.000\n2.326\n5.295\n\n\nCompPrice\n0.0106\n0.007\n1.420\n0.156\n-0.004\n0.025\n\n\nIncome\n0.0184\n0.004\n4.473\n0.000\n0.010\n0.026\n\n\nShelveLoc_Bad\n-0.9343\n0.312\n-2.994\n0.003\n-1.548\n-0.321\n\n\nShelveLoc_Good\n3.8167\n0.322\n11.843\n0.000\n3.183\n4.450\n\n\nShelveLoc_Medium\n0.9277\n0.287\n3.235\n0.001\n0.364\n1.492\n\n\n\n\n\n\n\n\nOmnibus:\n0.001\nDurbin-Watson:\n1.941\n\n\nProb(Omnibus):\n1.000\nJarque-Bera (JB):\n0.030\n\n\nSkew:\n-0.003\nProb(JB):\n0.985\n\n\nKurtosis:\n2.958\nCond. No.\n7.74e+17\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 1.38e-29. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\nFor quick and dirty things this is nice and fairly straightforward. We are not really going to delve to deep on fitting a ton of models but really this would just involve some F-string. Instead we are going to focus on the machine learning workflow. Going through step by step and doing these are not terribily time consuming but as things get more complicated we are going to need a more robust framework to deal with this.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression and Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#regression-assumptions",
    "href": "linear-regression.html#regression-assumptions",
    "title": "4  Linear Regression and Shrinkage Estimators",
    "section": "4.2 Regression Assumptions",
    "text": "4.2 Regression Assumptions\nOur basic assumptions of linear regression are that\n\nThere are linear relationship between our outcome and our predictors\n\n\nThis is something we violate all the time. For the most part we can transform the dependent or independent variable to dependent variable\n\n\nNo Perfect multicolinearity. This is effectively a mathematical constraint. If any of the predictors are an exact linear combination of each other then we can’t actually calculate the model. Software solves this for us and kicks out various terms. It will do this arbitrarily so.\n\n\nIn practice this assumption is never violated but we still need to worry about some collinearity. The general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can’t systematically account for whether this will\n\nThe general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can’t systematically account for whether this will inflate the standard errors or deflate them. Meaning that if we are interested in variable importance or making statements about what moving one variable up or down will do.\n\nSpepherical Error term aka IID error assumption\n\nFor simplicity we can group two related assumptions\nThe first part is that we expect homogeneity\n\\[\n\\begin{equation}\n  var(\\mu_i) = E[\\mu_i] - E[\\mu_i | X_i)^2]\n\\end{equation}\n\\]\nThe second part is that there is no autocorrelation.\n\\[\n\\begin{equation}\n  cov(\\mu_i, \\mu_j | X_i, X_j) = 0\n\\end{equation}\n\\]\nIn practice when we violate this asumption our coefficients are unaffected but our standard errors can be drastically wrong. We can correct the standard errors to account for violations by loosening up how we compute the variance-covariance matrix. In some ways this is acceptable under very specific settings however. This can generally point to deeper issues with modeling the data. Like one way to trigger heterosckedasicity is simply to model binary data with OLS. A more appropriate solution would just to be model the DGP correctly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression and Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#shrinkage-estimators",
    "href": "linear-regression.html#shrinkage-estimators",
    "title": "4  Linear Regression and Shrinkage Estimators",
    "section": "4.3 Shrinkage Estimators",
    "text": "4.3 Shrinkage Estimators\nOne solution that this flavor of statistics has proposed to reduce the variance of OLS estimates when the number of predictors large or the predictors are really collinear than we can use shirkage aka regularization to penalize regression coefficients towards zero.\nRidge regression and LASSO regression are not entirely different than OLS regression. Each of these estimators reduces the residual sum of squares however we tack on a penalty term to the lasso and ridge estimators.\n\\[\\begin{align}\n\n\\text{OLS} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 \\\\\n\n\\text{Ridge} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum \\beta^{2}_{j} \\\\\n\n\\text{LASSO} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum |{\\beta_{j}}|\n\n\\end{align}\n\n\n\\]\nWhat is going on underneath the hood?\n\nRidge penalty we are summing the squared coefficients and multiplying it by the hyperparameter.\n\nEffectively imposing a penalty equivelent to the square of the magnitude of the coefficients\n\nLASSO penalty we are just summing the absolute values of the coefficients.\n\nEffectively imposing a penalty equivelent to the absolute value of the magnitude of coefficients\n\n\nThe reason why we want to penalize a model is that as we start training and start adding in variables we think help predict our outcome our models are going to start to do a better job fitting the noise. Each approach has pros and cons. A ridge penalty will let all predictors enter into the predictions even if they don’t contribute much. A lasso penalty will implictly start to kick out variables that don’t contribute anything to the model. Theoretically this is important if we have a high amount of multicollinearity/we don’t wanted automated feature selection. Effectively this question is a little theoretical in practice we are going to compare the predicitve accuracy of the two to find this out. The ridge and lasso penalties will appear in other cases so I will probably go over this again.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression and Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#what-is-multicollinearity-and-how-do-we-fix-it",
    "href": "linear-regression.html#what-is-multicollinearity-and-how-do-we-fix-it",
    "title": "4  Linear Regression and Shrinkage Estimators",
    "section": "4.4 What is multicollinearity and how do we fix it?",
    "text": "4.4 What is multicollinearity and how do we fix it?\nTo set up this question lets think of a hypothetical business problem. We are trying to assign somebody a premium based on the data we have on other people. We have access to their age, their location, past driving history, make and model of their car, and gender. Theoretically we would expect these variables to be related to one another in some way. For example make and model of their car, age, and gender is probably going to tell us something about their past driving history. We would expect that a 20 year old male driving a sports car is probably going to have a higher likelihood of having one or more speeding tickets then a female driving the same car. When we go to model this relationship each of these variables are going to be related to each other.\nLets setup an OLS with this example where age enters into the model in years, car type is a factor with a Toyota Corolla as it reference level, past incidents is the log of past driving behavior, location is a simple indicator variable where rural is the reference level, and gender is a indicator variable where the reference level is female.\n\\[\nPremium = \\alpha + \\beta_{1} Age + \\beta_{2} \\text{car_type} + \\beta_{3} location + \\beta_{4} \\log{history} + \\beta_{5} Gender + \\varepsilon\n\\]\nFor simplicity we will focus on gender as the primary explainer for insurance premiums. Lets say that gender and location are highly correlated this may occur for a variety of reasons e.g. presence of a military base, presence of a college campus etc. When we go to interpret the effect of being male on an insurance premium we hold all variables in the equation constant. However, if the moving from a rural area to the city is highly deterministic of gender we can neatly unpack the effect of gender on insurance premium. While an interpretational problem this makes it difficult to unpack the effect of a single variable on insurance premiums.\nAnother important factor is multicollinearity impacts our uncertainty estimates either biasing them down leading us to fail to reject the null more times than we should or can bias our standard errors downwards over rejecting the null hypothesis (Gujarati, Porter, and Gunasekar 2012; Lenz and Sahn 2021). Lets take a simulated example to make this point a bit more explicit\n\nlibrary(performance)\nlibrary(magrittr)\nlibrary(tidyverse)\n\n\n\n\ngenerate_dummy_data &lt;- function(data, seed = NULL) {\n  set.seed(seed = seed)\n  data &lt;- dplyr::tibble(gender = rbinom(100, 1, 0.5),\n                        location = ifelse(runif(100) &lt; 0.9, gender, 1 - gender),\n                        y = 5 + 3 * gender + rnorm(100, 0, 3))\n  return(data)\n}\n\ngenerate_dummy_model &lt;- function(data) {\n  model &lt;- lm(y ~ gender + location, data = data)\n  return(model)\n}\n\nextract_model_coefficients &lt;- function(model) {\n  coefs &lt;- broom::tidy(model) |&gt; janitor::clean_names()\n  # replace dots and white spaces with underscores\n  return(coefs)\n}\n\nrun_lm_simulation &lt;- function(data_generator, model_generator, n = 1000) {\n  simulation &lt;- \n    dplyr::tibble(i = 1:n) %&gt;% \n    dplyr::mutate(\n      data = purrr::map(.x = i, .f = ~ data_generator(seed = .x)),\n      model = purrr::map(.x = data, .f = model_generator),\n      coefs = purrr::map(.x = model, .f = extract_model_coefficients)\n      ) %&gt;%\n  return(simulation)}\n\ndummy_simulation &lt;- run_lm_simulation(data_generator = generate_dummy_data,\n                                      model_generator = generate_dummy_model, \n                                      n = 1000)\n\n\n\nsimulated_p = dummy_simulation |&gt;\n    unnest(coefs) |&gt;\n    filter(term != '(Intercept)')\n\n\n\nggplot(simulated_p, aes(x = p_value, fill = term)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(term), scales = \"free_y\", ncol = 1) +\n  labs(title = \"Distribution of P-Values for x and x_collinear\",\n       x = \"P-Value\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this case we see that we have some encouraging results that we are not going to over reject gender but if we look at location we are much more prone to failing to reject the null hypothesis. This is fine in a simulated world because we induced this relationship ourselves. However, in the real world that is not available to us.\nTo get around multicollinearity we use methods that penalize our coeficients towards zero. The two most common forms of penalization are known as ridge and LASSO regression. The canonical regression equation minimizes the sum of squared residuals. Regularization adds a penalty term to this equation that shrinks the coefficient estimates toward zero. Ridge regression (L2 penalty) adds the sum of the squared coefficients to the loss function, which discourages large coefficient values but does not set them to zero. LASSO regression (L1 penalty) adds the sum of the absolute values of the coefficients, which can shrink some coefficients to exactly zero, effectively performing feature selection.\n\nlibrary(tidymodels)\n\nhitters = read_csv('data/Hitters.csv') |&gt;\n    janitor::clean_names() |&gt;\n    filter(!is.na(salary))\n\n\nridge_spec = linear_reg(mixture = 0, penalty = 0) |&gt;\n    set_mode('regression') |&gt;\n    set_engine('glmnet') |&gt;\n    fit(salary ~ ., data = hitters)\n\nridge_spec |&gt;\n    autoplot() + \n    theme_minimal() +\n    labs(title = 'Ridge Regularization')\n\n\n\n\n\n\n\n\nAs we can see with the ridge penalty as we increase the size of the penalty the closer to zero these coefficients get. For some coefficients they get pushed towards zero immediately. One of the things that happens with multicollinear data is that jumps in one variable can cause massive swings in another in ways that aren’t entirely transparent. In prediction context we may not neccessarily care about the individual impact of one of 100 variables. We are more concerned that our model is healthy and making good predictions on data that it hasn’t seen. By regularizing our model we penalize the model for being extremely flexible this may increase our bias a bit but this comes at the gain of reducing the variance. What this means is that we are going to get a healthier prediction machine. For a LASSO model some of these coefficients will be zero just by the nature of the penalty. Effectively performing feature selection for us.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression and Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#training-these-things",
    "href": "linear-regression.html#training-these-things",
    "title": "4  Linear Regression and Shrinkage Estimators",
    "section": "4.5 Training these things",
    "text": "4.5 Training these things\nOne important thing to note is that neither when we change the scale of our predictor variable the LASSO and Ridge esstimates will not adjust accordingly because the larger coefficients that result from say salary and age are going to be on different scales which our coefficients are going to respond to accrodingly. However, the penalty term is not going react well at all since it will penalize larger coefficients by default. To build on our prior knowledge lets build a scikit learn pipeline that validates\n\nK = 5\nkfold = KFold(K,\n                  random_state=0,\n                  shuffle=True)\n\nhitters = pl.read_csv('data/Hitters.csv').with_columns(\n     pl.col('Salary').str.to_integer(strict = False).alias('Salary')\n).drop_nulls()\n\nscaler = StandardScaler()\n\nK = 5\n\nx = hitters.select(pl.exclude('Salary')).to_dummies(cs.string())\n\ny = hitters.select(pl.col('Salary')).to_numpy()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2 )\n\nlambdas = 10**np.linspace(8, -2, 100) / y.std()\n\nridge = ElasticNet(l1_ratio = 0)\n\nparam_grid = {'ridge__alpha': lambdas}\n\npipe = Pipeline(steps = [('scaler', scaler), ('ridge', ridge)])\n\npipe.fit(x_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', ElasticNet(l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', ElasticNet(l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(l1_ratio=0) \n\nvalidation = ShuffleSplit(n_splits=1,\n                              test_size=0.5,\n                              random_state=0)\n\ngrid = GridSearchCV(pipe, param_grid, cv = validation, scoring='neg_mean_squared_error')\n\ngrid.fit(x, y)\n\nGridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', ElasticNet(l1_ratio=0))]),\n             param_grid={'ridge__alpha': array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', ElasticNet(l1_ratio=0))]),\n             param_grid={'ridge__alpha': array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0) \n\ngrid.best_params_['ridge__alpha']\n\nnp.float64(0.0009804286565854396)\n\ngrid.best_estimator_\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0) \n\nridge_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            -grid.cv_results_['mean_test_score'],\n            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\nax.set_ylim([50000,250000])\n\n(50000.0, 250000.0)\n\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)\n\n\n\n\n\n\n\n\nSo we have it looking at the MSE but we can also look at it with R2\n\nridge_cv = ElasticNetCV(alphas = lambdas,\n                        l1_ratio = 0,\n                        cv = kfold)\n\n\npipe_cv = Pipeline(steps =[('scaler', scaler), ('ridge', ridge_cv)])       \n\npipe_cv.fit(x, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNetCV?Documentation for ElasticNetCVElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03, 3.60638894e+03, 2.85800156e+03,\n       2.26491736e+03, 1.794908...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n             cv=KFold(n_splits=5, random_state=0, shuffle=True), l1_ratio=0) \n\ntuned_ridge = pipe_cv.named_steps['ridge']\nridgeCV_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            tuned_ridge.mse_path_.mean(1),\n            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_ylim([50000,250000])\n\n(50000.0, 250000.0)\n\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)                   \n\n\n\n\n\n\n\n\nWe could do the same thing with a lasso regression but this workflow is not great instead we can make a function and then loop over the grids.\n\n\nmodels = {\n    'ridge': ElasticNet(l1_ratio=0),\n    'lasso': ElasticNet(l1_ratio=1)  # Lasso is a special case of ElasticNet\n}\n\n# Prepare for GridSearchCV\n\nparam_grids = {\n    'ridge': {'elasticnet__alpha': lambdas},  # Use 'elasticnet' as the step name\n    'lasso': {'elasticnet__alpha': lambdas}\n}\n\nresults = []\n# Set up ShuffleSplit cross-validation for GridSearchCV\nvalidation = ShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n\n# Function to perform grid search and output results\ndef tune_model(model_name, model, param_grid):\n    # Build pipeline\n    pipe = Pipeline(steps=[('scaler', scaler), ('elasticnet', model)])  # Step name matches model_name\n    \n    # Perform grid search\n    grid = GridSearchCV(pipe, param_grid, cv=validation, scoring='neg_mean_squared_error')\n    grid.fit(x_train, y_train)\n    \n    # Extract best parameters and model\n    best_alpha = grid.best_params_['elasticnet__alpha']\n    best_model = grid.best_estimator_\n    results.append({\n        'model': model_name.capitalize(),  # Store as \"Ridge\" or \"Lasso\"\n        'best_alpha': best_alpha,\n        'best_model': str(best_model),\n        'best_score': grid.best_score_\n    })\n    return best_model\n    \nbest_models = {}\n# Tune each model\nfor model_name, model in models.items():\n    best_models[model_name] = tune_model(model_name, model, param_grids[model_name])\n\n\nresults_df = pl.DataFrame(results)\n\nSo now we have the best model but we would like to grab the most important features. We may need something to present to stakeholders or to better understand what is going on in our models.\n\ndef get_variable_importance(best_model, feature_names):\n    # Access the 'elasticnet' step in the pipeline\n    elastic_net_step = best_model.named_steps['elasticnet']\n    coef = elastic_net_step.coef_\n    feature_importance = sorted(zip(feature_names, coef), key=lambda x: abs(x[1]), reverse=True)\n    features, coefficients = zip(*feature_importance)\n    return features, coefficients\n# Prepare side-by-side plots\nfig, axes = plt.subplots(1, len(best_models), figsize=(15, 6), sharey=True)\n\n# Get feature names\nfeature_names = x.columns\n\n# Plot VIP for each model\nfor i, (model_name, best_model) in enumerate(best_models.items()):\n    # Extract variable importance\n    features, coefficients = get_variable_importance(best_model, feature_names)\n    \n    # Determine bar colors based on coefficient sign\n    colors = ['green' if coef &gt; 0 else 'red' for coef in coefficients]\n    \n    # Create subplot\n    axes[i].barh(features, np.abs(coefficients), color=colors)\n    axes[i].set_title(f'Variable Importance: {model_name.capitalize()}')\n    axes[i].set_xlabel('Absolute Coefficient Value')\n    if i == 0:  # Add y-axis label only for the first plot\n        axes[i].set_ylabel('Features')\n    axes[i].invert_yaxis()  # Invert y-axis for descending order\n    \n    # Add a legend\n    axes[i].legend(['Positive Impact', 'Negative Impact'], loc='lower right')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhich is nice we see what is postively impacting value. However, I don’t neccessarily like these plots since we are not really showing anything interesting. A better way would be to show marginal effects. In R we would do something to this effect.\n\n\nCode\nlibrary(tidymodels)\nlibrary(marginaleffects)\nlibrary(ISLR)\n\nHitters &lt;- as_tibble(Hitters) |&gt;\n  filter(!is.na(Salary))\n\nHitters_split &lt;- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train &lt;- training(Hitters_split)\nHitters_test &lt;- testing(Hitters_split)\n\nHitters_fold &lt;- vfold_cv(Hitters_train, v = 10)\n\n\nridge_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) |&gt; \n  step_novel(all_nominal_predictors()) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nridge_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 0) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"glmnet\")\n\nridge_workflow &lt;- workflow() |&gt; \n  add_recipe(ridge_recipe) |&gt; \n  add_model(ridge_spec)\n\npenalty_grid &lt;- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\ntune_res &lt;- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nbest_penalty = select_best(tune_res, metric = 'rsq')\n\nridge_final &lt;- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit &lt;- fit(ridge_final, data = Hitters_train)\n\navg_predictions(ridge_final_fit, newdata = Hitters_test, by = 'somefactor', newdata = some_grid_of_values)\n\n\nIn machine learning parlance we could make a partial dependency plot. Which is really just plotting the predictions when we move values of our variables that we may think are interesting.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression and Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#evaluation-metrics",
    "href": "linear-regression.html#evaluation-metrics",
    "title": "4  Linear Regression and Shrinkage Estimators",
    "section": "4.6 Evaluation Metrics",
    "text": "4.6 Evaluation Metrics\nWe have more than a few evaluation metrics for linear regression or regression based tasks to be specific so it is worth going over the most popular ones. In at ?tbl-ols-mets I outline the basics of each measure but will discuss each of these measures in greater detail.\n\n\nCode\nols_mets = pl.DataFrame({\n    'Metric': [\n        'R-Squared',\n        'Mean Square Error (MSE)',\n        'Root Mean Square Error (RMSE)',\n        'Mean Absolute Error (MAE)',\n        'Mean Absolute Percentage Error (MAPE)',\n        'Symmetric Mean Absolute Percentage Error (SMAPE)'\n    ],\n    'What it Does': [\n        'The proportion of variation explained by the model',\n        'The average error of the model',\n        'The square root of the average error of the model',\n        'Similar to the MSE but we calculate with |y - yhat|',\n        'Similar to the MSE but calculated by |y - yhat| / yi',\n        'Similar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)'\n    ]\n})\n\nGT(ols_mets)\n\n\n\n\n\n\n\n\nMetric\nWhat it Does\n\n\n\n\nR-Squared\nThe proportion of variation explained by the model\n\n\nMean Square Error (MSE)\nThe average error of the model\n\n\nRoot Mean Square Error (RMSE)\nThe square root of the average error of the model\n\n\nMean Absolute Error (MAE)\nSimilar to the MSE but we calculate with |y - yhat|\n\n\nMean Absolute Percentage Error (MAPE)\nSimilar to the MSE but calculated by |y - yhat| / yi\n\n\nSymmetric Mean Absolute Percentage Error (SMAPE)\nSimilar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)\n\n\n\n\n\n\n        \n\n\nBefore we dive into each of these metrics it is worth going over the OLS optimization problem. For an OLS the optimization problem is the sum of the squared residuals. The objective function is given by\n\\[\n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - \\hat{y}_{i})^{2}\n\\]\nFor notational purposes \\(\\beta\\) is a vector of regression coefficients, \\(y_{i}\\) is the observed value of our dependent variable and \\(\\hat{y}\\) is the predicted value from our model. It is a little bit clearer if we rewrite the optimization problem with\n\\[\n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - (\\alpha + \\beta \\times x_i))^{2}\n\\]\nOf the optimatization problems in the world this one is kind of beautifully simple. So linear regression is going to draw a line (or hyperplane) through our data that tries to minimize the distance between each of the points. We are going to see how far away the predicted value is from the observed value and then we are going to square them to ensure they aren’t negative then we are going to sum them to get the total distance for each point. In effect what is going on is something to this\n\n\nfitted = form_model.fittedvalues\n\nresids = form_model.resid\n\nboston['fitted'] = fitted\n\nm,b = np.polyfit(boston['lstat'], boston['medv'], 1)\n\nfig,ax = plt.subplots(figsize=(15,6))\nplt.plot(boston['lstat'], boston['lstat'] * m + b, color = 'pink')\nplt.scatter(boston['lstat'], fitted, color = 'red',alpha = 0.1)\nplt.scatter(boston['lstat'], boston['medv'], color = 'blue', alpha = 0.5)\nfor i in range(len(boston)):\n    plt.plot([boston['lstat'].iloc[i], boston['lstat'].iloc[i]], \n             [boston['medv'].iloc[i], boston['fitted'].iloc[i]], color='blue', alpha=0.5)\nplt.xlabel('Percent Low Status')\nplt.ylabel('Home Value Median ($1000)')\n\n\n\n\n\n\n\n\nEffectively we are minimizing the distance between the points and our line via this objective function. These metrics are all broadly related to this same idea.\n\n4.6.1 R-Squared\n\\(R^2\\) aka the ‘coefficient of determination’ is some what straight forward it is \\(1-\\frac{RSS}{TSS}\\). Lets break this down a little further\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum^N_{i = 1} (y_{i} - \\hat{y}_i)^{2}}{\\sum^N_{i = 1} (y_i - \\bar{y})^{2}}\n\\]\nSo if we look closely at the fraction is the numerator is the sum of the squared residuals or the distance metric we talked about earlier. In the denominator we have the something broadly similar. The total sum of squares is something akin to a more naive predicition error. Instead of \\(\\hat{y}\\) we have \\(\\bar{y}\\) which is just the mean of y. So we are effectively taking the summed distance of the observed value of y and the mean of y. What this gives us is the fraction of the variance explained by our model. In political science and the social sciences more generally this is something that most people don’t even report anymore. In many cases we are creating regression tables where we have different dependent variables. One of the primary weaknesses of using \\(R^{2}\\) is that the easiest way to improve your \\(R^2\\) is to simply add more variables. As the model gets more information it is is going to a better job of getting \\(\\hat{y}\\) close to the observed value of Y.\nAs a political scientist by training I am always skeptical of the idea of using \\(R^2\\) for anything other than chuckling at people who see it as some sort of measure of distance from the true model. I think King (1986) sums some of the criticisms quite well. Decisions about how we process our data are also going to influence this metric because we are going to either increase or decrease the distance. \\(R^2\\) in a machine learning context may fit the description of a goodness of fit test where we are figuring out the amount of variation in the data explained. We are often swapping out variables on the right hand side. \\(R^2\\) we can assess if some combination of these variables reduce the amount of unexplained variation. Generally we would probably want to use the penalized version of \\(R^2\\) so we don’t reward our model for just adding more variables. However, I think one of the most interesting points about \\(R^2\\) are things that are already present in our OLS model. This is going to hold less and less as our model gets less interpretable.\n\n\n4.6.2 Mean Squared Error and its flavors\nHeuristically it seems a lot of machine learning evaluation for regression models uses mean squared error or some variation of it to check model performance. Conceptually these metrics are linked because we can get the \\(R^2\\) by doing:\n\\[\nR^2 = 1 - \\frac{x \\times MSE}{\\sum^n_{i =1} (y_i - \\hat{y})^2}\n\\]\nHowever, the thing with \\(R^2\\) is that it is not neccessarily that sensistive to extreme values. While in general it is difficult to have wildly divergent measure of \\(R^2\\) and MSE we may actually care about how our models perform in response to outliers. If big errors are costly to our model performance than we are going to want a measure that more directly captures their influence.\n\\[\nMSE = \\frac{1}{n} \\sum^n_{i =1} (y_i - \\hat{y})^2\n\\]\nWhere we are effectively getting the average error instead of the total error. One thing that flips with respect to \\(R^2\\) is that we are getting a slightly more interpretable difference metric. Meaning that we are getting the average distance between our predicted values and the observed values. So we are looking for this number to be kind of low! However, whats important to note is that there is no scaling factor. Meaning that outliers are going to really effect this metric because we are taking the difference and then going to square it. Lets imagine a situation where I am trying to increase the distance between a friend and the ball. If I stand right next to them and throw a ball it will land closer to them then if I started from farther away and threw the ball. Since we go through this squaring process what ends up happening is that we don’t have a great mental model of how to interpret this distance. What is the squared deviation of the median home value? Kind of not that intuitive because that distance is harder to put into words. To get it back on the original scale we can use the Root mean squared error which is just\n\\[\nRMSE = \\sqrt{MSE}\n\\]\nThis puts the MSE back on its original scale. If our model is predicting median home prices and the RMSE is 1000, this means that a typical prediction error is around 1000 dollars, with larger errors being weighted more heavily than smaller ones.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression and Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#bias-variance-tradeoff",
    "href": "linear-regression.html#bias-variance-tradeoff",
    "title": "4  Linear Regression and Shrinkage Estimators",
    "section": "4.7 Bias Variance Tradeoff",
    "text": "4.7 Bias Variance Tradeoff\nIn the prior, our model could be good at predicting the data we have. However, when we introduce new data, it might struggle to generalize and predict new observations accurately. This challenge arises from overfitting, where the model is really good at describing the dataset it has already seen. To mitigate this we split our data into training sets, test sets, and validation sets effectively hiding parts of our data from our model. It never has access to every single part of the data. By reducing the amount of data the model sees it can’t learn every single strange data point in the model.\nThe reason we do this is because we want our model to predict new data but also do a good job of approximating the data generating process. These two goals are inherently conflictual. Bias represents how far away we are from the target while variance represents how far away our guesses are from each other. If we build a model that is good at predicting every single quirk of the dataset, aka reducing variance, then when we introduce new data to the model it is going to be very very brittle. If we reduce the complexity of the model making it more flexible, aka reducing bias, then we risk not being able to catch the patterns in our data.\nIn the bias-variance tradeoff, we aim to find a balance: a model that is simple enough to generalize well to new data but complex enough to capture the important patterns. Techniques like cross-validation, regularization, and hyperparameter tuning help us navigate this tradeoff and improve the model’s predictive performance.\nMathematically we can think of the bias-variance tradeoff along the lines of mean squared error where\n\\[\nError = Bias^2 + Variance + Noise\n\\]\nWhere we have the how far away the predictions are from the real values, how far away the predictions are from each other, and the randomness from the data generating process. We can’t really doing anything about the randomness from the DGP. A useful heuristic is that we can reduce how far away our predictions are from our real values by introducing more complexity to the model. This may mean more complex functional forms of variables, introduce interactions, or use more flexible models. We can increase variance by using simple models, use bagging or boosting to leverage “bad learners”, or use regularization to penalize our models.\n\n4.7.1 Double Descent\n\n\nCode\n# code taken from https://www.r-bloggers.com/2021/07/double-descent-part-i-sample-wise-non-monotonicity/\n\nf &lt;- function(x){\n  (-0.4 + 1/(x+0.5)) + (0.5*exp(x))\n}\n#The point where the prediction error is minimized\noptimum &lt;- optimize(f, interval=c(0, 1), maximum=FALSE, tol = 1e-8)\ntemp_data &lt;- data.frame(x = optimum$minimum, y=optimum$objective)\n\nf1 &lt;- function(x){\n  ifelse(x&lt;=2,   (-0.4 + 1/(x+0.5)) + (0.5*exp(x)), NA)\n}\nf2 &lt;- function(x){\n  ifelse(x&gt;=2, (0 + 1/(1/(0.5*exp(4/x)))), NA)\n}\n#Prediction variance function (it is piecewise so creating two of them).\nvar_f1 &lt;- function(x){\n  ifelse(x&lt;=2,   (0.5*exp(x)), NA)\n}\nvar_f2 &lt;- function(x){\n  ifelse(x&gt;=2, 1/(1/(0.5*exp(4/x))), NA)\n}\n#Prediction bias function (it is piecewise so creating two of them).\nbias_f1 &lt;- function(x){\n  ifelse(x&lt;=2,-0.4 + 1/(x+0.5),NA )\n}\nbias_f2 &lt;- function(x){\n  ifelse(x&gt;=2,0,NA )\n}\nggplot(data = temp_data, aes(x=x, y=y)) +\n  xlim(0,4) +\n  geom_function(fun = var_f1, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = var_f2, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = bias_f1, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = bias_f2, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = f1, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_function(fun = f2, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  geom_point() + \n  theme_minimal() + ylab(\"Error\") + xlab(\"Number of Predictors/Number of observations\") +\n  theme(axis.text=element_blank(),\n        axis.ticks=element_blank()) +\n  annotate(\"text\", x=0.32, y=-0.2+1/(0.2+0.5), label= expression(paste(\"B\", ias^2)), color = \"blue\") +\n  annotate(\"text\", x=0.2, y=-0.2+0.5*exp(0.2), label= \"Variance\", color = \"red\") +\n  annotate(\"text\", x=0.26, y=0.21+(0.5*exp(0.2) + 1/(0.2+0.5)), label= expression(paste(\"Variance + B\", ias^2)), color = \"forestgreen\") +\n  annotate(\"text\", x=2.4, y=-0.2+1/(0.2+0.5), label= \"Interpolation limit\", color = \"black\") \n\n\n\n\n\n\n\n\n\nDouble descent is effectively just the idea that we don’t always see degrading performance in the test set when our model is really complex. Once we get past a certain threshold our model will start to perform well again.\nEffectively at some point the model will pass through every point in our dataset in multidimensional space. Effectively what is happening is that the model will memorize the training set because we let it be super complex and all we are doing is slightly modifying how we draw the line. Thus within some region we call the interpolation threshold our model is going to do poorly within this region. Once we get past this region our model’s performance will start to get better again and even outperform the ‘simple’ model again.\nWe can think of double descent as recasting our bias variance tradeoff a bit. Once we reach the interpolation threshold our model gains a somewhat crude approximation of the entirety of the data. In the bias variance framework think of bivariate linear regression. Its predictions are going to be pretty far off in some cases. A similar thing kind of happens in the interpolation region. The interpolations are crude but as we add more and more parameters the interpolations start to get better and better until the test error improves.\nThe reason this happens is that when we hit this interpolation region there is effectively only one model that can perfectly interpolate the data and when we add new data this will get thrown off. With to few parameters in the interpolation region we are not giving our model enough shots to be flexible enough to get this specification correct. By adding more and more parameters what we are doing is giving the model more chances to draw the line effectively.\n\n\n\n\nGujarati, Damodar N, Dawn C Porter, and Sangeetha Gunasekar. 2012. Basic Econometrics. Tata McGraw-Hill Education.\n\n\nKing, Gary. 1986. “How Not to Lie with Statistics: Avoiding Common Mistakes in Quantitative Political Science.” American Journal of Political Science 30 (August): 666–87.\n\n\nLenz, Gabriel S., and Alexander Sahn. 2021. “Achieving Statistical Significance with Control Variables and Without Transparency.” Political Analysis 29 (3): 356–69. https://doi.org/10.1017/pan.2020.31.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression and Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "6  Classification",
    "section": "",
    "text": "6.1 Logistic Regression\nLets work through a somewhat contrived example. Lets say we wanted to predict the whether the stock market is going up or down. This is not neccessarily all that interesting but will be good practice. Lets visualize the data\nWe may also want to see some descriptives. A line plot would be nice but it would kind of hide alot so we are going to make a beeswarm plot\nWe are slowly but surely seeing an upwardsish trend in trading volume. There are definitely more graphs that we can make but for now we are going to move to the study portion.\nLogistic regression is probably your first classifier and is an incredibly important. They are a subfamily of Maximum Likelihood estimation. In a canned example lets say we have the a coin flip and we do it like a ton of times. We record each outcome where the probability is defined something like this.\nWe have some data and now we want to model it. We take the joint probablility as the function of some parameters aka the likelihood function. The value of theta that maxmimizes the likelihood function is called the maximum likelihood estimator.\nLogit is just a sub estimator of these estimators where we make the assumption that the DGP follows a binomial distribution and that the most appropriate link function is the logistic link which is just a technical sounding way of saying we are taking the log of \\(\\frac{p}{1-p}\\). This is really usefull because we bound the odds of an event happening between 0-1. The problem with MLE is that when underneath the hood we are logging the likelihood function because well prior to the invention of computer with this much power you would never be able to hand derive these things, but we still had data to model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#logistic-regression",
    "href": "classification.html#logistic-regression",
    "title": "6  Classification",
    "section": "",
    "text": "\\[\\begin{align}\nP(Heads) = \\frac{number of heads}{number of tosses} \\\\\nP(Tails) = 1 - P(Heads)\n\n\n\\end{align}\n\n\\]\n\n\n\n6.1.1 Modeling it in Python\nLets take our stock market data and try to classify whether the direction is up or down using contemporaneous volumne and what appears to be lags of volume. Using the Stats model api we can do it like this\n\nstocks_pd = stocks.with_columns(\n    direction = pl.when(pl.col('Direction') == 'Up').then(1).otherwise(0)\n)\n\nform_version = smf.logit('direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume', data = stocks_pd.to_pandas()).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.691034\n         Iterations 4\n\nx = stocks_pd.drop(['Direction', 'direction', 'Year', 'Today']).to_numpy()\n\nx = sm.add_constant(x)\n\ny = stocks_pd['direction'].to_numpy()\n\nstats_version = sm.GLM(y, x, family = sm.families.Binomial()).fit()\n\nstats_version.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1250\n\n\nModel:\nGLM\nDf Residuals:\n1243\n\n\nModel Family:\nBinomial\nDf Model:\n6\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-863.79\n\n\nDate:\nWed, 26 Feb 2025\nDeviance:\n1727.6\n\n\nTime:\n16:23:38\nPearson chi2:\n1.25e+03\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.002868\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.1260\n0.241\n-0.523\n0.601\n-0.598\n0.346\n\n\nx1\n-0.0731\n0.050\n-1.457\n0.145\n-0.171\n0.025\n\n\nx2\n-0.0423\n0.050\n-0.845\n0.398\n-0.140\n0.056\n\n\nx3\n0.0111\n0.050\n0.222\n0.824\n-0.087\n0.109\n\n\nx4\n0.0094\n0.050\n0.187\n0.851\n-0.089\n0.107\n\n\nx5\n0.0103\n0.050\n0.208\n0.835\n-0.087\n0.107\n\n\nx6\n0.1354\n0.158\n0.855\n0.392\n-0.175\n0.446\n\n\n\n\nform_version.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\ndirection\nNo. Observations:\n1250\n\n\nModel:\nLogit\nDf Residuals:\n1243\n\n\nMethod:\nMLE\nDf Model:\n6\n\n\nDate:\nWed, 26 Feb 2025\nPseudo R-squ.:\n0.002074\n\n\nTime:\n16:23:38\nLog-Likelihood:\n-863.79\n\n\nconverged:\nTrue\nLL-Null:\n-865.59\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.7319\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.1260\n0.241\n-0.523\n0.601\n-0.598\n0.346\n\n\nLag1\n-0.0731\n0.050\n-1.457\n0.145\n-0.171\n0.025\n\n\nLag2\n-0.0423\n0.050\n-0.845\n0.398\n-0.140\n0.056\n\n\nLag3\n0.0111\n0.050\n0.222\n0.824\n-0.087\n0.109\n\n\nLag4\n0.0094\n0.050\n0.187\n0.851\n-0.089\n0.107\n\n\nLag5\n0.0103\n0.050\n0.208\n0.835\n-0.087\n0.107\n\n\nVolume\n0.1354\n0.158\n0.855\n0.392\n-0.175\n0.446\n\n\n\n\n\nTo assess our classifier we should go through several steps first we can look at the confusion matrix. Basically we can get as very general look at how well our predictions lineup with the actual data. First we need to get our predictions and then bin them into 1’s and zero\n\nform_preds = form_version.predict(stocks_pd.to_pandas())\n\ny_pred = (form_preds &gt;= 0.5).astype(int)\n\ny_actual = stocks_pd['direction'].to_pandas()\n\nconf_mat = confusion_matrix(y_actual, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n\ndisp.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x15cb4f090&gt;\n\n\n\n\n\n\n\n\n\nThis is a nice intuitive display. We are seeing what the model predicted versus what actually happens. When it comes to classification we have a variety of metrics.\n\\[\\begin{align}\n\\text{Accuracy} = \\frac{\\text{True Postive} + \\text{True Negative}}{TP + TN + FN + FP} \\\\\n\\text{Precision} = \\frac{TP}{TP + FP} \\\\\n\\text{Recall} = \\frac{TP}{TP + FN} \\\\\n\\text{F1 Score} = 2 \\times \\frac{precision \\times recall}{precision + recall}\n\n\\end{align}\\]\nEach of these metrics have a variety of benefits and tradeoffs.\n\nAccuracy\n\nAccuracy is nice and intuitive what proportion of correct predictions are we making? In a perfect world this is the only thing that we would use when evaluating models. However, if we have a lot of positives and very few negatives or vice versa our model is going to get good at predicting positives. But not that great at predicting negatives. When the class balance is bad enough we are going to get high accuracy because it is good at predicting the dominant class.\n\nPrecision and Recall\n\nPrecision is useful if we decompose what is in the denominator. If false positives are costly meaning that if flagging something as a positive would lead to not great outcomes we may need to maximize precision. However, we should always consider how it does with recall. Recall is the compliment to precision. In this case we are looking at the proportion of true positives and false negatives.\nIf we take the case of fraud and think about it like along these lines we want to strike a balance between the two maybe slightly favoring recall. While false positives are something we want to mininmize because it can cause frustrations and eat up company resources. Which isn’t good but significantly more costly. We don’t want to miss actual cases of fraud.\n\nF1 Score\n\nF1 score tries to strike this balance because maximizing precision or recall will lead the model to overcorrect. F1 score does a bit better with imbalanced datasets than accuracy the big drawback is we lose the interpretibility of whether the model is doing better minimizing false negatives or false positives.\n\n\n6.1.2 Metrics\n\nBrier Score: The Brier Score is closer in spirit to the mean squared error. So in effect we are talking about the average squared distance in probability from the true class label. This may seem kind of hard to get your head around since we are just classifying something as yes or no in a binary context or yes, no, or maybe in a multiclass context. Instead of taking the predicted class which will take on a value of 1 or 0 we are instead going to use the predicted probablity of the class. So the equation looks something like\n\n\\[\n\\text{Brier Score} = \\frac{1}{n} \\sigma^{n}_{t = 1} (\\text{Predicted Probablilty of class} - \\text{True Class Label})^2\n\\]\nSo much like the MSE a lower Brier score would indicate that the squared probability distance from the actual class on average. What this also means is that in the face of class imbalances it is going to bias the dominant class. If we think about the predicted probablity as simplistically as possible then the numerator is just the predicted frequency of events. If one class dominants the set then it is likely that they are also going to dominate the the predicted frequency.\n\nMatthew’s Correlation Coefficient\n\nThe MCC measures the correlation of predicted class labels with true class labels\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FN) \\times (TN + FN) }}\n\\]\nThe nice thing about the MCC is that it has the same interpretation as the correlation coefficient. So values closer to one indicate that there is a strong positive correlation between the predicted class and the observed class. A value closer to negative one indicates that their is a strong negative correlation. As holistic measure of how well our classifier is doing it is less sensitive to class imbalance. However, it does not neccessarily tell us how well it is doing with respect to telling us the proportion of false positives like precision does or false negatives like recall does.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#test-questions",
    "href": "classification.html#test-questions",
    "title": "6  Classification",
    "section": "6.2 Test Questions",
    "text": "6.2 Test Questions\nSome times employers will test you on things that aren’t just going through these. One real example you ran into is that you needed to hand calculate the True postives and the True negatives which you failed miserably and wasn’t able to even get to the rest.\nYou are working on a classification problem with two classes: Class 1 and Class 2. There are a total of 2000 observations in the dataset, with 1200 observations in Class 1 and 800 observations in Class 2. Your classifier produces the following predictions:\nIt assigns 1000 observations to Class 1. It assigns 800 observations to Class 2. Additionally, your classifier correctly classifies 1000 observations in total.\nUsing this information:\nHow many observations are true positives for Class 1 and Class 2? How many observations are false positives for each class?\nFor the True Postives we would do some\n\n# Define the known values\ncorrectly_classified &lt;- 1000\npredicted_class1 &lt;- 1000\npredicted_class2 &lt;- 800\nactual_class1 &lt;- 1200\nactual_class2 &lt;- 800\n\n\nTP2 &lt;- (predicted_class2 - actual_class1 + correctly_classified) / 2\n\n# Solve for TP1 using TP1 + TP2 = correctly_classified\nTP1 &lt;- correctly_classified - TP2\n\n# Solve for FP1 and FP2\nFP1 &lt;- predicted_class1 - TP1\nFP2 &lt;- predicted_class2 - TP2\n\nTP1 + TP2\n\n[1] 1000\n\n# Print the results\ncat(\"True Positives for Class 1 (TP1):\", TP1, \"\\n\")\n\nTrue Positives for Class 1 (TP1): 700 \n\ncat(\"True Positives for Class 2 (TP2):\", TP2, \"\\n\")\n\nTrue Positives for Class 2 (TP2): 300 \n\ncat(\"False Positives for Class 1 (FP1):\", FP1, \"\\n\")\n\nFalse Positives for Class 1 (FP1): 300 \n\ncat(\"False Positives for Class 2 (FP2):\", FP2, \"\\n\")\n\nFalse Positives for Class 2 (FP2): 500",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#the-workflow",
    "href": "classification.html#the-workflow",
    "title": "6  Classification",
    "section": "6.3 The workflow",
    "text": "6.3 The workflow\nIn the stock market example we can do something like this.\n\n\nx = stocks[['Lag1', 'Lag2']]\n\ny = stocks['Direction']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1994)\n\nThen we fit our models like this we initiate a logit object same as we would in tidy models same goes for various other. To pair this down we are going to just use two features. One thing to note is that scikit learn regularizs the logit by default with an l2 norm aka the ridge penalty.\n\n\n\nlogit = LogisticRegression()\n\nSo lets go ahead and fit the logit and see how it does\n\nlogit_mod = logit.fit(x_train, y_train)\n\nlogit_preds = logit_mod.predict(x_test)\n\n\naccuracy_score(y_test, logit_preds)\n\n0.508\n\nconfusion_matrix(y_test, logit_preds)\n\narray([[33, 83],\n       [40, 94]])\n\nclassification_report(y_test, logit_preds)\n\n'              precision    recall  f1-score   support\\n\\n        Down       0.45      0.28      0.35       116\\n          Up       0.53      0.70      0.60       134\\n\\n    accuracy                           0.51       250\\n   macro avg       0.49      0.49      0.48       250\\nweighted avg       0.49      0.51      0.49       250\\n'\n\n\nSo it has about a coin toss chance on being right. If we wanted to give it specific values we can give it some values\n\n\nnew_dat = pl.DataFrame({\n    'Lag1': [0.5, 0.8, 1.1],\n    'Lag2': [1.2, -0.5, 0.3],\n})\n\npredicted_probs = logit_mod.predict(new_dat)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#lda",
    "href": "classification.html#lda",
    "title": "6  Classification",
    "section": "6.4 LDA",
    "text": "6.4 LDA\nLDA is like a lot of these a dimensionality reduction machine. We make some assumptions one being that the classes are linearly separable hence the L in LDA. We also assume equal variance covariance matrices that follow a multivariate normal distribution. To check this we plot the matrices and they should look like an ellipsis.\n\nWhen classes are pretty close to perfectly seperable. This is because the MLE starts to break down. Even a firth correction may not be optimal.\nIf we have small sample size and the distribution of the predictors is approx normal.\n\n\nlda = LinearDiscriminantAnalysis(store_covariance=True)\n\nlda_mod = lda.fit(x_train, y_train)\n\nlda_preds = lda_mod.predict(x_test)\n\nconfusion_matrix(y_test, lda_preds)\n\narray([[33, 83],\n       [40, 94]])\n\n\nI will comeback to this but for the most part we are still doing a bad job of predicting the down direction. We are also seeing some bad things in the diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#qda",
    "href": "classification.html#qda",
    "title": "6  Classification",
    "section": "6.5 QDA",
    "text": "6.5 QDA\nQDA is pretty similar to LDA in a lot of respects howevr it assumes that each class has its own mean and covariance rather than enforcing and equality assumptions\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\n\nqda_mod = qda.fit(x_train, y_train)\n\n\nqda_preds = qda_mod.predict(x_test)\n\n\nconfusion_matrix(y_test, qda_preds)\n\narray([[ 27,  89],\n       [ 33, 101]])",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#naive-bayes",
    "href": "classification.html#naive-bayes",
    "title": "6  Classification",
    "section": "6.6 Naive Bayes",
    "text": "6.6 Naive Bayes\nNaive Bayes is a classic we make the assumption that our predictors are drawn from a gaussian distribution, that each of the features is conditionally independent, and we make the assumption that the classes are linearly seperable. What is interesting about Naive Bayes is that it works pretty well\n\nnb = GaussianNB()\n\nnb_mod = nb.fit(x_train, y_train)\n\n\nnb_preds = nb.predict(x_test)\n\nconfusion_matrix(y_test, nb_preds)\n\narray([[32, 84],\n       [36, 98]])",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#k-nearest-neighbors",
    "href": "classification.html#k-nearest-neighbors",
    "title": "6  Classification",
    "section": "6.7 K-Nearest Neighbors",
    "text": "6.7 K-Nearest Neighbors\nFinally the most “machine-learny” of the models of these classifiers that we have covered so far is K-Nearest neightbors. KNN is fairly intuitive things that are close to each other are more likely to be related to each other. We don’t make any assumptions of the functional form of the decision boundary. For the most part each of the classifiers so far we make linearity assumptions or that the classification boundary follows a Bernoulli distribution. We also make no assumptions about the distribution of the data. This is kind of cool but as we make less and less assumptions about the data we start needing more of it. However we need to ensure that each of our features are on the same scale or the algorithm is not going to do well. If we have the difference in years versus 1,000 or millions of dollars. A jump of 100 yeasrs is substantively larger than a jump in a 100 dollars but K-nearest neighbors is going to let the larger numbers dominate. So we need to rescale everything.\nThe other thing is we don’t have any a priori knowledge of the optimal number of neighbors. We have have some idea but for machine learning models we use something called a hyperparameter to improve our model. There are mechancical parts of our models that we don’t have control over. In this setting we are not going to change how we calculate Euclidean distance. However, the number of neighbors to set that determines the classification boundaries are. Nothing in dataset or model can tell us what is the correct number of neighbors. We basically iterate over these to find the optimal k aka the optimal number of neighbors\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ncaravan = pl.read_csv('data/Caravan.csv')\n\nx_df = caravan.select(pl.exclude('Purchase'))\n\nscaler.fit(x_df)\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\nx_std = scaler.transform(x_df)\n\nfeature_sd = pl.DataFrame(x_std, schema=x_df.columns)\n\n\nx_train, x_test, y_train, y_test = train_test_split(np.asarray(feature_sd),\n                                                    caravan['Purchase'],\n                                                    test_size = 1000)\n\n\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn_preds = knn.fit(x_train, y_train).predict(x_test)\n\nknn.fit(x_train, y_train).predict_proba\n\n&lt;bound method KNeighborsClassifier.predict_proba of KNeighborsClassifier(n_neighbors=1)&gt;\n\nconfusion_matrix(y_test, knn_preds)\n\narray([[879,  61],\n       [ 56,   4]])\n\n\nSo one neighbor does pretty well but what if we could do better? We can perform a grid search over the number of neighors. 10 Neighbors is probably unreasonable. Since this is not actually all that intensive we could theoretically just use a for loop to tune this parameter. However, thats not really the best way since we have built in tools.\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors' : list(range(1,10))}\n\ngrid_search = GridSearchCV(knn, param_grid, cv = 5)\n\ngrid_search.fit(x_train, y_train)\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]}) best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=8) KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=8) \n\ngrid_search.best_score_\n\nnp.float64(0.9400666480338831)\n\ngrid_search.score(x_test, y_test)\n\n0.939\n\nbest_grid = grid_search.best_estimator_\n\n\nbest_grid_preds = best_grid.predict(x_test)\n\nconfusion_matrix(y_test, best_grid_preds)\n\narray([[939,   1],\n       [ 60,   0]])\n\nbest_grid.n_neighbors\n\n8\n\n\nThis is kind of nice. So lets breakdown what we did. We k-fold cross-validation meaning we created 5 evenly sized folds where the model will be trained on k-1 fold. Meaning we trained the model on 4 folds. Then repeat this process. In a grid search we are kind of just going through each individual combination of hyperparameters. So we are doing k =1 distance = manhattan, k =1 distance = euclidean etc. So this maybe fine if we don’t have a ton of things to do but if we have a ton of hyperparameters than that is not all that efficient.\n\nimport itertools\n\np_grid_2 = {'n_neighbors': list(range(1,5)), 'metric': list(['euclidean', 'manhattan', 'minkowski'])}\n\ncombos = itertools.product(p_grid_2['n_neighbors'], p_grid_2['metric'])\n\nfor comb in combos:\n    print(comb)\n\n(1, 'euclidean')\n(1, 'manhattan')\n(1, 'minkowski')\n(2, 'euclidean')\n(2, 'manhattan')\n(2, 'minkowski')\n(3, 'euclidean')\n(3, 'manhattan')\n(3, 'minkowski')\n(4, 'euclidean')\n(4, 'manhattan')\n(4, 'minkowski')\n\n\nWhereas random search is will take a random sample of these combos\n\nparam_dist = {'n_neighbors': np.arange(1,5), 'metric': ['euclidean', 'manhattan', 'minkowski']}\n\nrandom_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter = 4)\n\nrandom_search.fit(x_train, y_train)\n\nRandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={'metric': ['euclidean', 'manhattan',\n                                                   'minkowski'],\n                                        'n_neighbors': array([1, 2, 3, 4])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={'metric': ['euclidean', 'manhattan',\n                                                   'minkowski'],\n                                        'n_neighbors': array([1, 2, 3, 4])}) best_estimator_: KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=np.int64(4)) KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=np.int64(4)) \n\nbest_random = random_search.best_estimator_\n\nbest_preds_random = best_random.predict(x_test)\n\nbest_random.score(x_test, y_test)\n\n0.935\n\nconfusion_matrix(y_test, best_preds_random)\n\narray([[935,   5],\n       [ 60,   0]])",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#selecting-the-best-classifier",
    "href": "classification.html#selecting-the-best-classifier",
    "title": "6  Classification",
    "section": "6.8 Selecting the best classifier",
    "text": "6.8 Selecting the best classifier\nOften times we do something akin to this where we train a bunch of models and then have to compare which one is the best. This would be a huge pain to do manually. However this is why computers are nice\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_curve, auc\n\nscaler = StandardScaler()\n\n\nx = stocks.select(pl.col('Lag1', 'Lag2', 'Volume')).to_numpy()\n\ny =  stocks.select(pl.col('Direction')).to_numpy().flatten()\n\nlabel_encoder = LabelEncoder()\n\ny_encoded = label_encoder.fit_transform(y)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y_encoded,\n                                                    test_size = 0.2)\n\n\nx_train_scaled = scaler.fit_transform(x_train)\n\nx_test_scaled = scaler.fit_transform(x_test)\n\nclassifiers = [\n    {'label':'QDA',  'model': QuadraticDiscriminantAnalysis()},\n    {'label':\"Logit\",'model': LogisticRegression()},\n    {'label':\"LDA\",  'model': LinearDiscriminantAnalysis()}, \n    {'label':\"KNN\",  'model': KNeighborsClassifier(n_neighbors=5)}\n]\n\n\nfor m in classifiers:\n    model = m['model']\n    model.fit(x_train_scaled, y_train)\n    pred = model.predict(x_test_scaled)\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test_scaled)[:,0])# direction down \n    plt.plot(fpr, tpr, label=f'{m[\"label\"]} ROC')\n\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\n\n(0.0, 1.0)\n\nplt.ylim([0.0, 1.05])\n\n(0.0, 1.05)\n\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()   \n\n\n\n\n\n\n\n\nSo we plotting ROC-AUC curves. Generally we want it to look a lot better than this. But this will give us a chance to talk about these. When it comes to evaluating models ROC-AUC curves are a favorite. These complement each other in a lot of respects.\nIn our accuracy, precision, recall, and F1 scores they all try to evaluate the proportion of true positives in comparision to either total classifications or something else. However, we don’t really ever have a good intuition at what level we should be cutting off these judgements. ROC-AUC lets us plot the performance of our models at various thresholds and against the random chance. In this case there are lots of instances whwer our model is not even as good as the random classifier. We probably need more features in order to improve our sensitivity.\nIn general we want it to look more curvy where the ROC-AUC is a lot closer to the 1 on the Y axis. A flat curve indicates that our model performs as well as flipping a coin. In any classification task we are going to have some mistakes in classification no matter the threshold. A AUC of 0.8 would indicate that our classifier is going to classify that point correctly close to 80% of the time.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "classification.html#what-should-we-do-if-we-have-some-class-imbalance",
    "href": "classification.html#what-should-we-do-if-we-have-some-class-imbalance",
    "title": "6  Classification",
    "section": "6.9 What should we do if we have some class imbalance?",
    "text": "6.9 What should we do if we have some class imbalance?\n\nlibrary(tidyverse)\nlibrary(tinytable)\n\ndf = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv\") |&gt;\n    janitor::clean_names() \n\n df |&gt;\n    group_by(success, died) |&gt;\n    summarise(total = n()) |&gt;\n    mutate(prop = total/sum(total)) |&gt;\n                tt()\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                success\n                died\n                total\n                prop\n              \n        \n        \n        \n                \n                  FALSE\n                  FALSE\n                  46452\n                  0.981656805\n                \n                \n                  FALSE\n                   TRUE\n                    868\n                  0.018343195\n                \n                \n                   TRUE\n                  FALSE\n                  28961\n                  0.991849036\n                \n                \n                   TRUE\n                   TRUE\n                    238\n                  0.008150964\n                \n        \n      \n    \n\n\n\nSo for the most part nicely balanced class data is pretty rare! Lets take a fairly simple from Julia Silge’s excellent blog post. For the most part mountaneering trips don’t tend to have a lot of deaths which is good! However, when we want to start predicting what makes a succesful climb this can be a problem if we aren’t careful.\n\n6.9.1 Preprocessing: A variety of sampling techniques\nSo we can think about fraud and war along similar lines in the class imbalance space. For the most part we have a ton of not war or not fraud cases in these kinds of dataset. I am sure if we start to skew the class imbalances a little bit more or even just added a third class like ‘maybe fraud’ or ‘mid’ to the COW database we are going to start pushing the limits of what the model could handle.\nOne of the real important things is choosing evaluation metrics that can handle imbalanced classes. As we saw in the last section accuracy tends to be a poor metric with big class imbalances becuase its the total number of correct predictions divided by the total number of predicitions. The thing about these models is that they tend to do pretty well in predicting things after they are trained on some data. So the model gets really good at predicting the dominant class but gets really bad at predicting the less dominant class.\nWe can pray that we get more data and just by pure chance we got a bad draw with lots of class imbalance. But, for the most part those prayers will never be answered. Fundamentally if the DGP of war or fraud change we have kind of a big problem on our hands. Instead what we can do is use as variety of resampling methods to artificially create balance between the two classes. In Table 6.1 I outline the broad strokes of each resampling method.\n\n\n\n\nTable 6.1\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Method\n                What it Does\n              \n        \n        \n        \n                \n                  Oversampling                                     \n                  Bias the classifier towards the minority class by duplicating the minority class                                                      \n                \n                \n                  Undersampling                                    \n                  Bias the classifier towards the minority class by removing examples of the dominant class                                             \n                \n                \n                  Random Oversampling Examples (ROSE)              \n                  Generate new synthetic points with some noise to the minority class                                                                   \n                \n                \n                  Synthetic Minority Oversampling Technique (SMOTE)\n                  Generates new synthetic points by interpolating between existing points                                                               \n                \n                \n                  Adaptive Synthetic Sampling (ADASYN)             \n                  Identify hard to classify examples meaning they do not have a ton of neighbors. Generate some synthetic points via K-nearest neighbors\n                \n        \n      \n    \n\n\n\n\n\n\nEffectively what each of these are doing from the perspective of the classifier because it imposes non-uniform missclassfication costs. The simplest approach is to simply randomly add data to the minority class or randomly delete data from the dominant class. However, these approaches have some obvious drawbacks. One thing that you will notice is that we have to manually set a ratio. What ratio do we set 🤷. We may want to shoot for equal balance but we have kind of fundamentaly changed the DGP which does not come at zero cost. By oversampling we are increasing our computational cost because we are copying our data and we risk over fitting. For undersampling we are chucking useful information away a lot of useful information that may improve predictive power down the line.\nTo combat this there are various procedures to use more data driven approaches to make up data or sample data. SMOTE generates synthetic examples by selecting a minority class instance, finding one of its k-nearest neighbors (from the same class), and creating a new point along the line segment between them. The synthetic point is placed at a random position between the two, ensuring that the newly generated data follows the distribution of the minority class via K-NN. SMOTE is viable strategy if the skew isn’t really bad. One of the problems is that while k-NN is kind of clever we have a hyperparameter to tune which can be computationally expensive. Since SMOTE is an oversampling technique some of the same draw backs of oversampling still apply.\nTo handle some of the weakpoints of SMOTE we have a family of synthetic data generators try to combat overfitting. A slightly modified version of the SMOTE framework is ADASYN which generates more and more synthetic data points near points closer to the decision boundary. Effectively what is happening is that we end up generating more and more points closer to the decision boundary with the goal of class balance. Another technique, ROSE which is based on bootstrap re-sampling techniques. Effectively what that means is that we are going to randomly draw a row from our dataset, setting the probability of the drawing the minority and the majority class to be the same. Then we are going to generate a synthetic example in the same neighborhood with a small amount of noise estimated via a kernel density estimate. We are going to keep doing this over and over again till we get a balanced dataset.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html",
    "href": "tree-based-methods.html",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "7.1 A Tree\nFor a variety of regression and classification tasks we can use what was once the state of the art and that is random forrests\nUnderriding the entire idea of Bagging and Boosting are regression trees.. Regression trees have a lot of math behind an inherently kind of simple and beautifully dumb idea.\nWhen we fit a single tree there are a set of rules that we give it to make decisions about the data if a variable hits that rule then depending on what side of the line it is then it will split the data off into the left or right side. In the example above when income is not greater than or equal to 24.5 we split it into the false category an then it stops. We have satisfied the criteria. In the true column we then repeat the proces however when we hit the false column again we have a new criterion and that is how much there is on advertising spending.\nFormally we are splitting the predictor space into a number of regions or leave or the little boxes. The branches, lines are the branches. Somewhat unituitevely at first the leaves appear at the bottom and the stem appears at the top. Intuitively each side gets labelled region one on the graph with the associated points and so on and so forth. In practice this creates a lot of weird boundaries that map onto interactions or different functional forms. Making them good at finding thse relationships without directly specifying them.\nlibrary(rpart)\nlibrary(parttree)\n\nout = rpart(Kyphosis ~ Start + Age, data = kyphosis)\n\n# Grab the partitions and plot\nfit_pt = parttree(out)\n\nplot(fit_pt)\n\n\n\n\n\n\n\nFigure 7.1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#why-do-this",
    "href": "tree-based-methods.html#why-do-this",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.2 Why do this?",
    "text": "7.2 Why do this?\nOLS is really nice in the social sciences where we have a better understanding of the DGP and not alot of variables and we don’t neccessarily have that or have the luxury of a lot of data. One thing that OLS does a poor job of is that we have to enter in the relationships ourselves and every possible combination of interactions or functional form of a small set of variables is going to overfit the model/we are going to have a lot of multicollinearity. OLS has a closed form solution where we are making these predictions over the entire data. Which is going to get messy in a way we may not quite understand.\nRegression trees divide the this mess into smaller regions. We are effectively dividing up the space into small mutually exclusive regions. We do this by starting off greedy we find some value that does well with our splitting rule. Then it will make a split into a mutually exclusive regions then redo this process. Instead of looking ahead to see if there is some partiontion that will make its predictions better in the future it will choose what is convenient.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#gardening",
    "href": "tree-based-methods.html#gardening",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.3 Gardening",
    "text": "7.3 Gardening\nAs you may imagine growing a tree can maximize in sample fit but thats not really what we are after. However a tree with two branches maybe really interpretable and less sensitive to new data but is going to be somewhat biased. In the regression part we did a little cross validation but didn’t really go over it so it is worth talking about.\nCross-validation is critical to our workflow. We use training and test sets to evaluate our models. This is good but effectively we are only evaluating our model once with the validation set. Instead of a spray and pray approach we can hold out “more” data to evaluate our model and tweak it. Generally we want a model that minimizes our test set error. But if tweak the model based on the test set we are going to end up overfitting. Effectively as I understand it is that we are getting leakage into the training phase without cross-validation.\nCross-validation helps with this because we are using parts of our training set that the model hasn’t seen before to tune our models. We shuffle the data a bit and then evaluate our model on the folds. There are lots of methods most people use is K-fold cross validation. We effectively create lots of datasets from our training set where some of the data is used in the evaluation phase and some of it is used in the training phase. This is really beneficial because you can’t just wait around for more data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#creating-a-forest-or-boosting-a-tree",
    "href": "tree-based-methods.html#creating-a-forest-or-boosting-a-tree",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.4 Creating a Forest or Boosting a Tree?",
    "text": "7.4 Creating a Forest or Boosting a Tree?\nAs you may imagine striking a delicate balance in growing a single tree is tricky. Basiscally trees are a little bit like adaptive nearest neighbors. This gets a little more complicated but we will not get into that to much. Once we start partition to things into finer and finer neighborhoods we may be able to tune it pretty well but it is going to be extremely sensitive. Enter bagging and boosting. These rely on a similar idea but go about it in a different way. Basically what if we just made a bunch of dumb models and found a way to make them not dumb?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#bagging",
    "href": "tree-based-methods.html#bagging",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.5 Bagging",
    "text": "7.5 Bagging\nBagging is just a fancy way of saying voting or averaging. Instead of one smart tree we make a bunch of them! We fit the trees on random parts of our training data and penalize them for getting them to smart. Once each tree makes its predictions we ask them to vote or average their predictions. Whatever gets spit out is the answer to our problem. Conditional on us doing it correctly. So in classification problem if we are predicting whether or not a transaction is fraudulent we would fit the model asking it to classify a bunch of transactions as fraud or not. Whatever number of trees we ask it to make we are just going to take a simple vote on whether that transaction is fraud or not. It is a little beautifully democratic. This works because we are using the predictions from sevearl weak learners where we have high variance but low bias. Meaning that we may not always get our darts in the same area but the difference between predicted and real values is low. Then once we average over these weak learners we reduce our variance.\nRandom forests are based off of this idea but we add a small tweak. We still fit a large number of bad learners but we decorrelate the trees to improve performance. We sample the data with replacement where some trees see the same information a few times while others never see the same inforamtion. We also never let any one tree see a majority of the predictors. This will bias downward really good predictors and give other predictors a chance. Than we take a vote/average of what the trees spit out.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#the-intuition-behind-why-random-forests-work-well",
    "href": "tree-based-methods.html#the-intuition-behind-why-random-forests-work-well",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.6 The Intuition Behind Why Random Forests Work Well",
    "text": "7.6 The Intuition Behind Why Random Forests Work Well\nOne thing that is important is what goes into the linear algebra machine. For things like XGBoost and Random Forests these are kind of the epitome of the “Two Culture” mentioned by Breiman (2001) where the result of a bunch bad learners end up doing really well predicting the relationships between independent and dependent variables. This happens because well we don’t impose many restriction on what the functional form of the relationship between the independent variables between the dependent variable. So if the actual regression equation is something to the effect of\n\\[\n\\text{Wreckless Driving} = \\alpha + \\beta_{1} \\log{\\text{risk taking}} + \\beta_{2} age^{2} + \\beta_{3} \\log(\\text{risk taking}) \\times age^{2} + \\varepsilon\n\\]\nThese models are going to have a fairly good chance of picking this weird relationship up because it is dividing up the space into smaller pieces to either classify or generate predicitions. For a visual demstration see Figure 7.1\nSubstantively what this means is that these methods are uber flexible but if we were are going to use this as a feature selection machine for models with more parametric assumptions than that could be problem. Random forest are a form of self-regularizing adaptive smoothers. As Curth, Jeffares, and van der Schaar (2024) argue\n\nWe showed that forests improve upon trees through multiple distinct mechanisms that are usually implicitly entangled: they reduce the effect of noise in outcomes, reduce the variability in realized predictors and reduce potential bias by enriching the class of functions that can be represented.\n\nAs you may have guessed a logit doesn’t have these self regularizing properties. So if we cram a bunch of predictors on the right hand side of the equation the likelihood that there is some multicollinearity is pretty high. While this doesn’t effect our coefficient estimates it does effect our standard errors in many cases this can cause them to be inflated resulting in us failing to reject the null. However, it is also possible that this effect is reversed which is highly problematic. Lets say we are interested in describing the effect of some new strategy on some KPI. We can come up with a plausible causal mechanism for why that is. We then go and do our feature selection and our main variable of interest is statistically significant. WOOHOOO great! Not so fast in many political science papers by adding correlated variables we can actually deflate the standard errors causing over rejection of the null (Lenz and Sahn 2021).\nThe other key property that we should focus on is that random forests search over a more diverse array of functions making their predictions less sensitive to us inputting the features in there wrong. Lets say we have a really good idea that age is an important predictor of reckless driving. We would expect that not only is the effect non-linear but has an interaction with gender or risk taking behavior. We may not be able to directly observe risk taking behavior but we may have some proxies that we could get ahold of like traffic tickets. If this interaction is a significant predictor of wreckless driving then our random forest models are going to pick them up. Unless we explicitly enter these into our model we are not going to pick this relationship up. So if we enter age in without transforming the variable we are likely not going to find its effect or the effect of a one unit increase in age is likely to be incredibly small.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#boosting",
    "href": "tree-based-methods.html#boosting",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.7 Boosting",
    "text": "7.7 Boosting\nOur other option is boosting. Boosting has different mechanics but the same idea. Instead of making a really good tree we start with a tree that slightly better than random guessing but not that much better on some sample of our training data. However, instead of modeling a lot of trees at once we sequentially fit a series of tree and then sum over them to grab our predictions or classifications\n\n7.7.1 Adaboost\nADAboost focuses on the misclassification part. Where we take the residuals of the weak learners then re-weight the samples to give more weight to mistakes. Then at the end we take the contribution of each learner as the weighted average of the predictions/majority vote. This was popular for awhille but tends not to perform that well compared to gradient boosting.\n\n\n7.7.2 Gradient Boosting\nMore recent innovations of boosting use gradient boosting. What is annoying about machine learning is that they are always coming up with catchier terms for old concepts. By gradient we are really just talking about a derivative of our loss function. It depends on what framework you are working in but this could be first order derivatives or second order derivatives. All gradient descent is doing is using the derivative (slope) to move in the direction of the steepest descent (negative gradient).\nIn OLS we can think of this as finding the slope and finding the value that gets us closer to minimizing the sum of the squared. So we would get predicted values from one tree and see how much this reduces this distance then we take the residuals from the model to try and reduce the distance between the residuals and real values in the next tree. We repeat this process by taking the slope and seeing what direction the values would take. Then we would repeat this process until until we no longer get any improvement minimizing the loss function. In this model the size of the step that we take right or left is determined by our learning rate. So if we the set a high learning rate we may take 2 or three steps in the positive direction rather than a half step in the positive direction. This isn’t a perfect analogy since OLS has a closed form solution but this is the general idea.\n\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\ngbr = GradientBoostingRegressor(n_estimators=5, learning_rate=0.1, max_depth=2, random_state=42)\ngbr.fit(X, y)\n\nGradientBoostingRegressor(max_depth=2, n_estimators=5, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressor?Documentation for GradientBoostingRegressoriFittedGradientBoostingRegressor(max_depth=2, n_estimators=5, random_state=42) \n\nstage_predictions = np.array(list(gbr.staged_predict(X)))\n\ncolors = ['red', 'green', 'blue', 'orange', 'purple']\n\n# Plot predictions from each tree\nplt.figure(figsize=(10, 6))\nfor i, preds in enumerate(stage_predictions):\n    plt.scatter(X, y, label='True Data', color='blue', alpha=0.3)  # True data is blue\n    plt.scatter(X, preds, label=f'Tree {i+1} Predictions', color=colors[i], alpha=0.6)  # Different colors for each tree\n    plt.title(f\"After Tree {i+1}\")\n    plt.xlabel(\"Feature\")\n    plt.ylabel(\"Target\")\n    \n    # Calculate residuals after this tree's prediction\n    residuals = y - preds\n    print(f\"Residuals after Tree {i+1}: {residuals[:5]}\")\n\n# Move legend outside the plot\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Legend\")\n\nplt.tight_layout()  # Adjust layout to fit the legend outside\nplt.show()\n\n\n\n\n\n\n\n\nWithin the gradient boosting framework we have two popular approaches: XGboost and LightGBM. Each of these approaches focus on impurity meaning how well a tree can distinguish between two classes using something like a GINI coeficient or Entropy that tries to measure class separtion or using mean squared error.\nThey both work to reduce these impurities however they differ in how they grow the tree. LightGbm focuses on the leaf wise growth whereas XGBoost will do branch or level wise growth. Functionally all this means is that for a level wise strategy we are growing both sides of the flow chart which kind of imposes some regularization. Whereas leafwise growth adds more and more leaves making better predictions but is prone to overfitting.\n\n\n7.7.3 Tuning Parameters of XGBoost\nIt is worth going over since there are a ton of them. Some we may not actually end up touching but still. For referecnce I am going to just use the XGBoost function from sci-kit learn.\nloss: Just the loss function to be optimized - So if we were to use a OLS loss function than we would just be using the sum of the squared residuals. Meaning XGBoost is going to minimize the distance between the set of the points using the sum of the squared residuals.\nlearning rate: The learning rate effectively regularizes the contribution of each tree. So if we have 3 trees the learning rate is going to add a penalty to minimize the contribution of the tree to slow down how much information an individual tree contributes. Typically we have to balance this penalty with the number of trees. In effect the learning rate is a weighting factor for each new model that minimizes how quickly a new tree responds to errors from an old tree.\ntrees/N_estimators: really just the number of trees our model is going to have.\ntree depth: So this is really just a way of saying how tall the tree is. so a tree depth of 5 is going to have 5 levels\ncriterion: So this governs the ambigous cases. We set a metric to measure the quality of the split\nnumber of iterations no change: How many times does it get to keep trying to improve on the validation score before we stop trainging it.\nsubsamples: controls the fraction of training data that the trees use for the boosting round. I.e if we set it to .5 it will use half the training data.\nmin samples split: Effectively how many data points does a node have to have to make a split. Higher values effectively penalize the tree for splitting off points below this threshold. This helps the tree from learning to much about points that could just be weird for any number of recisions\nStop iterations/number of iterations no change: This is effectively the number of times we let the model try to improve its validation score. We only set this if we plan on implementing early stopping. Early stopping is one way to try to prevent overfitting. However, it is a bit of a double edged sword. We could stop it to early and it may not get to learn some of the quirks of the data generating process. We could let it retry 100 times but on the 101st try it actually improves on the validation score. We are also making the assumption that the validation set is representative of the test data. This may be true but we are making a bit of an assumption that random sampling isn’t biting.\nmtry/ max features: specifies the number of features that each tree can randomly select from when considering a split at each node. This prevents the tree from overfitting the data since one tree does not see all the features. Since it is still an ensemble method we want the tree to have more shots are a variety of functional forms and interactions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#gradient-boosting-or-random-trees",
    "href": "tree-based-methods.html#gradient-boosting-or-random-trees",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.8 Gradient Boosting or Random Trees",
    "text": "7.8 Gradient Boosting or Random Trees\nThe answer is that it kind of depends. Gradient boosting of what ever flavor you choose is going to be more prone to overfitting that random forests. Gradient boosting is also a little bit better in the face of class imbalances. Since the prior tree is used to inform the previous tree if you don’t select good features in the beginning you could end of fitting the noise. The other problem is compute time. Since gradient boosting relies on the previous tree it has to wait till that tree is done being fit.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "tree-based-methods.html#imputation-methods",
    "href": "tree-based-methods.html#imputation-methods",
    "title": "7  Tree Based Methods and an Aside on Validation",
    "section": "7.9 Imputation Methods",
    "text": "7.9 Imputation Methods\nOne thing about random forests and Gradient boosting machines is that they can handle missing values or in the case of random forests can be used as missing values imputers. For the most part missing data is a little bit of a fact of life. Data goes missing for a variety of reasons. Respondents could forget or refuse to identify their race and ethnicity on a survey some people may be embarassed to report their educational attainment. The issue is not that the data is missing the issue is that we don’t actually know the how these data are missing. Classifically we don’t really about missingness in the DV. We will kind of see why when we cover the difference in mechanisms.\n\nmissingess_mechanisms = pl.DataFrame(\n    {'Missingess Mechanism':[\n        'Missing Completly at Random (MCAR)',\n        'Missing at Random (MAR)',\n        'Missing Not at Random (MNAR)'\n    ],\n    'Assumption': [\n        'Data is missing completly at random. We update a database and the power goes out. In the full database this data is kind of missing completly at random. Nothing about the DGP dicatates that these missing values',\n        'A weaker form of missngness. Missingness in our DV is not determined by our DV. What this means is that we can learn about the missingness by knowing our independent variables',\n        'Means that a missing DV is not determined by any of the IVs instead it is determined by the DV iteself'\n    ]\n\n    }\n)\n\nGT(missingess_mechanisms)\n\n\n\n\n\n\n\nMissingess Mechanism\nAssumption\n\n\n\n\nMissing Completly at Random (MCAR)\nData is missing completly at random. We update a database and the power goes out. In the full database this data is kind of missing completly at random. Nothing about the DGP dicatates that these missing values\n\n\nMissing at Random (MAR)\nA weaker form of missngness. Missingness in our DV is not determined by our DV. What this means is that we can learn about the missingness by knowing our independent variables\n\n\nMissing Not at Random (MNAR)\nMeans that a missing DV is not determined by any of the IVs instead it is determined by the DV iteself\n\n\n\n\n\n\n        \n\n\nIn practice we can never really tell the difference between MNAR or MAR. We can start to investigate the missingness by seeing if we can build a model to classify missing values. If our models start to do a good job of classfiying missing values than we at least have a lead on the possible mechanism. This doesn’t fully get us there because we can only really tell the difference between them with data we never observe. Missing at random is only applicable in very rare instances.\n\n7.9.1 Solutions\nFor missing completly at random we can simply delete the missingness observations. Whether this is through listwise deletion or just modifying our query to ignore missing values. If we want to save time than we can let the model handle it for us. Certainly for simple models like linear regression the default is to drop it so it can actual solve the optimization problem.\nHowever, if we assume that we can’t simply ignore the values than we have a lot of options. One of the most common forms of handling missingess is that we can perform interpolation. Effectively we are using information in that variable to predict the missing value. Typically this is done with the next value and the previous value to generate a predicition. We then have to use some kind of distance metric to get that value. We can use the mean and make the guess, if we are worried about extreme values we can use the median, or if we want to get fancy then we can use nearest neighbors where it will try and find a typically value in a cluster. So if a 4 is generally surrounded by 3’s and 5’s and we have a missing value sandwhiched between a 3 and a 5 the nearest neighbor interpolation will probably guess 4 because those are the values that are typically within that neighborhood. We can either to this column wise or rowwise. Column wise is going to be a lot cheaper but doesn’t integrate another information that could be helpful in estimating the missing values.\n\n\n7.9.2 Multiple imputation\nInterpolation is fine but we are only guessing once and not integrating all the information that we could into the estimating the missingess or treat the missingness as something as observable. Multiple imputation or imputation methods generally allow us to estimate those missing values but preseve the uncertianty associated with the estimation proceduce. Effectively what we are going to do is treat the missing values as a statistical problem where create some number of copies of our dataset. We make the assumption that the missingness is multivariate normal and the data is MAR or MCAR. We can then start to think of the probability of missingness as a function of the parameter estimates. In the copies of our dataset we then estimate a plausible value. We then bootstrap the data and use a model to get an estimate on the value of the mising data then we draw a single point from the posterior distribution. Once all m datasets are imputed, we fit our model to each dataset separately. The final estimates are obtained by averaging the parameter estimates across all datasets, while the standard errors incorporate both within-dataset variance and between-dataset variance (following Rubin’s rules)\n\n\n7.9.3 Where do Random Forests Come into this?\nWell it turns out that random forest are really great at learning hidden relationships between variables. Unless we place priors on certain variables we are imposing a flat prior or making other assumptions about the data. Random Forests for missing data imputation proposes a more flexible model for missing value imputation then interpolation. For things like KNN imputation or Lasso imputation tuning hyperparameters can be difficult and often effect the imputations in ways that are hard to asssess. One alternative to MICE or AMELIA is missForest which is doesn’t require some of the same overhead for large datasets. In multiple imputation we are making m copies of our dataset then running a model to predict the missing values in the dataset and then running m models on the various datasets and then combining them at the end. These multiple imputation methods have been kept up to date to incorporate machine learning models. As you may imagine this is a lot of heavy lifting computationally for a moderatly sized dataset much less a large dataset.\nA compromise, in some respects, is using a specially tailored random forest model to predict the missing values. These random forest approaches do really well with large datasets and incorporate the benefits of bagging into the missing value replacement. By that we mean we can start to model more complex relationships that could explain the missingness in our data\n\n\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and Arejoinder by the Author).” Statistical Science 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nCurth, Alicia, Alan Jeffares, and Mihaela van der Schaar. 2024. “Why Do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers.” https://arxiv.org/abs/2402.01502.\n\n\nLenz, Gabriel S., and Alexander Sahn. 2021. “Achieving Statistical Significance with Control Variables and Without Transparency.” Political Analysis 29 (3): 356–69. https://doi.org/10.1017/pan.2020.31.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tree Based Methods and an Aside on Validation</span>"
    ]
  },
  {
    "objectID": "unsupervised-learning.html",
    "href": "unsupervised-learning.html",
    "title": "8  Unsupervised Learning",
    "section": "",
    "text": "8.1 The Curse\nIn the prior “chapters” we covered a variety of supervised learning techniques. By supervised we have some direct control over hyperparameter tuning and how we enter things into the model to predict a response. Unsupervised learning doesn’t neccessarily have a dependent variable that we are trying to predict. Instead the unsupervised techniques covered in ISLR and ESLR look at unsupervised learning as an approach to help us uncover hidden groups in our data. They focus on PCA and various clustering methods. There are no hard and fast rules with this and we often think of it as an EDA excercise or a way to reduce the dimensionality of our data. In effect supervised learning is learning the relationship of data without labels. So say we need to build a classifier. We label the data as penguin not a penguin without then we train a model on these things. For an unsupervised model we may not care so much but may want to throw everything in a model and then see if the model can uncover groupings of penguin not penguin.\nThe curse of dimensionality frequently comes up in machine learning because we often have a ton of predictors. The curse of dimensionality can be a little weird if you don’t deal with math a whole lot. The general idea of the curse of dimensionality is that as the number of dimensions increase our models are going to start to break down because as the number of dimensions increase our model is going to find data points standing all alone. We may not know that because our model starts to do well finding things that are alone in a neighborhood. A good model will be able to find these points hiding in a crowd a bad model wouldn’t! Both good and bad models will find points by themselves which is not neccessarily helpful as we start to introduce new data points the model’s performance will start to detoriate.\nThe thing with the curse of dimensionality is that we never escape it because we are going to run into when modeling variables. For OLS the curse of dimensionality enters as we start to add more and more variables because we are stratiying our data by more and more information. Even when we are doing something like inverse propensity score weighting and then using these weights in a bivariate OLS you are running into the curse in the propensity score equation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised-learning.html#what-can-we-do-about-the-curse",
    "href": "unsupervised-learning.html#what-can-we-do-about-the-curse",
    "title": "8  Unsupervised Learning",
    "section": "8.2 What can we do about the curse?",
    "text": "8.2 What can we do about the curse?\nWe can use methods like PCA to reduce the number of dimensions. The general idea is that we are trying to map high dimensional relationships into lower dimesional proxies.\n\nimport polars as pl\nimport polars.selectors as cs \nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\npengs = pl.from_pandas(load_penguins())\n\npengs.select(pl.exclude('year', 'species', 'sex', 'island')).drop_nans().corr()\n\n\nshape: (4, 4)\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\nf64\nf64\nf64\nf64\n\n\n\n\n1.0\n-0.235053\n0.656181\n0.59511\n\n\n-0.235053\n1.0\n-0.583851\n-0.471916\n\n\n0.656181\n-0.583851\n1.0\n0.871202\n\n\n0.59511\n-0.471916\n0.871202\n1.0\n\n\n\n\n\n\nEach of these variables map on to some measure of penguin bigness some of them are more correlated with each other but if we had flipper width or bill width or some other measure of penguins bigness there is going to significant overlap. Once we enter these into a regression these models are going to pretty good job at descrbing penguin weirdness. A way to help us is to summarize these variables into what we call principal components.\nPrincipal components are a way to summarize the variation in these overlapping variables in a minimal set of variables.\n\nfrom sklearn.decomposition import PCA\n\nnormalized = pengs.select(cs.numeric(), pl.col('species')).select(pl.exclude('year')).with_columns(\n        (cs.numeric() - cs.numeric().mean() / cs.numeric().std())\n    ).drop_nulls()\n\npca = PCA()\nnumeric_data = normalized.select(cs.numeric())\npca.fit(numeric_data)\n\n# Transform the data and include species labels\ntr_df = pca.transform(numeric_data)\npca_components = pl.DataFrame(\n    tr_df,\n    schema=[f'PC{i+1}' for i in range(tr_df.shape[1])]\n).with_columns(normalized['species']) \n\npca_df = pca_components.to_pandas()\n\n# Define a color map for the species\ncolor_map = {\n    \"Adelie\": \"red\",\n    \"Gentoo\": \"blue\",\n    \"Chinstrap\": \"green\"\n}\n\n# Create the scatter plot\nplt.figure(figsize=(8, 6))\nfor species, color in color_map.items():\n    subset = pca_df[pca_df['species'] == species]\n    plt.scatter(\n        subset['PC1'], \n        subset['PC2'], \n        label=species, \n        color=color, \n        alpha=0.7\n    )\n\n# Add plot details\nplt.title('PCA: First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title='Species')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that these two variables do a reasonable enough job of distinguishing the species from each other. What is going on underneath the hood is that we are creating some linear combinations of these variables. We can relax this constraint using gams if we want but in practice we do this linearly alot.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised-learning.html#clustering",
    "href": "unsupervised-learning.html#clustering",
    "title": "8  Unsupervised Learning",
    "section": "8.3 Clustering",
    "text": "8.3 Clustering\nIn a sense we are finding subgroups with this PCA example but we can also use clustering algorithms. Basically clustering algorithms will try to ifer subgroups from the data either through minimizing a distance measure if we are using k-means. We could also use mixture clustering where we make some assumptions of the distributions but can let more moment conditions enter into the clustering algorithm.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "Causal-Inference.html",
    "href": "Causal-Inference.html",
    "title": "9  Causal Inference",
    "section": "",
    "text": "9.1 The Fundamental Problem\nIn industry there are lots of opportunity for causal inference! Actually in a lot of cases we don’t really realize! Platforms are constantly changing and companies want to evaluate the rollout of new features and whether or not they actually move the needle is important since they are sinking a lot of time and resources into these features. So today we are going to tackle A/B experiments broadly while touching up our understanding of a host of other causal inference methods. One thing that I think is kind of cool is that there is a lot more communication between Academic Social Sciences and the Causal Inference world. We are all dealing with the same problem but some us have more money than the others. However, we can’t run away from of the fundamental problems we face.\nThe fundamental problem of causal inference is that we don’t have a time machine. By that I mean we would love to have a data frame like this where we can get the individual level causal effect. Meaning we can observe what would happen if we assigned people the treatment and what would happen if people never received the treatment.\ntime_machine_world = {\n    'Treated Outcome': [50, 60, 70],\n    'Untreated Outcome': [10, 25, 23],\n\n}\n\npl.DataFrame(time_machine_world).with_columns(\n    (pl.col('Treated Outcome') - pl.col('Untreated Outcome')).alias('Treatment_Effect')\n)\n\n\nshape: (3, 3)\n\n\n\nTreated Outcome\nUntreated Outcome\nTreatment_Effect\n\n\ni64\ni64\ni64\n\n\n\n\n50\n10\n40\n\n\n60\n25\n35\n\n\n70\n23\n47\nTo do this we would need a time machine to go back in time and either give them the treatment or take away the treatment. We only ever observe one state of an individual. We are missing half of the information to get this individual result. Instead we have to come up with a framework that lets us bridge these worlds. The most common framework and kind of the working model that motivates the observational causal inference framework is the Randomized Control Trial/ A/B test framework. We kind of all intuitevely understand this framework. We have a, more or less, exchangeable group of people, villages etc and then we randomize treatment status then take the average of the outcome. Effectively we can think of it as\n\\[\n\\text{Average Treatement Effect} = (\\bar{Y} | \\text{Treated}) - (\\bar{Y} | \\text{Untreated})\n\\]\nIn a DAG framework we can think of it like this.\nNotice that the randomizer is not effected by anything and the only effect of randomization is through the treatment status. Importantly any things that make us special don’t actually determine our potential outcomes. It eliminates a lot of potential avenues for selection or and treatment status is not determined by anything else in our model. This is a pretty powerful and magical idea, but it comes at a cost. In this case we mean money. Just because we can randomize doesn’t mean we will magically find the causal effect of something on something else. In the preceding sections we will spend some time on how to do this and potential areas for trouble.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "Causal-Inference.html#rcts",
    "href": "Causal-Inference.html#rcts",
    "title": "9  Causal Inference",
    "section": "9.2 RCTs",
    "text": "9.2 RCTs\nTechnically in industry they are called A/B tests because well reasons. The general idea is that this follows the canonical RCT. We have an intervention we want to test so we will run an experiment. This is straightforward idea but we have to make a lot of design considerations. Since experiments have an air of magic to them there is lots of room for them to go wrong. There are lots of statistical avenues to explore including Multi-Armed Bandit, problems staggered rollout, conjoint experiments, and whole host of other things. However, before we start with those its going to be pretty important to talk about power.\nStatistical power is one thing that a variety of designs have in common since they kind of broadly fall under the null hypothesis significance testing framework. Very basically the idea of power comes down to the idea that we want our experiment to have a good shot of rejecting the null hypothesis if the alternative is true. Basically we want to reduce the probability of Type II error or the false negative rate. Importantly when power is low we tend to estimate a treatement effect that is larger than the ‘true’ treatment effect and/or the sign of the effect may be wrong (Gelman and Carlin 2017).\nSo what are the components of statistical power. Political scientists tend to think of power and what we are interested a bit differently then psychologists do. We tend to think of treatment effects on the scale of our oiutcome and through a regression lens. This is important because Psychologist tend to deal with standardized effect sizes and ANOVA/ANCOVA analysis which is just regression in a trench coat, but thats a conversation for another time. Much of the following section is just written notes to Carlise Rainey’s excellent power primer. I should mention that there are bespoke tools to simulate a variety of designs and identifying some potential sticking points. The setup of the paper is that we have a some experiment where we assume a null hypothesis of no treatment effect and an alternative of H1 &gt; 0 implying a positive treatment effect. Although we can extend this to a two-tailed and/or a negative treatment effect.\n\n9.2.1 Rules 1 & 2 of Statistical power\nThe main takeaway from this section is that we can think of power as\n\\[\n\\text{Statistical Power} = \\frac{\\text{Treatment}}{\\text{Standard Error}}\n\\]\nLets first start with the intuition. If we could rerun our trial infinite times and collect the treatment effects and count how we frequently we observe the treatment effect we would get our canonical Gaussian distribution. The center of the distribution will be centered over the treatment effect because of the CLT. How far away a single point is from the treatment effect is just the variance. However, if we run the trial a thousand times than that would be the sample treatment effect. The distance measure in the sample is we just refer to as the standard error. To reject the null hypothesis of Treatment Effect &gt; 0 than we would want our sampling distribution to look something like this\n\ndat = data.frame(treat = rnorm(1000, mean = 5, sd = 1))\n\nggplot(dat, aes(x = treat)) +\n    geom_histogram() +\n    theme_minimal() +\n    geom_vline(xintercept = 0) +\n    labs(x = 'Treatment Effect')\n\n\n\n\n\n\n\n\nKind of by extension we are powering the experiment to be able to adjudicate this. What we are saying in words that an experiment has “80% power to detect a treatment effect of ___” if \\(\\text{Treatment effect} - 1.64 \\times \\hat{SE}_{\\hat{\\text{TE}}} &gt; 0\\) in 80% of repeated experiments. Whats noticeable is that we are inserting the critical value of a 90% confidence level in this equation. Obviously as change our significants level then that changes the critical value. If we subsitute in the moment conditions of the normal CDF than we get back to\n\\[\n\\text{Statistical Power} = \\frac{\\text{Treatment}}{\\text{Standard Error}}\n\\]\nEffectively when we are doing a priori analysis we are making an assumption a plausible treatment effect and the variability of that estimate. However, this is pretty challenging because if we knew the treatment effect ahead of time we either simulated the data or we wouldn’t need to spend time and money on an experiment!\n\n\n9.2.2 What treatment effect should I asume?\nFrom a statistical power perspective there are a few things we can think about. What our best guess is of the effect, the smallest plausible effect, and the smallest substantively meaninful effect. We may have run a gajillion mailer experiments so we may have a pretty good sense of what the effect could be. That would be great but thats not neccessarily generalizeable to the rest of us. Instead we should use our substantive expertise to think about what would be a smallest plausible effect of our treatment or the smallest meaningful effect of our treatment. This will be important later because we are going to use it in our power equation.\n\n\n9.2.3 From the SD to the SE\nWhen we go to actually estimate our treatment effect we are going to do something like this\n\\[\nOutcome = \\alpha + \\beta \\times Treatment + \\varepsilon\n\\]\nWe make the assumption that \\(\\varepsilon\\) is distributed \\(N(0, \\sigma^2)\\) which gets us the classic standard error of\n\\[\nSE^{Classic}_t = \\sqrt{\\frac{\\sigma^2}{N \\times \\bar{D} (1-\\bar{D})}}\n\\]\nWhere N is our total sample size and D is the fraction assigned to each condition. This \\(\\bar{D}\\) will be important as we start adding conditions. However, lets remain focused on what this tells us. We can further decompose this which gets to the idea that \\(\\sigma\\) is just the standard deviation within the outcome. It makes sense because what we are really doing is stratifying Y by our treatment conditions. To get a good guess of what our \\(\\sigma\\) to plug into our treatment condition is we should find a suitable reference population. The chances are that the company or somebody else is working this area. What this gets us is that we can use features of a reference population to predict a plausible standard error. Where it looks like\n\\[\n\\frac{2 \\times SD(Y)}{\\sqrt{2 \\times N}}\n\\]\nWhere N is the sample size per each condition. What this intuition gets us is that we can think of ways to reduce \\(\\varepsilon\\) by including covariates. This is taking things out of the error term reducing the SE.\n\n\n9.2.4 From the SE to the Minimum Detectable Effect Size\nSo this is a long way to get to this idea. If we set an alpha level of 1.90 and a target of 80% power than we know a two things. We know that the arms of either side of a 90% confidence interval are 1.64 standard errors wide thus power is the percent of the sampling distribution that is largert than 1.64 standard errors. To find the MDE we need to solve for the treatment effect that positions 80% of the sampling distribution above 1.64. So after passing it through the inverse of the standard normal CDF and centering it we get the result that for 80% power we can multiply the SE by 2.5 to get the MDE. First, the arms of the 90% confidence interval are 1.64 standard errors wide, so the power of the study is the percent of the sampling distribution that is larger than 1.64 standard errors.\nCarlise uses the example of a ANES experiment where the outcome is a feelings thermometer for respondents PID. If we were to want to run a similar experiment we could take the SD of 20.8 and plug it into our numerator. The reference experiment has two conditions with more or less 500 experiments. So we can back out the minimal detectable effect size like this\n\nsd_ref = 20.8 \n\n\nnumerator = sd_ref * 2 \n\ndenominator = math.sqrt(2 * 500)\n\n\nmde = (numerator / denominator) * 2.5\n\nprint(f'The minimal detectable effect is for 80% power {mde}')\n\nThe minimal detectable effect is for 80% power 3.2887687665751146",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "Causal-Inference.html#getting-to-the-sample-size",
    "href": "Causal-Inference.html#getting-to-the-sample-size",
    "title": "9  Causal Inference",
    "section": "9.3 Getting to the Sample Size",
    "text": "9.3 Getting to the Sample Size\nCool we could do some more stuff to bring down the standard error. However, how do we move from mde and SE to a good sample size? When we combine our rules we get something like this\n\\[\n\\text{Sample Size} = 2 \\times (\\frac{2.5 \\times \\hat{SD}(y)}{\\hat{Treatment}})^2\n\\]\nIf we are using an existing studies we can grab the standard errors and",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "Causal-Inference.html#rules-of-thumb-according-to-jpal",
    "href": "Causal-Inference.html#rules-of-thumb-according-to-jpal",
    "title": "9  Causal Inference",
    "section": "9.4 Rules of Thumb According to Jpal",
    "text": "9.4 Rules of Thumb According to Jpal\n\nA larger sample increases the statistical power of the evaluation\nIf the effect size of the program is small, the evaluation needs a large sample to achieve a given level of power\nAn evaluation of a program with low take-up needs a large sample\nIf the underlying population has high vartiation in outcomes, the evalution eneds a large sample\nFor a given sample size power is maximized when the sample s equally split between the treatment and control group\nFor a given sample size, randomizing at the cluster level as opposed to the individual level reduces the power of the evaluation. The more similar the outcomes of individuals within clusters are, the larger the sample needs to be.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html",
    "href": "Neural-Networks.html",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "10.1 Preface\nRight now this is going to cover lots of different things that feel a little scatter shot. This is because an interview I am prepping for is scant on detail but explicitly mentions a whole host of deep learning stuff.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html#neural-networks",
    "href": "Neural-Networks.html#neural-networks",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "10.2 Neural Networks",
    "text": "10.2 Neural Networks\nSo I am just going to go through the zero to neural network video. One of the important basic concepts is back propagation.\n\nfrom micrograd.engine import Value \nfrom micrograd import nn \n\n\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f\nprint(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\n\n24.7041\n\ng.backward()\n\nprint(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\n\n138.8338\n\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db\n\n645.5773\n\n\nBack propagation is going to start at g and recursively go back and apply the chain rule. We are going to be able to evaluate the derivative of g through each of the nodes c, d, e, f or the inputs of a and b. What this back propagation will tell us is what will end up happening to g if we nudge a or b. So the 138 of is the slope of the growth. Neural networks are simply just little math machines that take input data and the weights of a neural network as inputs into this math machine and output its predicitions.\nNone of the math changes we just end up bundling them to make it more efficient to actually train these things.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html#what-is-a-derivative",
    "href": "Neural-Networks.html#what-is-a-derivative",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "10.3 What is a derivative",
    "text": "10.3 What is a derivative\n\nimport math \nimport numpy as np \nimport matplotlib.pyplot as plt \n\ndef f(x):\n    return 3*x**2-4*x+5\n\n\nxs = np.arange(-5,5, 0.25)\n\nys = f(xs)\n\nplt.plot(xs, ys)\n\n\n\n\n\n\n\n\nSo we are just going to define a scalar value function that takes a single scalar and returns a single scalar. What is the derivative of different points of x for this function. What a derivative is when we slightly increase an input say x how does the function respond with what sensitivity what is the slope how much does it go up or down.\n\nh = 0.001\n\nx = 3.0\n\nf(x + h)\n\n20.014003000000002\n\n(f(x + h) - f(x))/h\n\n14.00300000000243\n\nz = -3\n\n(f(z + h) - f(z))/h\n\n-21.996999999998934\n\n\nas we start fiddling with it we get closer and closer to the true slope. When its at three its about 14. If we do this same excercise with a negative number. In a bit more of a complex case where we have three scalars as inputs and a single output. We are again going to look at the deriviatives for d for the inputs of a, b, and c\n\nh = 0.001\n\na = 2.0 \n\nb = -3\n\nc = 10.0 \n\nd1 = a * b + c \n\n# inputs \n\na += h \n\nd2 = a * b + c\n\nprint(f\"d1 = {d1}, d2 = {d2}, slope = {(d2-d1)/h}\")\n\nd1 = 4.0, d2 = 3.997, slope = -3.0000000000001137\n\n\nHere we are looking at the derivative of with respect to a so we are going to bump it by h. If we did it with b by a little bit\n\nh = 0.001\n\na = 2.0 \n\nb = -3\n\nc = 10.0 \n\nd1 = a * b + c \n\n# inputs \n\nb += h \n\nd2 = a * b + c\n\nprint(f\"d1 = {d1}, d2 = {d2}, slope = {(d2-d1)/h}\")\n\nd1 = 4.0, d2 = 4.002, slope = 1.9999999999997797\n\n\n\n\nCode\nfrom graphviz import Digraph\na = Value(2.0)\n\nb = Value(-3.0)\n\nz = Value(4.0)\n\nc = a + b * z\n\n\nd = c * a\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef trace(root):\n  # builds a set of all nodes and edges in a graph\n  nodes, edges = set(), set()\n  def build(v):\n    if v not in nodes:\n      nodes.add(v)\n      for child in v._prev:\n        edges.add((child, v))\n        build(child)\n  build(root)\n  return nodes, edges\n\ndef draw_dot(root):\n  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n  \n  nodes, edges = trace(root)\n  for n in nodes:\n    uid = str(id(n))\n    # for any value in the graph, create a rectangular ('record') node for it\n    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n    if n._op:\n      # if this value is a result of some operation, create an op node for it\n      dot.node(name = uid + n._op, label = n._op)\n      # and connect this node to it\n      dot.edge(uid + n._op, uid)\n\n  for n1, n2 in edges:\n    # connect n1 to the op node of n2\n    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n  return dot\n\nx = Value(1.0)\ny = (x * 2 + 1).relu()\ny.backward()\ndot = draw_dot(y)\n\n\nWhere this gets a little bitter harder mentally as a political scientist is that there are a lot of these layers. The input of the next layer is effectively a weighted transformation of the previous layer with a nonlinear function applied to it. So it is not neccessarily ‘independent’ in the strictest sense of the term. W\nThe thing about neural networks it will keep track all of all these little steps. Then using backpropagation to see how all these little weights impact the loss function. Backpropagation lets us keep track of what weights actually led us down the wrong path.\nWe use backpropagation to compute partial derivatives of the loss with respect to the weights using the chain rule. Neurons apply activation functions to introduce non-linearity. We don’t control the input data, but we can tune the weights, which are initially random. If doing classification, we often apply softmax in the final layer to produce predicted probabilities. These predictions are compared to true labels via a loss function, which guides weight updates using gradient descent (which uses first-order derivatives, not second-order). The learning rate controls the step size, and we iterate this process over multiple epochs to optimize the model.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html#flavors-of-neural-networks",
    "href": "Neural-Networks.html#flavors-of-neural-networks",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "10.4 Flavors of Neural Networks",
    "text": "10.4 Flavors of Neural Networks\nOne really intersting usage of Neural Networks is the NFL. We have a lot of game film that we need to sort through and do it efficiently. We need to track tendencies which entails us correctly being able to determine coverages and where players are positionally on the field. However, this is a really complex task when we break it out into what we are asking the model to do. For an army of football nerds this is not that difficult it is just time consuming.\nWe have lots of things that are going on we have sequential data in the time, but we also have sequential data in the down and distance which are dependent on one another. We are less likely to see a team pass on 3rd and 1 then we are if they are 3rd and 10. How we got to those down and distances are also dependent on what happened the down before. Play calling is also dictated by the game clock and score. If we are down 6 in the 4th quarter with two minutes left to go with no timeouts running is going to be less likely. How players shift and where they are on the field is also dependent on playclock. Where a player is on offense is going to be dictated by the playclock. If we send a receiver in motion they have to be where they are supposed to be by the time that ball is snapped, and same with defensive players. Using neural networks we can at least make this process more efficient and through careful retraining we can get the model pretty good at identifying and predicting various things.\nWithin this example there are two intermingled tasks that require different neural network architectures. We have a time series classification task to predict what coverages or plays the offense is running. We also have a prediction task where we are trying to predict who is most likely to be the blitzer or who is going to get the ball. These are then intermingled with a computer vision task. While related they require changes in the underlying linear algebra.\n\n10.4.1 Image Processing Tasks\nNeural networks are pretty adept and vision processing tasks. Part of the reason this they are really good at this is because we can use convulations to classify and identify images. Convolutions are really just a filter matrix where we take a matrix of some width and length from the image and then mutliplies it by another matrix which we call a filter. This filter is a smaller part of the image. We then multiply these two matrices to get a score. A high score indicates that the input images is similar to the filter. The filters will be learned via training. So if we take an NFL field we have two directions that the model can learn. North-South where the end zone the offense is trying to get to is North. We also have East-West where the sidelines to the quarterback’s throwing arm could be east if they are a right handed quarterback. We would take these two directions and generate “two” images for the filters. We then multiply the input image matrix by these two different filter matrices. As the model gets more complex we get more of these filters and multiply the input image by more of these filters. So in the next layer we would capture the width of the widest players on the East-West axis and the distance from the receiver and the cornerback on the North-South Axis. We can then differentiate the teams by differences in there jersey colors and the details of the decals as we get further down the layers of the network. Each time we convolve we apply multiply a region by this filter and then sum all those dot product to get a scalar. So we would do something to this effect.\n\ninput_matrix = np.array([[8, 4, 4,2 ,8],\n                         [6, 7, 5,4, 6],\n                         [2, 1, 3,7, 2]])\n\nfilter_matrix = np.array([[1, 0], \n                          [-1, 2]])\n\n\nflipped_filter = np.flipud(np.fliplr(filter_matrix))\n\nregion_one = np.array([[8,4],\n                       [6,7],\n                       [2,1]])\n\n\nconvolution_result = np.sum(np.dot(region_one, flipped_filter))\n\nconvolution_result = np.sum(convolution_result)\nprint(\"\\nConvolution result:\", convolution_result)\n\n\nConvolution result: 28\n\nprint(\"\\nWhats happening:\")\n\n\nWhats happening:\n\nprint(f\"{flipped_filter[0,0]} * {region_one[0,0]} + {flipped_filter[0,1]} * {region_one[0,1]} + \"\n      f\"{flipped_filter[1,0]} * {region_one[1,0]} + {flipped_filter[1,1]} * {region_one[1,1]} = {convolution_result}\")\n\n2 * 8 + -1 * 4 + 0 * 6 + 1 * 7 = 28\n\n\nAfter these convolutions we then pool the convolutions by taking, typically, non-overlapping 2x2 blocks and place them in a matrix with the maximum of those two by two blocks. So if we go ahead and convolve the entire input matrix we get something like this\n\nfrom scipy.signal import convolve2d\n\nconvolution_result = convolve2d(input_matrix, filter_matrix, mode = 'valid')\n\nconvolution_result\n\narray([[19,  9, 10,  2],\n       [ 6, 12, 13,  4]])\n\n\nThen in the pooling result we would get a pooled matrix that would effectively be a matrix with the values of 19 and 13. Except this would be a much much bigger matrix. We then take this slightly smaller matrix and then pass it through another convolution layer with smaller number of filters and so on and so forth until we get a single image. We can do a similar thing with lower quality images where we deconvolve them. This is less that we are reversing the convolution process but we are convolving the inputs like usual but we change the filter a bit.\n\nflip_one = np.flipud(filter_matrix)\n\nflip_two = np.fliplr(flip_one)\n\nprint(f\"Original Filter Matrix:\\n {filter_matrix}\\n\"\n      f\"After Flipping North-South:\\n {flip_one}\\n\",\n      f\"After Flipping East-West: \\n {flip_two}\")\n\nOriginal Filter Matrix:\n [[ 1  0]\n [-1  2]]\nAfter Flipping North-South:\n [[-1  2]\n [ 1  0]]\n After Flipping East-West: \n [[ 2 -1]\n [ 0  1]]\n\n\nWhat we are effectively doing is expanding the search over many grainy pixels rather than focusing on narrower regions of a high quality images. If we have many big pixels this has the advantage of expanding the underlying matrix and in the best case scenarios filling them with the correct values that then get turned back into colors, textures, etc to make a higher quality image.\n\n\n10.4.2 Neural Networks for Time Series Classification\nOne nice feature that people have found is that Neural Networks are actually pretty good at modeling time series for multiclass prediction problems. If we cheat a little bit and add units to this problem in a two class problem this just becomes a survival model. What is nice is that we can model duration dependence by creating duration dummies and then including duration, \\(duration^2\\), and \\(duration^3\\) as variables in our models or splines (Carter and Signorino 2010). This becomes a bit more difficult when we are in a multiclass prediction world, but this would effectively just be a competing risks model.\nIn effect what goes on in a recurrent neural network is that weights are reused between layers by keeping them stored in the hidden layers. In a cannonical NN each layer will generate its own weights independent of the prior layer. We then just use back propagation to track everything and update the weights once we pass everything to through the softmax function. However, in an RNN we store everything we learn in a vector containing information we learned in a prior layer which are then used to learn the weights for the next layer. Then we repeat this process for the next layer.\nTo make this a bit clearer. We start with training our model with the first point in our time series where some combination of inputs values, hyperparameters, and some randomly initialized weights plus an activation function. We then capture the outputs of that layer and store it as a vector. We then effectively use these outputs as a lagged dependent variable to help us predict the next output. This repeats for each time then we use back propagation through time to understand which hidden state is responsible for bad forecasts or classification.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html#generative-models",
    "href": "Neural-Networks.html#generative-models",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "10.5 Generative Models",
    "text": "10.5 Generative Models\n\n10.5.1 Generative Adversarial Networks (GAN)\nThese were interesting and new to me. These are used in a lot of image, video, and audio tasks mostly of the generative variety. So if you think of deep fakes these are done via a GAN in a lot of cases. It is kind of funny because the opening example of one of the most cited reviews is that of a forger and an art expert. So imagine we have a new art expert who is learning the tricks of the trade pitted against a new art forger. The art forger will generate images without access to the reference material and the art expert will authenticate them against real images. This is happening in an iterative process the art forger given an infinite number of iterations will eventually become a good enough forger to slip some images past the art forger.\nWhile they are in competition with each other they are intimately linked. The loss function that both the forger and the expert are trying to minimize are intimately linked. The tasks that are GANs are often tasked with are incredibly difficult. Grounding it in our art example mixing colors correctly and making putting them on the canvas correctly is difficult. Lets take a very simple example where the forger is trying to get the correct color. In 1000 tries to get the colors correctly the forger is going to get some correct but it is going to get a lot wrong\n\nlibrary(tidyverse)\nlibrary(patchwork)\nn = 1000\n\n\nart_forger_examp = tibble(`Fake Color` = rnorm(n, mean = 8),\n                              `Real Color` = rnorm(n, mean = 2)) |&gt;\n                            pivot_longer(everything())\n\nggplot(art_forger_examp, aes(x = value, fill = name)) +\n  geom_density(alpha = 0.5) +\n  labs(fill = NULL, x = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs the number of adversarial interactions increase the forger is going to get better at getting the color correct.\n\niterative_interactions &lt;- function(n, m = 2, m2_start = 8, steps = 5) {\n  mean_fake_values &lt;- seq(m2_start, m, length.out = steps)\n\n  plots &lt;- map(mean_fake_values, function(m2) {\n    plot_dat &lt;- data.frame(\n      `Fake Color` = rnorm(n, mean = m2),\n      `Real Color` = rnorm(n, mean = m)\n    ) |&gt; tidyr::pivot_longer(everything())\n\n    ggplot(plot_dat, aes(x = value, fill = name)) +\n      geom_density(alpha = 0.5) +\n      labs(fill = NULL, x = NULL, title = paste(\"Mean Fake:\", round(m2, 2))) +\n      theme_minimal()\n  })\n\n  # Combine the plots using patchwork\n  wrap_plots(plots, ncol = 2)\n}\n\n# Run simulation for different sample sizes\nsims &lt;- iterative_interactions(n = 10000, steps = 4)\n\n\nsims\n\n\n\n\n\n\n\n\nEffectively as the art forger banks more and more information they will start get better at this. This step repeats for every part of the image.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html#so-how-do-these-work",
    "href": "Neural-Networks.html#so-how-do-these-work",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "10.6 So How Do These Work?",
    "text": "10.6 So How Do These Work?\nFor LLM’s and these generative models we have to look at Attention, Encoders, and decoders the entire architecture is known as a transformer. The foundational paper in these models gets the idea from translations. In gendered languages like French or Spanish is going to point the translator in the wrong direction. Take le homme mange or the man eats in French. Homme doesn’t depend on any other word in the sentence but the and eats depends on man. Le could be la, nous, or ils depending the number of men and mange could mangeons or mangent depending on tense. For a sentence in any language a word is going to attend to another word in the sentence but not all of them.\nIn the model attention works kind of like a key value pair where attention is the dot product of the query and the key. Or in another words\n\\[\n\\text{Attention Weights} = \\text{What the work is looking for in another words} \\times \\text{The actual representation of the word that gets passed along}\n\\]\nWhere we get a score of how much one word should pay attention to another. We then feed it into a sofmax function to effectively spit out weights that can be interpreted as probabilities. So an attention score of 0.000001 means that a word has a low probability of attending to another. So in our French example homme would have a low probability of attending to any other word in the example. For computational purposes we stack these into multiple layers to create a multiheaded attention check. So instead of doing the attention of one word to another sequentially we can compute the relation of each word to each other much quicker.\nAn encoder in these models is really just comprised of N layers of these attention scores and a neural network. The NN is critical to actually compute word embeddings and to calculate where the word is in N dimensions and capture the relationship between words positionally and with respect to the whole context window and relationships in the entire sentence. So in an example sentence of I am really really hungry and haven’t had fried chicken in awhile lets go eat at KFC. The first layer could have a multiheaded attention check and a NN that looks at the relationship between words directly next to each other where we would focus on the sematantic relationship between “had”, “fried”, and “chicken” then the last layer would look at the temporal relationship between awhile and fried chicken. The stack is also important because we are taking the residuals of one layer and using it in the next layer. So the model would “understand” that ‘fried’ and ‘chicken’ have high attention scores. Then this result would be passed onto the next layer so it doesn’t have to relearn the semantic relationship and focus on other semantic relationship. In effect we are looking at a form of boosting where the results of one ‘tree’ impacts the way the next ‘tree’ is going to learn.\nYou can think of the encoder as producing a kind of “cumulative” attention score for each word, reflecting how each word relates to the others. The decoder then takes those cumulative attention scores and applies a second layer of attention—focusing on different parts of the encoder’s output to generate the next part of the output sequence. This makes the decoder’s understanding of the input sequence more nuanced and dynamic.\nAn additional revolution in this literature comes from Devlin et al. (2018) where they add a subtle but powerful augmentation of this transformer architecture. For the most part, the majority of languages read right to left. Instead of simple left-right attention BERT uses bidirectional self-attention letting the model consider prior and preceding words. Self-attention is simply the idea that we focus the attention locally where the word’s meaning is determined within the sentence. Attention would pick up on a larger global definition . So if we think about sports we could have two sentences with dog in them. For example lets take this passage &gt; “Matthew Stafford was a Georgia Bulldog. Matthew Stafford is known for his toughness he once led a comeback of a game where he separated his shoulder, he is a dog. The Staffords have a dog named Jack”\nIf we tasked a model with a Next Sentence Prediction (NSP) task using a standard transformer, it might predict the second sentence based on the global frequency of the word “dog” as the animal, especially if it has not seen enough context to disambiguate the term. This could result in the model misunderstanding that “dog” refers to toughness in the second instance.\nFor example, if we asked the model, “What are some examples of NFL players being dogs?”, the model might produce responses based only on the animal interpretation, because the word “dog” in the input text was not disambiguated properly. However, with self-attention, the model can better capture the contextual differences, focusing on the surrounding words like “toughness” and “comeback” to understand that “dog” here refers to toughness, rather than the literal animal\nThe combination of self-attention and bidirectionality is an inherently powerful idea. However, by combining these layers of complexity this can lead to overfitting. In the context of a NLP task all this means is that we will start to get semantic relationships that don’t make a ton of sense. To guard against this we could, theoretically, impose some regularization on the underlying neural networks. This would likely significantly impact the training time making an already expensive process more expensive. A cheaper alternative is to simply hide some words randomly and then after the encoder layer have the model predict the masked word. This ensures that the model isn’t just using one direction to improve its predictions and we are getting semantically meaningful relationships.\nOpenAI is a major player in the space of large language models (LLMs). Text data is inherently multi-dimensional, and fine-tuning a model for a specific task is both expensive and requires a large corpus of task-specific data, which might not be feasible for smaller datasets. Instead of starting with a task-specific model, OpenAI and other organizations working with LLMs often use a task-agnostic approach.\nThe process starts with feeding the model a diverse corpus of data rather than just task-specific documents. This is effective because GPTs (Generative Pre-trained Transformers) leverage the pretrained knowledge from a wide array of texts. Unlike some earlier models, GPTs do not rely solely on RNNs (Recurrent Neural Networks), but rather on the transformer architecture, which doesn’t store information between layers in the same way RNNs do. Instead, transformers use self-attention mechanisms that enable the model to better capture contextual relationships between words across the entire input sequence.\nDespite not being specifically trained for a task initially, the model can still apply its preexisting, general knowledge learned during pretraining to new, task-specific corpora. This approach allows for more efficient fine-tuning with smaller datasets, as the model is able to build on the knowledge it has already acquired. This approach is related to the Retrieval-Augmented Generation (RAG) framework, where the model can retrieve relevant information from a large corpus and generate task-specific outputs.\nGPT models also have a variety of hyperparameters that influence their behavior. One of the most well-known is temperature, which controls the probability distribution from which words are sampled. Higher temperatures (e.g., 1.0 or above) allow the model to pick words from the tail of the distribution, leading to more diverse but less predictable output. Conversely, a lower temperature (e.g., 0.2 or 0.5) narrows the distribution, making the model more deterministic and focused on high-probability words.\nWhile the temperature is just one of many hyperparameters, there are potentially billions of parameters in the model, each contributing to various aspects of the language and its understanding. These parameters allow the model to grasp complex semantic relationships and context, enhancing its ability to perform diverse tasks.\nA crucial element of OpenAI’s approach is the use of semi-supervised learning, which integrates reinforcement learning into the training process. One way this is applied is through few-shot learning, where the model is provided with a few labeled examples of the task it is being trained to perform. These labeled examples serve as a form of ground truth, which helps guide the model’s predictions.\nIn addition to few-shot learning, fine-tuning is a key process in improving the model’s output for specific tasks. For example, if the model is tasked with generating Python code but the output is inefficient, an expert can provide a more efficient code example to refine the model’s performance. This feedback loop allows the model to continuously improve based on real-world feedback and domain expertise.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html#so-how-does-this-relate-to-generative-models",
    "href": "Neural-Networks.html#so-how-does-this-relate-to-generative-models",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "10.7 So how does this relate to generative models?",
    "text": "10.7 So how does this relate to generative models?\nEffectively we are doing the same thing with images. If we are generating art we are making attention scores with the position of the colors and the location in an image and the orientation of the pixels. Our forger is effectively just as series of encoders and decoders that makes art and our expert is effectively just a series of encoders and decoders that score the image as fake or real by taking the generated image and a real image and comparing the attention scores",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Neural-Networks.html#rag-architecture",
    "href": "Neural-Networks.html#rag-architecture",
    "title": "10  Deep Learning (Mostly Neural Networks)",
    "section": "10.8 RAG architecture",
    "text": "10.8 RAG architecture\nOne of the major things companies are interested in these days is Retrieval Augmented Generation with vector search (RAG). The general idea of RAG is that lets say we have a question about pandas. LLM’s are likely going to have a large pool of examples where they can pull relevant code from. However, lets say pandas updates its API at time + 1 from when the model was last trained and deprecates well known functions in favor of doing something. We are running the model locally and don’t have access to the internet but we have downloaded a copy of the pandas documentation. The model is going to do its best to output the right answer but is never really going to acknowledge that it doesn’t know something so it will ‘hallucinate’ answers. We can use this new document to augment things and get the appropriate answers we want.\nThis is kind of the entire idea of the RAG framework. As a company its not neccessarily cost effective or legal to have Anthropic or OpenAi train a bespoke model or turn over confidential records to train a model on. This data is going to pop up in ways we can’t anticipate. Instead we can use RAG to augment our model.\n\n10.8.1 How does it work\n\\[\n\\text{user} \\overbrace{\\rightarrow}^{\\text{prompt}} \\text{Vector Data Base of Word emeddings} \\overbrace{\\rightarrow}^{\\text{Relevant Sources}} \\text{User prompt meets relevant source} \\rightarrow Model \\rightarrow Output\n\\]\nThe general idea is that we take our relevant documents and convert them into embeddings and take the user’s prompt and convert them to embeddings. We are kind of effectively taking our prompt and a document with a very close semantic relationship and then generate relevant output based on the most comparable document.\nWhat does this look like? Lets take Positron the new IDE by Posit. It is still a pretty young so it is not in the training dataset\n\n\n\n\n\n\nWithout RAG\n\n\n\nUser: What is Positron?\nOutput: The Positron IDE is a lightweight, fast, and customizable code editor designed to provide a simple and intuitive environment for software development. It was originally built as a plugin for Atom, a popular open-source text editor. Positron aimed to combine the best aspects of an editor (like Atom) with a focus on speed and simplicity.\n\n\nNone of this is actually correct in the context of what I was looking for. ChatGpt gave kind of a general definition of an IDE. Lets say that the internal documents were available for Zed and Positron and we wanted to ensure that an LLM would output useful information for our IDE. We could take these documents and then make embeddings then add it to a vector database. Now our query would look something like this\n\n\n\n\n\n\nWith RAG\n\n\n\nUser: What is Positron Model: A next-generation data science IDE built by Posit PBC An extensible, polyglot tool for writing code and exploring data A familiar environment for reproducible authoring and publishing\n\n\n\n\n\n\nCarter, David B, and Curtis S Signorino. 2010. “Back to the Future: Modeling Time Dependence in Binary Data.” Political Analysis 18 (3): 271–92. https://doi.org/10.1093/pan/mpq013.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” CoRR abs/1810.04805. http://arxiv.org/abs/1810.04805.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deep Learning (Mostly Neural Networks)</span>"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#simulating-worlds",
    "href": "Bayesian-Stats.html#simulating-worlds",
    "title": "5  Bayesian Statistics",
    "section": "5.1 Simulating Worlds",
    "text": "5.1 Simulating Worlds\nSimulations are superpowerful because we get to play god in a way that we don’t normally get to do as social scientists. We can really simply simulate that you have roughly a 50/50 chance of getting heads or tails.\n\ncoin_flips = replicate(10000, sample(c('heads', 'tails'), 1))\n\ncoin_flips |&gt;\n    as_tibble() |&gt;\n    group_by(value) |&gt;\n    summarise(counts = n()) |&gt;\n    ungroup() |&gt;\n    mutate(probs = counts/sum(counts))\n\n# A tibble: 2 × 3\n  value counts probs\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 heads   5079 0.508\n2 tails   4921 0.492\n\n\nWhile this is intuitive we can see how this varies by number of flips.\n\nnum_flips = 100\n\nflips = sample(c('heads', 'tails'), size = num_flips, replace = TRUE)\n\ncoin_flips = tibble(\n    heads_frequency = cumsum(flips == 'heads')/1:num_flips,\n    flip_number = 1:num_flips\n\n)\n\n\nggplot(coin_flips, aes(x = flip_number, y = heads_frequency)) +\n    geom_line() +\n    geom_hline(yintercept = 0.5) +\n    scale_y_continuous(limits = c(0,1)) +\n    labs(x = 'Flip Number', y = 'Proportion of Heads') +\n    theme_minimal()\n\n\n\n\n\n\n\n\nIn expectation we start getting closer and closer to 50% heads. Neat, but why should we care? Well we can test how well our prior does on various situations with a known truth.\nLets take a randomized control trial with unobserved confounding using the really excellent DeclareDesign package.\n\noptions(scipen = 999)\nlibrary(DeclareDesign)\n\nrct &lt;-\n  declare_model(N = 100,\n                U = rnorm(N),\n                potential_outcomes(Y ~ 0.2 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\nfake_data = draw_data(rct)\n\nSo now we have some fake data where we can display the ‘truth’ or in this case 0.2\n\ndiagnose_design(rct)\n\n\nResearch design diagnosis based on 500 simulations. Diagnosis completed in 2 secs. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates).\n\n Design Inquiry Estimator Outcome Term N Sims Mean Estimand Mean Estimate\n    rct     ATE estimator       Y    Z    500          0.20          0.21\n                                                     (0.00)        (0.01)\n   Bias SD Estimate   RMSE  Power Coverage\n   0.01        0.20   0.20   0.19     0.95\n (0.01)      (0.01) (0.01) (0.02)   (0.01)\n\n\nWhen we go and estimate it on some fake data we can see how keeping or omitting the unobserved confounding.\n\ncontrol_for_confounding = lm(Y ~ Z + U, data = fake_data)\n\nno_controls = lm(Y ~ Z, data = fake_data)\n\n\nmodelsummary::modelsummary(list('Controls Added' = control_for_confounding,\n                                 'No Controls' = no_controls),\n                          gof_map = 'nobs',\n                          stars = TRUE)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Controls Added\n                No Controls\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -0.000\n                  0.254+\n                \n                \n                  \n                  (0.000)\n                  (0.130)\n                \n                \n                  Z\n                  0.200***\n                  0.128\n                \n                \n                  \n                  (0.000)\n                  (0.185)\n                \n                \n                  U\n                  1.000***\n                  \n                \n                \n                  \n                  (0.000)\n                  \n                \n                \n                  Num.Obs.\n                  100\n                  100\n                \n        \n      \n    \n\n\n\nThis is pretty cool to see how this can go if we don’t account for things that should be accounted for. The same general principle applies in Bayesian analysis. The reason we simulate out a RCT is that they are super expensive! We want to diagnose what could go wrong before we tell our partners what to do. The same idea applies for Bayes because we have two separate problems that can make it hard to tell what is going on. We have computational problems that can arise due to how Bayesian models are fit and we have modeling problems which are really just scientific problems. Conceptually these are somewhat distinct but practically these two run into each other all the time. We can isolate some of the computational mechanics of fitting a bad model on data we know is ‘good’. This makes fitting lots of models easier.\nHow should we simulat the data? Typically we will define a parameter that seems reasonable! So if we are trying measure. So if we were trying to model the impact of a treatment on conversion rate aka how often do we move from a free user to a subscriber setting simulating a uniform distribution across treatment and control.\n\nlibrary(patchwork)\nlibrary(brms)\nlibrary(modelsummary)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\nconversions_tibble = tibble(\n    Control = runif(100, min = 0, max = 100),\n    Treatment = runif(100, min = 0, max = 100),\n) |&gt;\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none') \n\n\n\n\n\n\n\n\nWe wouldn’t expect that by doing nothing that conversion rate is uniformly distributed between 0 and 100 percent. Conversion rate for Netflix or established streaming services is probably closer to something that looks like this.\n\n\nCode\nconversions_tibble_reasonable = tibble(\n    # most in the control group don't convert \n    Control = rbeta(n = 100, shape1 = 3, shape2 = 7) * 10,\n    # people in the control group are slightly more likely to convert\n    Treatment = rbeta(n = 100, shape1 = 6, shape2 = 4) * 10,\n) |&gt;\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble_reasonable, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#checking-our-priors",
    "href": "Bayesian-Stats.html#checking-our-priors",
    "title": "5  Bayesian Statistics",
    "section": "5.2 Checking our priors",
    "text": "5.2 Checking our priors\nThere are lots of ways to check our priors! One way that is fairly common because Gelman reccomends it is prior predictive checks. The first is a prior predictive check which is just a way for us to check on what influence the prior is going to have on the posterior distribution. As our data grows the influence of our prior is going to decrease and the likelihood function is going to start to have more influence on the posterior distribution. Or in other words how probable the observed data is given different value of the model. However, they are not totally irrelevant the prior can impose a form of regularization shrinking the posterior predictions back towards the ‘true value.’ This is to say that even in large-n settings a well calibrated prior is still important in simple models. In much more complex models the prior is going to do a lot more.\nIn a sense we did something similar but what we are doing is modeling the problem only using the priors.\n\ntitanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\ntitanic &lt;- subset(titanic, PClass != \"*\")\n\nf &lt;- Survived ~ SexCode + Age + PClass\n\n\n\nmod_prior &lt;- brm(PClass ~ SexCode + Age,\n    data = titanic,\n     prior = c(\n        prior(normal(0, 3), class = b, dpar = \"mu2nd\"),\n        prior(normal(0, 3), class = b, dpar = \"mu3rd\")),\n    family = categorical(link = logit),\n    sample_prior = \"only\")\n\n\npp_check(mod_prior)   \n\n\n\n\n\n\n\n\nIn this case we the prior does okay matching the observed data. It could definitely be better\n\n\nLets break down some brms syntax first.\n\nprior - somewhat self explantory\nnormal - somewhat self explantory\nclass = b - Set the prior at the population level.\ndpar = 'class' - Here we are telling it that we are making an assumption about the probability of being in second or 3rd class relative to 1st class\n\n\nmod_prior |&gt;\n    modelsummary()\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  b_mu2nd_Intercept\n                  0.966\n                \n                \n                  b_mu3rd_Intercept\n                  1.859\n                \n                \n                  b_mu2nd_SexCode\n                  0.060\n                \n                \n                  b_mu2nd_Age\n                  -0.022\n                \n                \n                  b_mu3rd_SexCode\n                  -0.058\n                \n                \n                  b_mu3rd_Age\n                  -0.055\n                \n                \n                  Num.Obs.\n                  756\n                \n                \n                  ELPD\n                  -111346.9\n                \n                \n                  ELPD s.e.\n                  2853.9\n                \n                \n                  LOOIC\n                  222693.9\n                \n                \n                  LOOIC s.e.\n                  5707.9\n                \n                \n                  WAIC\n                  1622021.9\n                \n        \n      \n    \n\n\n\nOne important thing to note is that brms is based off of lme4 which is one of the premier multilevel modeling packages for frequentist. As a political scientist we aren’t really multilevel model people we are more throw OLS at everything people so some of the terminology around is different.\nIn OLS political science land we would say we include ‘class fixed effects’ to refer to a model that does something like this.\n\n# this is a bad model don't judge \nlogit_fe = glm(Survived ~ Age + SexCode + factor(PClass), data = titanic, family = binomial(link = 'logit'))\n\nmodelsummary(logit_fe)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  1.128\n                \n                \n                  \n                  (0.351)\n                \n                \n                  Age\n                  -0.039\n                \n                \n                  \n                  (0.008)\n                \n                \n                  SexCode\n                  2.631\n                \n                \n                  \n                  (0.202)\n                \n                \n                  factor(PClass)2nd\n                  -1.292\n                \n                \n                  \n                  (0.260)\n                \n                \n                  factor(PClass)3rd\n                  -2.521\n                \n                \n                  \n                  (0.277)\n                \n                \n                  Num.Obs.\n                  756\n                \n                \n                  AIC\n                  705.1\n                \n                \n                  BIC\n                  728.3\n                \n                \n                  Log.Lik.\n                  -347.570\n                \n                \n                  F\n                  50.339\n                \n                \n                  RMSE\n                  0.38\n                \n        \n      \n    \n\n\n\nWhen we say class fixed effects we really just mean including an intercept per classs. In multilevel land what we mean by a fixed effect is really just saying that these effects are constant across groups. So think of the effect of school on classroom performance or the effect of product popularity on ROI on adverstising channels. The way we would replicate the this is something to the effect of\n\nbrm(Survived ~ Age + (1|PClass), data = titanic) |&gt;\n   modelsummary()\n\nAdmittedly also a really bad model but you get the idea.\nWe can then check our priors. We can also set priors on the intercept in our Titanic example if we stripped away age, class, and sex what would our baseline expectation of survival be? In an RCT this is really just what is a reasonable expectation for our control group. In Bayesian analysis we can set priors on lots of things.\n\n\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis Third Edition.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "Causal-Inference.html#observational-methods",
    "href": "Causal-Inference.html#observational-methods",
    "title": "9  Causal Inference",
    "section": "9.5 Observational Methods",
    "text": "9.5 Observational Methods\nThese are certainly less popular then in academic social sciences there are certainly lots of opportunties for example Recast uses an augmented synthetic control model for its lift tests. Netflix uses a lot of observational causal inference methods outside of just A/B tests so its certainly worth going over some of them.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "Causal-Inference.html#difference-in-difference",
    "href": "Causal-Inference.html#difference-in-difference",
    "title": "9  Causal Inference",
    "section": "9.6 Difference in Difference",
    "text": "9.6 Difference in Difference\nDiD dates back to as early as Snow in 1840’s London. The idea is fairly simple something happens that we don’t have any control over causing a change in the system. In the original study the Lambeth water company moved their water supply upstream of London while other water suppliers remained downstream of London. During this time Cholera was running rampant in London and most of the sewers either dumped into the Thames, people dumped things in the Thames, and then just commerical activity. The move kind of came out of nowwhere and was not influenced by any other factor other than the fact that an act of parliament forced them to move the production upstream. So if we wanted to estimate the effect of moving the water source we would follow a fairly simple formula\n\\[\nATE = (\\text{Mean Post-Treatment}_{Treated} - \\text{Mean Pre-Treatment}_{Treated}) - (\\text{Mean Post-Treatment}_{Control} - \\text{Mean Post-Treatment}_{Control})\n\\]\nLets take the mock data from that period and then get the treatment effect.\n\ncholera = tribble(~`Region Supplier`,   ~`Death Rates 1849`,    ~`Death Rates 1854`,\n'Non-Lambeth Only (Dirty)', 134.9,  146.6,\n'Lambeth + Others (Mix Dirty and Clean)',   130.1,  84.9) |&gt;\n    mutate(treatment = ifelse(`Region Supplier` == 'Non-Lambeth Only (Dirty)', 'Control', 'Treated')) |&gt;\n    pivot_longer(cols = !c(`Region Supplier`, treatment)) |&gt;\n    mutate(name = str_extract(name, '\\\\d+')) |&gt;\n    rename(year = name)\n\n(pull(cholera |&gt;\n    filter(treatment == 'Treated', year == '1854')) - pull(cholera |&gt;\n    filter(treatment == 'Treated', year == '1849'))) - (pull(cholera |&gt;\n    filter(treatment == 'Control', year == '1854')) - pull(cholera |&gt;\n    filter(treatment == 'Control', year == '1849')))\n\n[1] -56.9\n\n\nWhich gives us our treatment effect. The reason we have to incorporate all this information simply boils down to the fact is that we need to ensure that we are making the proper comparisions. If we simply did the simple difference in means between the treated group we are not really getting the correct estimate because we can’t discount that the treated group would have improved over time. If we did the means post treatment and pre treatment we can’t discount that this is just a time effect. This is pretty neat and we can recover the ATE as long as we meet some identifying assumptions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#checking-our-models",
    "href": "Bayesian-Stats.html#checking-our-models",
    "title": "5  Bayesian Statistics",
    "section": "5.2 Checking our Models",
    "text": "5.2 Checking our Models\nThere are lots of ways to check our model! One way that is fairly common because Gelman reccomends it is prior predictive checks. The first is a prior predictive check which is just a way for us to check on what influence the prior is going to have on the posterior distribution. As our data grows the influence of our prior is going to decrease and the likelihood function is going to start to have more influence on the posterior distribution. Or in other words how probable the observed data is given different value of the model. However, they are not totally irrelevant the prior can impose a form of regularization shrinking the posterior predictions back towards the ‘true value.’ This is to say that even in large-n settings a well calibrated prior is still important in simple models. In much more complex models the prior is going to do a lot more.\nIn a sense we did something similar but what we are doing is modeling the problem only using the priors.\n\ntitanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\ntitanic &lt;- subset(titanic, PClass != \"*\")\n\nf &lt;- Survived ~ SexCode + Age + PClass\n\n\n\nmod_prior &lt;- brm(PClass ~ SexCode + Age,\n    data = titanic,\n     prior = c(\n        prior(normal(0, 3), class = b, dpar = \"mu2nd\"),\n        prior(normal(0, 3), class = b, dpar = \"mu3rd\")),\n    family = categorical(link = logit),\n    sample_prior = \"only\")\n\n\npp_check(mod_prior)   \n\n\n\n\n\n\n\n\nIn this case we the prior does okay matching the observed data. It could definitely be better\n\n\n\n\n\n\nLets break down some brms syntax first.\n\nprior - somewhat self explantory\nnormal - somewhat self explantory\nclass = b - Set the prior at the population level.\ndpar = 'class' - Here we are telling it that we are making an assumption about the probability of being in second or 3rd class relative to 1st class\n\n\nmod_prior |&gt;\n    modelsummary()\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  b_mu2nd_Intercept\n                  0.966\n                \n                \n                  b_mu3rd_Intercept\n                  1.859\n                \n                \n                  b_mu2nd_SexCode\n                  0.060\n                \n                \n                  b_mu2nd_Age\n                  -0.022\n                \n                \n                  b_mu3rd_SexCode\n                  -0.058\n                \n                \n                  b_mu3rd_Age\n                  -0.055\n                \n                \n                  Num.Obs.\n                  756\n                \n                \n                  ELPD\n                  -111346.9\n                \n                \n                  ELPD s.e.\n                  2853.9\n                \n                \n                  LOOIC\n                  222693.9\n                \n                \n                  LOOIC s.e.\n                  5707.9\n                \n                \n                  WAIC\n                  1622021.9\n                \n        \n      \n    \n\n\n\nOne important thing to note is that brms is based off of lme4 which is one of the premier multilevel modeling packages for frequentist. As a political scientist we aren’t really multilevel model people we are more throw OLS at everything people so some of the terminology around is different.\nIn OLS political science land we would say we include ‘class fixed effects’ to refer to a model that does something like this.\n\n# this is a bad model don't judge \nlogit_fe = glm(Survived ~ Age + SexCode + factor(PClass), data = titanic, family = binomial(link = 'logit'))\n\nmodelsummary(logit_fe)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  1.128\n                \n                \n                  \n                  (0.351)\n                \n                \n                  Age\n                  -0.039\n                \n                \n                  \n                  (0.008)\n                \n                \n                  SexCode\n                  2.631\n                \n                \n                  \n                  (0.202)\n                \n                \n                  factor(PClass)2nd\n                  -1.292\n                \n                \n                  \n                  (0.260)\n                \n                \n                  factor(PClass)3rd\n                  -2.521\n                \n                \n                  \n                  (0.277)\n                \n                \n                  Num.Obs.\n                  756\n                \n                \n                  AIC\n                  705.1\n                \n                \n                  BIC\n                  728.3\n                \n                \n                  Log.Lik.\n                  -347.570\n                \n                \n                  F\n                  50.339\n                \n                \n                  RMSE\n                  0.38\n                \n        \n      \n    \n\n\n\nWhen we say class fixed effects we really just mean including an intercept per classs. In multilevel land what we mean by a fixed effect is really just saying that these effects are constant across groups. So think of the effect of school on classroom performance or the effect of product popularity on ROI on adverstising channels. The way we would replicate the this is something to the effect of\n\nbrm(Survived ~ Age + (1|PClass), data = titanic) |&gt;\n   modelsummary()\n\nAdmittedly also a really bad model but you get the idea.\n\n\n\nWe can then check our priors. We can also set priors on the intercept in our Titanic example if we stripped away age, class, and sex what would our baseline expectation of survival be? In an RCT this is really just what is a reasonable expectation for our control group. For other more complex models we could put priors on the effect of time, space, interactions between groupings and so much more.\n\n\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis Third Edition.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "Causal-Inference.html#identifying-assumptions",
    "href": "Causal-Inference.html#identifying-assumptions",
    "title": "9  Causal Inference",
    "section": "9.7 Identifying Assumptions",
    "text": "9.7 Identifying Assumptions\n\n9.7.1 The parallel trends assumption\nThe parallel trends assumption simply states that if treatment would have never occured we would have seen similar trends post treatment.\n\nlibrary(geomtextpath)\n\nod = causaldata::organ_donations |&gt;\n    mutate(post_treatment = ifelse(Quarter %in% c('Q32011','Q42011','Q12012'), TRUE, FALSE),\n           treated = ifelse(State == 'California', TRUE, FALSE),\n           year = str_extract(Quarter, '\\\\d{4}$'),\n           quarter_num = str_extract(Quarter, 'Q\\\\d{1}'),\n           quarter_num = str_remove(quarter_num, 'Q'),\n           quarter_num = paste0('Q0', quarter_num),\n           quarter = paste0(year,quarter_num),\n           quarter = zoo::as.yearqtr(quarter))\n\npt_trends = od |&gt;\n    group_by(treated, quarter) |&gt;\n    summarise(rate = mean(Rate)) |&gt;\n    ungroup() |&gt;\n    mutate(treated = ifelse(treated == TRUE, 'Treated', 'Control'))\n\n\nggplot(pt_trends, aes(x =  as.factor(quarter),\n                      y = rate,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_textvline(xintercept = '2011 Q3', label = 'Treatment Turns On', linetype = 'dashed') +\nlabs(y = 'Organ Donation Rate', x = NULL) +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\n\n\n\n\n\nFigure 9.1\n\n\n\n\n\nIn Figure 9.1 we are simply plotting the difference between the trajectories of the treated group and the control group. This is nice but what should we look for?\nThere are visual inspections and there are answers that we must come to that are a little bit outside the data but the data can inform our judgements. The key idea of DiD is that our control group is\n\n\n\n\nGelman, Andrew, and John Carlin. 2017. “Some Natural Solutions to the p-Value Communication Problem — and Why They Won ’ t Work ∗ Some Natural Solutions That Won ’ t , on Their Own , Work.” Journal of the American Statistical Association, 1–5. http://www.stat.columbia.edu/~gelman/research/published/jasa_signif_2.pdf.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  }
]