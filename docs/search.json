[
  {
    "objectID": "Causal-Inference.html",
    "href": "Causal-Inference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "In industry there are lots of opportunity for causal inference! Actually in a lot of cases we don’t really realize! Platforms are constantly changing and companies want to evaluate the rollout of new features and whether or not they actually move the needle is important since they are sinking a lot of time and resources into these features. So today we are going to tackle A/B experiments broadly while touching up our understanding of a host of other causal inference methods. One thing that I think is kind of cool is that there is a lot more communication between Academic Social Sciences and the Causal Inference world. We are all dealing with the same problem but some us have more money than the others. However, we can’t run away from of the fundamental problems we face.\n\n\nThe fundamental problem of causal inference is that we don’t have a time machine. By that I mean we would love to have a data frame like this where we can get the individual level causal effect. Meaning we can observe what would happen if we assigned people the treatment and what would happen if people never received the treatment.\n\ntime_machine_world = {\n    'Treated Outcome': [50, 60, 70],\n    'Untreated Outcome': [10, 25, 23],\n\n}\n\npl.DataFrame(time_machine_world).with_columns(\n    (pl.col('Treated Outcome') - pl.col('Untreated Outcome')).alias('Treatment_Effect')\n)\n\n\nshape: (3, 3)\n\n\n\nTreated Outcome\nUntreated Outcome\nTreatment_Effect\n\n\ni64\ni64\ni64\n\n\n\n\n50\n10\n40\n\n\n60\n25\n35\n\n\n70\n23\n47\n\n\n\n\n\n\nTo do this we would need a time machine to go back in time and either give them the treatment or take away the treatment. We only ever observe one state of an individual. We are missing half of the information to get this individual result. Instead we have to come up with a framework that lets us bridge these worlds. The most common framework and kind of the working model that motivates the observational causal inference framework is the Randomized Control Trial/ A/B test framework. We kind of all intuitevely understand this framework. We have a, more or less, exchangeable group of people, villages etc and then we randomize treatment status then take the average of the outcome. Effectively we can think of it as\n\\[\n\\text{Average Treatement Effect} = (\\bar{Y} | \\text{Treated}) - (\\bar{Y} | \\text{Untreated})\n\\]\nIn a DAG framework we can think of it like this.\n\n\n\n\n\n\n\n\n\nNotice that the randomizer is not effected by anything and the only effect of randomization is through the treatment status. Importantly any things that make us special don’t actually determine our potential outcomes. It eliminates a lot of potential avenues for selection or and treatment status is not determined by anything else in our model. This is a pretty powerful and magical idea, but it comes at a cost. In this case we mean money. Just because we can randomize doesn’t mean we will magically find the causal effect of something on something else. In the preceding sections we will spend some time on how to do this and potential areas for trouble.\n\n\n\nTechnically in industry they are called A/B tests because well reasons. The general idea is that this follows the canonical RCT. We have an intervention we want to test so we will run an experiment. This is straightforward idea but we have to make a lot of design considerations. Since experiments have an air of magic to them there is lots of room for them to go wrong. There are lots of statistical avenues to explore including Multi-Armed Bandit, problems staggered rollout, conjoint experiments, and whole host of other things. However, before we start with those its going to be pretty important to talk about power.\nStatistical power is one thing that a variety of designs have in common since they kind of broadly fall under the null hypothesis significance testing framework. Very basically the idea of power comes down to the idea that we want our experiment to have a good shot of rejecting the null hypothesis if the alternative is true. Basically we want to reduce the probability of Type II error or the false negative rate. Importantly when power is low we tend to estimate a treatement effect that is larger than the ‘true’ treatment effect and/or the sign of the effect may be wrong (Gelman and Carlin 2017).\nSo what are the components of statistical power. Political scientists tend to think of power and what we are interested a bit differently then psychologists do. We tend to think of treatment effects on the scale of our oiutcome and through a regression lens. This is important because Psychologist tend to deal with standardized effect sizes and ANOVA/ANCOVA analysis which is just regression in a trench coat, but thats a conversation for another time. Much of the following section is just written notes to Carlise Rainey’s excellent power primer. I should mention that there are bespoke tools to simulate a variety of designs and identifying some potential sticking points. The setup of the paper is that we have a some experiment where we assume a null hypothesis of no treatment effect and an alternative of H1 &gt; 0 implying a positive treatment effect. Although we can extend this to a two-tailed and/or a negative treatment effect.\n\n\nThe main takeaway from this section is that we can think of power as\n\\[\n\\text{Statistical Power} = \\frac{\\text{Treatment}}{\\text{Standard Error}}\n\\]\nLets first start with the intuition. If we could rerun our trial infinite times and collect the treatment effects and count how we frequently we observe the treatment effect we would get our canonical Gaussian distribution. The center of the distribution will be centered over the treatment effect because of the CLT. How far away a single point is from the treatment effect is just the variance. However, if we run the trial a thousand times than that would be the sample treatment effect. The distance measure in the sample is we just refer to as the standard error. To reject the null hypothesis of Treatment Effect &gt; 0 than we would want our sampling distribution to look something like this\n\ndat = data.frame(treat = rnorm(1000, mean = 5, sd = 1))\n\nggplot(dat, aes(x = treat)) +\n    geom_histogram() +\n    theme_minimal() +\n    geom_vline(xintercept = 0) +\n    labs(x = 'Treatment Effect')\n\n\n\n\n\n\n\n\nKind of by extension we are powering the experiment to be able to adjudicate this. What we are saying in words that an experiment has “80% power to detect a treatment effect of ___” if \\(\\text{Treatment effect} - 1.64 \\times \\hat{SE}_{\\hat{\\text{TE}}} &gt; 0\\) in 80% of repeated experiments. Whats noticeable is that we are inserting the critical value of a 90% confidence level in this equation. Obviously as change our significants level then that changes the critical value. If we subsitute in the moment conditions of the normal CDF than we get back to\n\\[\n\\text{Statistical Power} = \\frac{\\text{Treatment}}{\\text{Standard Error}}\n\\]\nEffectively when we are doing a priori analysis we are making an assumption a plausible treatment effect and the variability of that estimate. However, this is pretty challenging because if we knew the treatment effect ahead of time we either simulated the data or we wouldn’t need to spend time and money on an experiment!\n\n\n\nFrom a statistical power perspective there are a few things we can think about. What our best guess is of the effect, the smallest plausible effect, and the smallest substantively meaninful effect. We may have run a gajillion mailer experiments so we may have a pretty good sense of what the effect could be. That would be great but thats not neccessarily generalizeable to the rest of us. Instead we should use our substantive expertise to think about what would be a smallest plausible effect of our treatment or the smallest meaningful effect of our treatment. This will be important later because we are going to use it in our power equation.\n\n\n\nWhen we go to actually estimate our treatment effect we are going to do something like this\n\\[\nOutcome = \\alpha + \\beta \\times Treatment + \\varepsilon\n\\]\nWe make the assumption that \\(\\varepsilon\\) is distributed \\(N(0, \\sigma^2)\\) which gets us the classic standard error of\n\\[\nSE^{Classic}_t = \\sqrt{\\frac{\\sigma^2}{N \\times \\bar{D} (1-\\bar{D})}}\n\\]\nWhere N is our total sample size and D is the fraction assigned to each condition. This \\(\\bar{D}\\) will be important as we start adding conditions. However, lets remain focused on what this tells us. We can further decompose this which gets to the idea that \\(\\sigma\\) is just the standard deviation within the outcome. It makes sense because what we are really doing is stratifying Y by our treatment conditions. To get a good guess of what our \\(\\sigma\\) to plug into our treatment condition is we should find a suitable reference population. The chances are that the company or somebody else is working this area. What this gets us is that we can use features of a reference population to predict a plausible standard error. Where it looks like\n\\[\n\\frac{2 \\times SD(Y)}{\\sqrt{2 \\times N}}\n\\]\nWhere N is the sample size per each condition. What this intuition gets us is that we can think of ways to reduce \\(\\varepsilon\\) by including covariates. This is taking things out of the error term reducing the SE.\n\n\n\nSo this is a long way to get to this idea. If we set an alpha level of 1.90 and a target of 80% power than we know a two things. We know that the arms of either side of a 90% confidence interval are 1.64 standard errors wide thus power is the percent of the sampling distribution that is largert than 1.64 standard errors. To find the MDE we need to solve for the treatment effect that positions 80% of the sampling distribution above 1.64. So after passing it through the inverse of the standard normal CDF and centering it we get the result that for 80% power we can multiply the SE by 2.5 to get the MDE. First, the arms of the 90% confidence interval are 1.64 standard errors wide, so the power of the study is the percent of the sampling distribution that is larger than 1.64 standard errors.\nCarlise uses the example of a ANES experiment where the outcome is a feelings thermometer for respondents PID. If we were to want to run a similar experiment we could take the SD of 20.8 and plug it into our numerator. The reference experiment has two conditions with more or less 500 experiments. So we can back out the minimal detectable effect size like this\n\nsd_ref = 20.8 \n\n\nnumerator = sd_ref * 2 \n\ndenominator = math.sqrt(2 * 500)\n\n\nmde = (numerator / denominator) * 2.5\n\nprint(f'The minimal detectable effect is for 80% power {mde}')\n\nThe minimal detectable effect is for 80% power 3.2887687665751146\n\n\n\n\n\n\nCool we could do some more stuff to bring down the standard error. However, how do we move from mde and SE to a good sample size? When we combine our rules we get something like this\n\\[\n\\text{Sample Size} = 2 \\times (\\frac{2.5 \\times \\hat{SD}(y)}{\\hat{Treatment}})^2\n\\]\nIf we are using an existing studies we can grab the standard errors and\n\n\n\n\nA larger sample increases the statistical power of the evaluation\nIf the effect size of the program is small, the evaluation needs a large sample to achieve a given level of power\nAn evaluation of a program with low take-up needs a large sample\nIf the underlying population has high vartiation in outcomes, the evalution eneds a large sample\nFor a given sample size power is maximized when the sample s equally split between the treatment and control group\nFor a given sample size, randomizing at the cluster level as opposed to the individual level reduces the power of the evaluation. The more similar the outcomes of individuals within clusters are, the larger the sample needs to be.\n\n\n\n\nThese are certainly less popular then in academic social sciences there are certainly lots of opportunties for example Recast uses an augmented synthetic control model for its lift tests. Netflix uses a lot of observational causal inference methods outside of just A/B tests so its certainly worth going over some of them.\n\n\n\nDiD dates back to as early as Snow in 1840’s London. The idea is fairly simple something happens that we don’t have any control over causing a change in the system. In the original study the Lambeth water company moved their water supply upstream of London while other water suppliers remained downstream of London. During this time Cholera was running rampant in London and most of the sewers either dumped into the Thames, people dumped things in the Thames, and then just commerical activity. The move kind of came out of nowwhere and was not influenced by any other factor other than the fact that an act of parliament forced them to move the production upstream. So if we wanted to estimate the effect of moving the water source we would follow a fairly simple formula\n\\[\nATE = (\\text{Mean Post-Treatment}_{Treated} - \\text{Mean Pre-Treatment}_{Treated}) - (\\text{Mean Post-Treatment}_{Control} - \\text{Mean Post-Treatment}_{Control})\n\\]\nLets take the mock data from that period and then get the treatment effect.\n\ncholera = tribble(~`Region Supplier`,   ~`Death Rates 1849`,    ~`Death Rates 1854`,\n'Non-Lambeth Only (Dirty)', 134.9,  146.6,\n'Lambeth + Others (Mix Dirty and Clean)',   130.1,  84.9) |&gt;\n    mutate(treatment = ifelse(`Region Supplier` == 'Non-Lambeth Only (Dirty)', 'Control', 'Treated')) |&gt;\n    pivot_longer(cols = !c(`Region Supplier`, treatment)) |&gt;\n    mutate(name = str_extract(name, '\\\\d+')) |&gt;\n    rename(year = name)\n\n(pull(cholera |&gt;\n    filter(treatment == 'Treated', year == '1854')) - pull(cholera |&gt;\n    filter(treatment == 'Treated', year == '1849'))) - (pull(cholera |&gt;\n    filter(treatment == 'Control', year == '1854')) - pull(cholera |&gt;\n    filter(treatment == 'Control', year == '1849')))\n\n[1] -56.9\n\n\nWhich gives us our treatment effect. The reason we have to incorporate all this information simply boils down to the fact is that we need to ensure that we are making the proper comparisions. If we simply did the simple difference in means between the treated group we are not really getting the correct estimate because we can’t discount that the treated group would have improved over time. If we did the means post treatment and pre treatment we can’t discount that this is just a time effect. This is pretty neat and we can recover the ATE as long as we meet some identifying assumptions.\n\n\n\n\n\n\nOne assumption that gets a little less attention in courses and texts is the no anticipation assumption. This one is a bit harder to test explicitly. You have to have a much more convincing story to justify that this assumption is not violated. In the Jon Snow water case this is a bit more straightforward to for me to think about than the organ donation example. Anticipation in this context may look like people strategically switching who they are getting their water from. If we remember the canoncal Braod Street pump from the Jon Snow story\n\nHistData::SnowMap()\n\n\n\n\n\n\n\n\nThere are a lot of oddities in the Jon Snow data but for now we will just focus on a few cases that are instructive. There are deaths in the dataset that don’t map onto the Broad Street water pump well. Some cases we would suspect are due to other water pumps. However, due to the impressive work done by Jon Snow we know that some of these deaths are school children or workers that drank from the pump on their way home. These cases are not significant for the anticipation assumption.\nThere are several interesting cases where the anticipation assumption could have come into play. One women who used to live near the Broad Street water pump actually really liked the taste of the water and had the water delivered to her house. This resulted in her death and the deaths of some of her nieces. Lets say that with the annoucement of the Parliamentary law people started actively switching who they got their water from because they liked the taste and were worried about the move affecting the taste of the water. If this happens post-treatment no harm no fowl really. If this happens before treatment turns on then we have kind of a big problem. People jumping to get treated or avoid treatment violates our no anticipation assumption.\nHow do we detect this? Well there are still some graphical measures we can leverage. In this case we can use pre-trends plots\n\nlibrary(fixest) \nlibrary(broom)\n\nod = causaldata::organ_donations |&gt;\n    mutate(for_pretends = State == 'California',\n           treated = ifelse(State == 'California' & Quarter %in% c('Q32011','Q42011','Q12012'), TRUE, FALSE),\n           year = str_extract(Quarter, '\\\\d{4}$'),\n           quarter_num = str_extract(Quarter, 'Q\\\\d{1}'),\n           quarter_num = str_remove(quarter_num, 'Q'),\n           quarter_num = paste0('Q0', quarter_num),\n           quarter = paste0(year,quarter_num),\n           quarter = zoo::as.yearqtr(quarter))\n\npre_trends_plots = feols(Rate ~ i(Quarter_Num, for_pretends, ref = 3) | State + Quarter_Num , data = od) \n   \niplot(pre_trends_plots)\n\n\n\n\n\n\n\n\nWhere we can see a significant pretrend. Is this the best way to use an event-study plot? Ehh not really for the most part people tend to use these in support of the parallel trends assumption. We may want to start assigning placebos to see if we can detect an effect prior to treatment. But, this is also us trying to find evidence to the contrary.\n\n\nThe parallel trends assumption is the biggest one that has lots of literature dedicated to it. The parallel trends assumption simply states that if treatment would have never occured we would have seen similar trends post treatment. An unconditional parallel trends assumption is a bit hard to justify so we tend to make a conditonal parallel trends assumption.\n\nlibrary(geomtextpath)\nlibrary(patchwork)\n\nod = causaldata::organ_donations |&gt;\n    mutate(for_pretends = State == 'California',\n           treated = ifelse(State == 'California' & Quarter %in% c('Q32011','Q42011','Q12012'), TRUE, FALSE),\n           year = str_extract(Quarter, '\\\\d{4}$'),\n           quarter_num = str_extract(Quarter, 'Q\\\\d{1}'),\n           quarter_num = str_remove(quarter_num, 'Q'),\n           quarter_num = paste0('Q0', quarter_num),\n           quarter = paste0(year,quarter_num),\n           quarter = zoo::as.yearqtr(quarter))\n\npt_trends = od |&gt;\n    group_by(treated, quarter) |&gt;\n    summarise(rate = mean(Rate)) |&gt;\n    ungroup() |&gt;\n    mutate(treated = ifelse(treated == TRUE, 'Treated', 'Control'))\n\n\nggplot(pt_trends, aes(x =  as.factor(quarter),\n                      y = rate,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_textvline(xintercept = '2011 Q3', label = 'Treatment Turns On', linetype = 'dashed') +\nlabs(y = 'Organ Donation Rate', x = NULL) +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nIn Figure 1 we are simply plotting the difference between the trajectories of the treated group and the control group. This is nice but what should we look for?\nThere are visual inspections and there are answers that we must come to that are a little bit outside the data but the data can inform our judgements. The key idea of DiD is that our control is a valid counterfactual for our treated group. This requires us to do some shoe leather investigation on whether your counterfactuals are plausible. In our organ donation example lets say that by some unforseen miracle lots of states in the control group somehow build high quality transits in major cities and high quality trains between major cities. Maybe less dramatic and possibly more plausible is there is a ‘sin’ tax of sorts on motorcycles making them much more expensive to buy and/or more green spaces like the Beltline are opened providing a free and safe bike path. All these have implications on organ donation rate because there are less situations where cars can collide with eachother, motorcycles, or bikes. What this would imply is that there are less traumatic brain injuries leading to brain death causing a decrease in organ donations. This might result in very dramatic shift in the trends.\n\n\nCode\npt_trends_violations = pt_trends |&gt;\n    mutate(\n           dramatic = ifelse(treated == 'Control' & quarter &gt;= '2011 Q2', (rate/2.25), rate),\n           subtle = ifelse(treated == 'Control' & quarter &gt;= '2011 Q2', rate/1.001, rate))\n\n\nog = ggplot(pt_trends, aes(x =  as.factor(quarter),\n                    y = rate,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3') +\nlabs(y = 'Organ Donation Rate', x = NULL, title = 'Original Plot') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\ndramatic = ggplot(pt_trends_violations, aes(x =  as.factor(quarter),\n                      y = dramatic,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3') +\nlabs(y = NULL, x = NULL, title = 'Dramatic Violation') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\nslight = ggplot(pt_trends_violations, aes(x =  as.factor(quarter),\n                    y = subtle,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3')+\nlabs(y = NULL, x = NULL, title = 'Subtle Violation') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\nog/dramatic/slight\n\n\n\n\n\n\n\n\n\nAs we can see when we reduce the donation rate dramatically visually we see that there is likely a problem with our counterfactual. For the subtle violation it is a lot harder to tell that the control group is changing at a much different rate than the treated group. If we are just evaluating the parallel trends visually this is likely to give us a different impression than we may have initially thought. As a first cut visually a simple 2 X 2 is maybe a good idea, but we might also want to dissegregate the plot. Just because in the aggregate these trends aren’t violated there may be some underlying policy that reduces our pool of candidate counterfactuals. So lets say NY implements congestion pricing reducing traffic into NYC from CT, NYC, and NJ. We may not pick this up but may not show up in our 2x2 plot.\n\nlibrary(ggrepel)\n\n\npt_weirdness = od |&gt;\n    mutate(rate_changed = ifelse(State %in% c('New Jersey', 'New York', 'Connecticut') & quarter &gt;= '2011 Q2', Rate/2.25, Rate),\n          nice_labs = ifelse( State %in% c('New Jersey', 'New York', 'Connecticut', 'California'),State ,NA_character_))\n\n\ntwo_by_two = pt_weirdness |&gt;\n    group_by(treated, quarter) |&gt;\n    summarise(rate = mean(Rate))\n\n\ntwo_by_two_plot = ggplot(two_by_two, aes(\n                    x = as.factor(quarter),\n                    y = rate,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3') +\nlabs(y = 'Organ Donation Rate', x = NULL, title = '2X2 with NYC policy change') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\ndisagg = ggplot(pt_weirdness, aes(x = as.factor(quarter),\n                         y = rate_changed,\n                         color = State,\n                         group = State)) +\n    geom_line() +\n    geom_vline(xintercept = '2011 Q3')+\n    geom_text_repel(data = filter(pt_weirdness, !is.na(nice_labs), quarter == '2012 Q1'),\n                     aes(label = nice_labs), nudge_x = 1) +\n    scale_color_manual(values = c('grey70', 'California' = '#f2ad22', 'New York' = 'darkblue', 'New Jersey'='darkorange', 'Connecticut' = 'darkorchid' )) +\n    labs(y = 'Organ Donation Rate', x = NULL) +\n    theme_minimal() +\n    theme(legend.position = 'none')\n\ntwo_by_two_plot / disagg\n\n\n\n\n\n\n\n\nIn this case we see that the pre-trends in the affected states disappear however when we dissagregate the plot we can clarly see that we have some states that are moving a lot quicker. These states trajectories are not a great counterfactual group to use. One other thing to think about is that traffic rates in CT, and NJ are declining not due to any policy that those states enacted put because New York is affecting their traffic flows suggesting that they may not be candidates anyways. This is definently where a well defined estimand is uber important.\nRecently Rambachan and Roth (2023) proposes a bounds method for testing parallel trends add a more transparent way to test for violations of the PT assumption and a well powered way to do it.\n\n# code taken from vignette \nlibrary(HonestDiD)\n\ndata('LWdata_EventStudy', package = \"HonestDiD\")\n\n# Number of pre-periods\nLW_numPrePeriods = length(LWdata_EventStudy$prePeriodIndices)\nLW_numPostPeriods = length(LWdata_EventStudy$postPeriodIndices)\n\nLW_l_vec = basisVector(15 - (-2), LW_numPostPeriods)\n\n# Construct robust confidence intervals for Delta^{SD}(M) for 15 years of exposure\nLW_DeltaSD_RobustResults = createSensitivityResults(betahat = LWdata_EventStudy$betahat,\n                                                    sigma = LWdata_EventStudy$sigma,\n                                                    numPrePeriods = LW_numPrePeriods,\n                                                    numPostPeriods = LW_numPostPeriods,\n                                                    l_vec = LW_l_vec,\n                                                    Mvec = seq(from = 0, to = 0.04, by = 0.005))\n\n\n# Construct dataframe with OLS confidence interval for theta\nLW_OriginalResults = constructOriginalCS(betahat = LWdata_EventStudy$betahat,\n                                         sigma = LWdata_EventStudy$sigma,\n                                         numPrePeriods = LW_numPrePeriods,\n                                         numPostPeriods = LW_numPostPeriods,\n                                         l_vec = LW_l_vec )\n\n# Construct sensitivity plot\nLW_DeltaSD_SensitivityPlot = createSensitivityPlot( #\n    robustResults = LW_DeltaSD_RobustResults,\n    originalResults = LW_OriginalResults)\n\n\nLW_DeltaSD_SensitivityPlot\n\n\n\n\n\n\n\n\nIn this case we can visualize the effect of different shocks on the outcome of interest. The blue coefficient is really just the OLS estimator and as we increase M we are introducing more and more non-linearity the estimates from the model look a little bit more defenseble if you think that strict linearity is not totally reasonable than woohooo you can defend your estimate a bit more coherently.\n\n\n\n\nThe problem we often run into is that we often don’t have all units turning treatment on at one point. Often times we have some kind of staggered adoption. While not neccessarily a problem we used to solve this with the TWFE estimator, but as Goodman-Bacon (2021) finds that is a problem.\n\n\nCode\n# code taken from Baker, Larcker, and Wang\ndata &lt;- tibble(\n  Y = c(2, 5, 1, 2),\n  Unit = c(\"Treat\", \"Treat\", \"Control\", \"Control\"),\n  T = c(0, 1, 0, 1)\n)\n\n# Make plot 1 - change in trends over time\nplot1a &lt;- data %&gt;% \n  ggplot(aes(x = T, y = Y, group = Unit, color = Unit)) + \n  geom_point() + \n  geom_line() + \n  geom_label(aes(label = Y), hjust = 0.5, vjust = -0.5, color = \"black\") + \n  labs(x = \"\", y = \"Outcome \\n Variable\") + \n  scale_color_manual(values = c(\"#A7473A\", \"#4B5F6C\")) +\n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"Pre\", \"Post\")) + \n  ggtitle('Panel A') + \n  ylim(c(0, 6)) + \n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(hjust = 0.5, vjust = 0.5, angle = 360),\n        legend.title = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust = 0.5)) \n\n# Make plot 2 - remove baseline differences\n# make a dataset that removes first difference\ndata_fd &lt;- data %&gt;% \n  group_by(Unit) %&gt;% \n  mutate(Y = Y - Y[which(T == 0)])\n\n# plot\nplot1b &lt;- data %&gt;% \n  ggplot(aes(x = T, y = Y, group = Unit, color = Unit)) + \n  geom_point(alpha = 1/2) + \n  geom_line(alpha = 1/2, linetype = \"dashed\") + \n  geom_point(data = data_fd, aes(x = T, y = Y, group = Unit, color = Unit)) +\n  geom_line(data = data_fd, aes(x =  T, y = Y, group = Unit, color = Unit)) +\n  annotate(\"label\", x = 1, y = 1, label = \"1\") + \n  annotate(\"label\", x = 1, y = 3, label = \"3\") + \n  # these colors correspond to certain birds native to new zealand. This is the \n  # only asymptotically correct manner to pick color palettes.\n  scale_color_manual(values = c(\"#A7473A\", \"#4B5F6C\")) +\n  labs(x = \"\", y = \"\") + \n  ylim(c(0, 6)) + \n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"Pre\", \"Post\")) + \n  ggtitle('Panel B') + \n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(hjust = 0.5, vjust = 0.5, angle = 360),\n        legend.title = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust = 0.5)) \n\n# Make plot 3 - difference in differences\n# dataset with the first differences\ndata_dd &lt;- tribble(\n  ~Unit, ~Diff,\n  \"Treat\", 3, \n  \"Control\", 1\n)\n\n# plot\nplot1c &lt;- data_dd %&gt;% \n  ggplot(aes(x = Unit, y = Diff, group = Unit, fill = Unit)) + \n  geom_col() + \n  scale_fill_manual(values = c(\"#A7473A\", \"#4B5F6C\")) +\n  labs(x = \"\", y = expression(Delta)) + \n  ylim(0, 3.5) + \n  geom_label(aes(label = Diff), fill = \"white\", vjust = -0.2) + \n  annotate(\"segment\", x = 2, xend = 2, y = 1, yend = 3, color = \"white\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 2, xend = 2, y = 3, yend = 1, color = \"white\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 1.5, xend = 2.5, y = 1, yend = 1, color = \"white\") +\n  annotate(\"label\", x = 2, y = 2, label = \"Treatment Effect \\n = 2\") + \n  ggtitle('Panel C') + \n  labs(y = expression(Delta), x = \"\") + \n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(hjust = 0.5, vjust = 0.5, angle = 360, size = 14),\n        legend.title = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust = 0.5))\n\n# combine subplots\nplot1 &lt;- plot1a + plot1b + plot1c\n\n# save\n\n\n# Plot - GB1 ---------------------------------------------------------\n# Goodman-bacon decomp, overall information\ndata &lt;- tibble(\n  time = 0:100,\n  U = seq(5, 12, length.out = 101),\n  l = seq(10, 17, length.out = 101) + c(rep(0, 85), rep(15, 16)),\n  k = seq(18, 25, length.out = 101) + c(rep(0, 34), rep(10, 67))\n) %&gt;% \n  pivot_longer(-time, names_to = \"series\", values_to = \"value\")\n\n# plot\nGB1 &lt;- data %&gt;% \n  ggplot(aes(x = time, y = value, group = series, color = series, shape = series)) + \n  geom_line(size = 2) + geom_point(size = 2) +\n  geom_vline(xintercept = c(34, 85)) +\n  labs(x = \"Time\", y = \"Units \\n of y\") +\n  scale_x_continuous(limits = c(0, 100), breaks = c(34, 85), \n                     labels = c(expression('t'['k']^'*'), expression('t'['l']^'*')), \n                     expand = c(0, 0)) + \n  annotate(\"text\", x = 10, y = 21, label = expression('y'^'k'), size = 9) +\n  annotate(\"text\", x = 50, y = 16, label = expression('y'^'l'), size = 9) +\n  annotate(\"text\", x = 90, y = 14, label = expression('y'^'U'), size = 9) +\n  annotate('label', x = 17, y = 4, label = 'T1') +\n  annotate('label', x = 60, y = 4, label = 'T2') +\n  annotate('label', x = 93, y = 4, label = 'T3') +\n  annotate(\"segment\", x = 1, xend = 33, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 33, xend = 1, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 35, xend = 84, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 84, xend = 35, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) + \n  annotate(\"segment\", x = 86, xend = 99, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 99, xend = 86, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  scale_y_continuous(limits = c(0, 40), expand = c(0, 0)) +\n  scale_color_manual(values = c(\"#A7473A\", \"#4B5F6C\", \"#51806a\")) + \n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16),\n        legend.position = 'none',\n        plot.background = element_blank(),\n        axis.title.y = element_text(angle = 0, hjust = 0.5, vjust = 0.5),\n        axis.ticks.y = element_blank())\n\n# save  \n\n\n# Plot - GB2 ---------------------------------------------------------\n\n# function to make subplots\nmake_subplot &lt;- function(omit, keep_dates, colors, breaks, break_expressions, series, \n                         series_x, series_y, break_names, break_loc, arrow_start, arrow_stop, title){\n  \n  data %&gt;% \n    filter(series != omit & time &gt;= keep_dates[1] & time &lt;= keep_dates[2]) %&gt;% \n    ggplot(aes(x = time, y = value, group = series, color = series, shape = series)) + geom_line() + geom_point() +\n    geom_vline(xintercept = breaks) + \n    labs(x = \"Time\", y = \"Units \\n of y\") +\n    scale_x_continuous(limits = c(0, 105), breaks = breaks, \n                       labels = break_expressions, \n                       expand = c(0, 0)) + \n    annotate(\"text\", x = series_x[1], y = series_y[1], label = series[1]) +\n    annotate(\"text\", x = series_x[2], y = series_y[2], label = series[2]) +\n    annotate('label', x = break_loc[1], y = 5, label = break_names[1]) +\n    annotate('label', x = break_loc[2], y = 5, label = break_names[2]) +\n    annotate(\"segment\", x = arrow_start[1], xend = arrow_stop[1], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = arrow_stop[1], xend = arrow_start[1], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = arrow_start[2], xend = arrow_stop[2], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = arrow_stop[2], xend = arrow_start[2], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) + \n    scale_y_continuous(limits = c(0, 40), expand = c(0, 0)) +\n    scale_color_manual(values = c(colors[1], colors[2])) +  \n    ggtitle(title) + \n    theme(axis.title = element_text(size = 18),\n          axis.text = element_text(size = 16),\n          legend.position = 'none',\n          plot.background = element_blank(),\n          plot.title = element_text(hjust = 0.5),\n          axis.title.y = element_text(angle = 0, hjust = 0.5, vjust = 0.5),\n          axis.ticks.y = element_blank())\n }\n\n# make the four subplots\np1 &lt;- make_subplot(omit = \"l\", keep_dates = c(0, 100), colors = c('#A7473A', '#51806a'), breaks = 34, \n                   break_expressions = expression('t'['k']^'*'), \n                   series = c(expression('y'^'k'), expression('y'^'U')),\n                   series_x = c(10, 90), series_y = c(23, 16), \n                   break_names = c('T1', 'T2 + T3'), break_loc = c(17, 66), \n                   arrow_start = c(1, 35), arrow_stop = c(33, 99), \n                   title = bquote(paste('A. Early Group vs. Untreated Group')))\n\np2 &lt;- make_subplot(omit = \"k\", keep_dates = c(0, 100), colors = c('#4B5F6C', '#51806a'), breaks = 85, \n                   break_expressions = expression('t'['l']^'*'), \n                   series = c(expression('y'^'l'), expression('y'^'U')),\n                   series_x = c(50, 90), series_y = c(18, 16), \n                   break_names = c('T1 + T2', 'T3'), break_loc = c(50, 95), \n                   arrow_start = c(1, 86), arrow_stop = c(84, 99), \n                   title = bquote(paste('B. Late Group vs. Untreated Group')))\n\np3 &lt;- make_subplot(omit = \"U\", keep_dates = c(0, 84), colors = c('#A7473A', '#4B5F6C'), breaks = c(34, 85), \n                   break_expressions = c(expression('t'['k']^'*'), expression('t'['l']^'*')), \n                   series = c(expression('y'^'k'), expression('y'^'l')),\n                   series_x = c(10, 50), series_y = c(23, 18), \n                   break_names = c('T1', 'T2'), break_loc = c(17, 60), \n                   arrow_start = c(1, 35), arrow_stop = c(33, 84), \n                   title = bquote(paste('C. Early Group vs. Late Group, before ', 't'['l']^'*', sep = \" \")))\n\np4 &lt;- make_subplot(omit = \"U\", keep_dates = c(34, 100), colors = c('#A7473A', '#4B5F6C'), breaks = c(34, 85), \n                   break_expressions = c(expression('t'['k']^'*'), expression('t'['l']^'*')), \n                   series = c(expression('y'^'k'), expression('y'^'l')),\n                   series_x = c(60, 50), series_y = c(36, 18), \n                   break_names = c('T2', 'T3'), break_loc = c(60, 95), \n                   arrow_start = c(35, 86), arrow_stop = c(84, 99), \n                   title = bquote(paste('D. Late Group vs. Early Group, after ', 't'['k']^'*', sep = \" \")))\n\n# combine plots\np1 + p2 + p3 + p4 + plot_layout(nrow = 2)\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nPut simply the TWFE estimator is just where we include intercepts for the time and the spatial unit (typically). There is nothing inherently ‘wrong’ with this the problem is that the ATT we are identifying is not comparing treated and non-treated groups. If we think about this our heads this makes sense! As Figure 2 demonstrates there are a variety of comparisions that the matrix algebra can make. We can compare:\n\nNever Treated vs Early Groups\nEarly Group vs Late Group\nLate Group vs Early Group\nLate Group vs Never Treated\n\nWhat this means is that the \\(\\beta\\) from our is really just weighted average of all these comparisions. What influences the weights is the sample size meaning what share of units are in the treatment group and the comparision group, the subsample variance of the treatment, and the timing. Functionally what is happening is the model is getting confused by what comparisions to make. Lets take another plot from Baker, Larcker, and Wang (2021) where we really great looking PT with staggered treatment timings.\n\n\nCode\nlibrary(lfe)\nlibrary(fastDummies)\nlibrary(ggthemes)\nlibrary(did)\n\n#----------------------------------------------------------------------------\niseed  = 20201221\nnrep &lt;- 100  \ntrue_mu &lt;- 1\nset.seed(iseed)\n\n#----------------------------------------------------------------------------\n## Generate data - treated cohorts consist of 250 obs each, with the treatment effect still = true_mu on average\nmake_data &lt;- function(nobs = 1000, \n                      nstates = 40) {\n  \n  # unit fixed effects (unobservd heterogeneity)\n  unit &lt;- tibble(\n    unit = 1:nobs,\n    # generate state\n    state = sample(1:nstates, nobs, replace = TRUE),\n    unit_fe = rnorm(nobs, state/5, 1),\n    # generate instantaneous treatment effect\n    #mu = rnorm(nobs, true_mu, 0.2)\n    mu = true_mu\n  )\n  \n  # year fixed effects (first part)\n  year &lt;- tibble(\n    year = 1980:2010,\n    year_fe = rnorm(length(year), 0, 1)\n  )\n  \n  # Put the states into treatment groups\n  treat_taus &lt;- tibble(\n    # sample the states randomly\n    state = sample(1:nstates, nstates, replace = FALSE),\n    # place the randomly sampled states into four treatment groups G_g\n    cohort_year = sort(rep(c(1986, 1992, 1998, 2004), 10))\n  )\n  \n  # make main dataset\n  # full interaction of unit X year \n  expand_grid(unit = 1:nobs, year = 1980:2010) %&gt;% \n    left_join(., unit) %&gt;% \n    left_join(., year) %&gt;% \n    left_join(., treat_taus) %&gt;% \n    # make error term and get treatment indicators and treatment effects\n    # Also get cohort specific trends (modify time FE)\n    mutate(error = rnorm(nobs*31, 0, 1),\n           treat = ifelse(year &gt;= cohort_year, 1, 0),\n           tau = ifelse(treat == 1, mu, 0),\n           year_fe = year_fe + 0.1*(year - cohort_year)\n    ) %&gt;% \n    # calculate cumulative treatment effects\n    group_by(unit) %&gt;% \n    mutate(tau_cum = cumsum(tau)) %&gt;% \n    ungroup() %&gt;% \n    # calculate the dep variable\n    mutate(dep_var = (2010 - cohort_year) + unit_fe + year_fe + tau_cum + error)\n  \n}\n#----------------------------------------------------------------------------\n# make data\ndata &lt;- make_data()\n\n# plot\n data %&gt;% \n  ggplot(aes(x = year, y = dep_var, group = unit)) + \n  geom_line(alpha = 1/8, color = \"grey\") + \n  geom_line(data = data %&gt;% \n              group_by(cohort_year, year) %&gt;% \n              summarize(dep_var = mean(dep_var)),\n            aes(x = year, y = dep_var, group = factor(cohort_year),\n                color = factor(cohort_year)),\n            size = 2) + \n  labs(x = \"\", y = \"Value\", color = \"Treatment group   \", title  = \"One draw of the DGP with homogeneous effects across cohorts \\n and with all groups being eventually treated\") + \n  geom_vline(xintercept = 1986, color = '#E41A1C', size = 2) + \n  geom_vline(xintercept = 1992, color = '#377EB8', size = 2) + \n  geom_vline(xintercept = 1998, color = '#4DAF4A', size = 2) + \n  geom_vline(xintercept = 2004, color = '#984EA3', size = 2) + \n  scale_color_brewer(palette = 'Set1') + \n  theme_minimal() +\n  theme(legend.position = 'bottom',\n        #legend.title = element_blank(), \n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12))  +\n  theme(plot.title = element_text(hjust = 0.5, size=12))\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nWe may think to ourselves well we can either our canonical TWFE estimator or some withcraft with leads and lags or some reviewer or stakeholder is going to want to see both so lets do both.\n\n\nCode\nkeepvars &lt;- c(\"`rel_year_-5`\",  \"`rel_year_-4`\",  \"`rel_year_-3`\",  \"`rel_year_-2`\",\n              \"rel_year_0\", \"rel_year_1\", \"rel_year_2\", \"rel_year_3\", \"rel_year_4\", \"rel_year_5\")\n\nrun_ES_DiD &lt;- function(...) {\n  \n  # resimulate the data\n  data &lt;- make_data()\n  \n  # make dummy columns\n  data &lt;- data %&gt;% \n    # make dummies\n    mutate(rel_year = year - cohort_year) %&gt;% \n    dummy_cols(select_columns = \"rel_year\") %&gt;% \n    # generate pre and post dummies\n    mutate(Pre = ifelse(rel_year &lt; -5, 1, 0),\n           Post = ifelse(rel_year &gt; 5, 1, 0))\n  \n  # estimate the model\n  mod &lt;- lfe::felm(dep_var ~ Pre + `rel_year_-5` + `rel_year_-4` + `rel_year_-3` + `rel_year_-2` + \n                `rel_year_0` + `rel_year_1` + `rel_year_2` + `rel_year_3` + `rel_year_4` + \n                `rel_year_5` + Post | unit + year | 0 | state, data = data, exactDOF = TRUE)\n  \n  # grab the obs we need\n  mod2 &lt;- tibble(\n    estimate = mod$coefficients,\n    term1 = rownames(mod$coefficients)\n    )\n  \n es &lt;-\n   mod2 %&gt;% \n    filter(term1 %in% keepvars) %&gt;% \n    mutate(t = c(-5:-2, 0:5)) %&gt;% \n    select(t, estimate)\n es\n}\n\ndata_classical &lt;- map_dfr(1:nrep, run_ES_DiD)\n\ncolors &lt;- c(\"True Effect\" = \"red\", \"Estimated Effect\" = \"blue\")\n\nES_plot_classical &lt;- data_classical %&gt;% \n  group_by(t) %&gt;% \n  summarize(avg = mean(estimate),\n            sd = sd(estimate),\n            lower.ci = avg - 1.96*sd,\n            upper.ci = avg + 1.96*sd) %&gt;% \n  bind_rows(tibble(t = -1, avg = 0, sd = 0, lower.ci = 0, upper.ci = 0)) %&gt;% \n  mutate(true_tau = ifelse(t &gt;= 0, (t + 1)* true_mu, 0)) %&gt;% \n  ggplot(aes(x = t, y = avg)) + \n  #geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), color = 'darkgrey', size = 2) + \n  geom_ribbon(aes(ymin = lower.ci, ymax = upper.ci), color = \"lightgrey\", alpha = 0.2) +\n  geom_point(color = 'blue', size = 3) + \n  geom_line(aes(color = 'Estimated Effect'), size = 1) + \n  geom_line(aes(x = t, y = true_tau, color = 'True Effect'), linetype = \"dashed\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  scale_x_continuous(breaks = -5:5) + \n  labs(x = \"Relative Time\", y = \"Estimate\") + \n  theme(axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12)) +\n  ggtitle(\"TWFE event-study regression with binned end-points\")+\n  scale_color_manual(values = colors) + \n  theme(plot.title = element_text(hjust = 0.5, size=12),\n        legend.position = \"bottom\", \n        legend.title = element_blank())\n\nrun_ES_DiD_sat &lt;- function(...) {\n  \n  # resimulate the data\n  data &lt;- make_data()\n  \n  # make dummy columns\n  data &lt;- data %&gt;% \n    # make relative year indicator\n    mutate(rel_year = year - cohort_year)\n  \n  # get the minimum relative year - we need this to reindex\n  min_year &lt;- min(data$rel_year)\n  \n  # reindex the relative years\n  data &lt;- data %&gt;% \n    mutate(rel_year = rel_year - min_year) %&gt;% \n    dummy_cols(select_columns = \"rel_year\")\n  \n  # make regression formula \n  indics &lt;- paste(\"rel_year\", (1:max(data$rel_year))[-(-1 - min_year)], sep = \"_\", collapse = \" + \")\n  keepvars &lt;- paste(\"rel_year\", c(-5:-2, 0:5) - min_year, sep = \"_\")  \n  formula &lt;- as.formula(paste(\"dep_var ~\", indics, \"| unit + year | 0 | state\"))\n  \n  # run mod\n  mod &lt;- felm(formula, data = data, exactDOF = TRUE)\n  \n  # grab the obs we need\n  mod2 &lt;- tibble(\n    estimate = mod$coefficients,\n    term1 = rownames(mod$coefficients)\n    )\n  \n es &lt;-\n   mod2 %&gt;% \n    filter(term1 %in% keepvars) %&gt;% \n    mutate(t = c(-5:-2, 0:5)) %&gt;% \n    select(t, estimate)\n es\n}\n\ndata_sat &lt;- map_dfr(1:nrep, run_ES_DiD_sat)\n\nES_plot_sat &lt;- data_sat %&gt;% \n  group_by(t) %&gt;% \n  summarize(avg = mean(estimate),\n            sd = sd(estimate),\n            lower.ci = avg - 1.96*sd,\n            upper.ci = avg + 1.96*sd) %&gt;% \n  bind_rows(tibble(t = -1, avg = 0, sd = 0, lower.ci = 0, upper.ci = 0)) %&gt;% \n  mutate(true_tau = ifelse(t &gt;= 0, (t + 1)* true_mu, 0)) %&gt;% \n  ggplot(aes(x = t, y = avg)) + \n  #geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), color = 'darkgrey', size = 2) + \n  geom_ribbon(aes(ymin = lower.ci, ymax = upper.ci), color = \"lightgrey\", alpha = 0.2) +\n  geom_point(color = 'blue', size = 3) + \n   geom_line(aes(color = 'Estimated Effect'), size = 1) + \n   geom_line(aes(x = t, y = true_tau, color = 'True Effect'), linetype = \"dashed\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  scale_x_continuous(breaks = -5:5) + \n  labs(x = \"Relative Time\", y = \"Estimate\") + \n  theme(axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12)) +\n  ggtitle(\"TWFE event-study regression with 'all' leads and lags\")+\n  scale_color_manual(values = colors) + \n  theme(plot.title = element_text(hjust = 0.5, size=12),\n        legend.position = \"bottom\", \n        legend.title = element_blank())\n        \n\nES_plot_sat/ES_plot_classical\n\n\n\n\n\n\n\n\n\nSo what do we do? Thankfully there are huge number of estimators that try to tackle this problem. Unfortunately there are a lot of estimators to choose from. I will focus on Callaway and Sant’Anna (2020) since it is a bit more straight forward and Pedro gives excellent talks. They focus on how we can create rigorous comparisions using a variety of double robust methods, but they allow for comparisions either with the ‘first-treated’ and ‘not yet treated’ or ‘never treated’. This will really depend on your application.\n\n\n\nSynthetic Control(SCM) is one of the youngest of the canonical observational causal inference models. In a way we are mixing some of the best of both worlds with IPW and DiD. So let take the canonical example of the California smoking tax. So in this example CA imposes a cigarette tax kind of out of nowhere and we are interested in the effect of this tax on smoking. The problem is that there are not a lot of clean counterfactuals for CA. This happends for a variety of reasons but the problem arises that many of the candidate estimators not going to produce accurate estimates of the effect of a program.\nAs Athey and Imbens (2017) argues this is one of the major breakthroughs in the last 15 years. Basically if draw out a DAG of pre-treatment controls and they are within the convex hull we can literally just create a fake CA. Convex hull is a mathy way of saying that the some combination of the unaffected units can approximate the pre-intervention charecteristic of the unaffected units. In a very simple example\n\nfake_intervention = tibble(\n    treated_unit = seq(5.6, 6.5, length.out = 17),\n    donor_1 = seq(4.5,4.7, length.out = 17),\n    donor_2 = seq(4.0,4.4, length.out = 17),\n    donor_3 = seq(3.5, 4.9, length.out = 17),\n    time_relative_to_treatment = -12:4) |&gt;\n        pivot_longer(-time_relative_to_treatment) |&gt;\n    mutate(name = str_to_title(str_replace(name, '_', ' ')))\n\nggplot(fake_intervention, aes(x = time_relative_to_treatment, y = value, color = name)) +\n    geom_line() +\n    geom_vline(xintercept = 0, linetype = 'dashed') +\n    geom_hline(yintercept = 5) +\n    geom_hline(yintercept = 3.4) +\n    annotate(geom = 'text',x = -2 , y = 5.1, label = 'Outside Convex Hull') +\n    annotate(geom = 'text',x = -2 , y = 3.3, label = 'Outside Convex Hull') +\n    annotate(geom = 'text',x = -2 , y = 4.5, label = 'Synthetic unit will fall \\n somewhere in here') +\n    scale_x_continuous(limits = c(-12,4), breaks = seq(-12,4, by = 1)) +\n    labs(color = NULL) +\n    theme_classic()\n\n\n\n\n\n\n\n\nWhen applied with the official algorithm that looks like this\n\n\nCode\n# code taken from the tidysynth vignette\n\nlibrary(tidysynth)\ndata('smoking')\n\nsmoking_out &lt;-\n  \n  smoking %&gt;%\n  \n  # initial the synthetic control object\n  synthetic_control(outcome = cigsale, # outcome\n                    unit = state, # unit index in the panel data\n                    time = year, # time index in the panel data\n                    i_unit = \"California\", # unit where the intervention occurred\n                    i_time = 1988, # time period when the intervention occurred\n                    generate_placebos=T # generate placebo synthetic controls (for inference)\n                    ) %&gt;%\n  \n  # Generate the aggregate predictors used to fit the weights\n  \n  # average log income, retail price of cigarettes, and proportion of the\n  # population between 15 and 24 years of age from 1980 - 1988\n  generate_predictor(time_window = 1980:1988,\n                     ln_income = mean(lnincome, na.rm = T),\n                     ret_price = mean(retprice, na.rm = T),\n                     youth = mean(age15to24, na.rm = T)) %&gt;%\n  \n  # average beer consumption in the donor pool from 1984 - 1988\n  generate_predictor(time_window = 1984:1988,\n                     beer_sales = mean(beer, na.rm = T)) %&gt;%\n  \n  # Lagged cigarette sales \n  generate_predictor(time_window = 1975,\n                     cigsale_1975 = cigsale) %&gt;%\n  generate_predictor(time_window = 1980,\n                     cigsale_1980 = cigsale) %&gt;%\n  generate_predictor(time_window = 1988,\n                     cigsale_1988 = cigsale) %&gt;%\n  \n  \n  # Generate the fitted weights for the synthetic control\n  generate_weights(optimization_window = 1970:1988, # time to use in the optimization task\n                   margin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 # optimizer options\n  ) %&gt;%\n  \n  # Generate the synthetic control\n  generate_control()\n\n\nsmoking_out |&gt;\n    plot_trends()\n\n\n\n\n\n\n\n\n\nThere is a bit more to it than my general intuition of ‘this is just a fancy weighted average’ but realistically we are kind of just creating a fancy weighted average for CA. Really this is just a prediction problem where we are using various estimators to produce ‘CA’ where are reducing the distance between real CA and fake CA by the MSPE.\n\\[\n\\begin{aligned}\n\\hat{weights} = \\arg\\min \\sum^{t-1}_{t=1}(Y_{nt} - \\sum^{n-1}_{i=1}weights_{i}Y_{it})^2 \\\\\nwith \\sum^{n-1}_{i=1} \\hat{weights}=1 \\\\\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\hat{weights} \\ge0 \\\\\n\\mu = 0\n\\end{aligned}\n\\]\nWe also put the additional restrictions that the weights are greater than zero and sum to one. We do this to prevent interpolation. Effectively what mu is saying is that we are not letting averages across the group.\nThis is a really cool idea! However, this comes at a really great cost. To create a realistic synthetic version of CA we need lots of periods prior to treatment or we need lots of periods post treatment for the algorithm to get a handle on the trajectory of the trends. This is because the weighted average is trying to leverage past information to balance the trajectory of the synthetic outcome against the treated unit.\nHow do we justify a fancy weighted average? The main motivating justifaction is the factor analytic model where\n$$\n\\[\\begin{aligned}\n\nY_{it} = \\mathit{T}_{it}D_{it} + \\Theta^\\prime_{t}Z_{i} + \\xi_{t} + \\lambda_i^{\\prime}f_t + \\varepsilon_{it}\n\\end{aligned}\\]\n$$\n\\[\n\\begin{aligned}\nY^{0}_it = \\Theta^{\\prime}_t Z_{i} + \\xi_{t} + \\lambda_i^{\\prime}f_t + \\varepsilon_{it} \\\\\nY^{1}_{it} = Y^{0}_it + \\mathit{T}_{it}\n\\end{aligned}\n\\]\nThis allows the control to be a function of time fixed effects a vector of covariates plus some time-invariant confounders to be flexible across time. Broken down a little bit more suppose there are some number of time-varying signals. Each unit will pick up some fixed linear combination of these time varying signals and the differences. Since theese confounders are picked up in the pre-treatment outcomes for both treated and controls we can try to use this information to ‘balance on’ these confounders. We want to learn \\(\\lambda_i^{\\prime}f_t\\) from the pre-treatment controls so this term will be balanced out. If we get a good synthetic unit we can effectively not worry about \\(\\lambda_i^{\\prime}f_t\\) which if left to its own devices will behave/is an unobserved time-varying confounders.\nThe restrictions of weights summing to one, the weights being non-negative, and only dealing with one unit is somewhat of an arbitrary limitation. Innovations on the SCM the Generalized SCM (GSCM) (Xu 2017), the Augmented SCM (ASCM)(Ben-Michael, and and Rothstein 2021), or SynthDiD (Arkhangelsky et al. 2021) in someways relax these constraints.\nRecast uses the Augmented SCM which deals with non-negativity and the convex hull restrictions. We may have a lot of donor units but a limited number of covariates that may result in no unique solution. We could also have fewe control units wiht non-zero weights. These all pose problems for the traditional SCM where we do not have a unique solution. Effectively what happens underneath the hood is we have our SCM model that we estimate our synthetic unit on. If the pre-treatment fit is somewhat poor or is a little bit outside of the convex hull this is an estimation problem rather than a failure of the method. The estimator for the ASCM is\n\\[\nY_{it} = \\sum\\lambda^{scm}_i Y_{it} + \\hat{m}_{it} - \\sum \\lambda^{scm}_i \\hat{m}_{it}\n\\]\nWhere \\(\\hat{m}_{it} - \\sum \\lambda^{scm}_i \\hat{m}_{it}\\) is the augmentation to the original SCM model. Effectively we are adding a small correction to account for estimation problems. In a IPW setting there could be remaining imbalances between the treatment and control groups. To deal with this problem we would simply use G-computation to adjust for these residual imbalances. That is we would estimate the propensity score model with the pre-treatment covariates to predict treatment status and if there are remaining imbalances then we can include them in the outcome modeling to handle these imbalances. We can use a similar logic to improve the fit of our synthetic unit. In statistical learning framework we can kind of think of this as a boosting problem.\nIf the pre-treatment fit is poor we can use the augmented estimator to upweight units that are more predictive of the post-treatment outcomes. The ASCM can also handle cases where we may need weights to be negative and rely on extrapolation to get a good synthetic unit. We can relax the constraint of non-negative weights by applying a ridge penalty to the end of the ASCM model. This will let us control the amount of extrapolation that we allow.\nThe GSCM and SynthDiD view the problem not entirely differently but view SCM and DiD as matrix completion methods. Basically the idea is that based on the structure of our data we want to impute the missing potential outcomes based on what our matrix looks like. If we have a setting where N &gt; T or a ‘tall’ then we would want to use cross-sectional information or vice-versa. The advantage of these approaches is that they are a bridge to the DiD stuff we did earlier and we can incorporate intercept shifts for units or time and can generalize to cases where lots of units are treated, there is staggered treatment timing, or poor pre-treatment fit.\nThese work based on the idea of the interactive fixed effect models where we are really stratifying our model by treated and control. We are losing some information we don’t let the model see the both the treated and control groups during training that way we aren’t worried about negative weighting. The reason this is happening is that the model can’t actually use early adopters as comparisions. So we have a model that tries makes predictions about Y so we get \\(\\hat{Y}\\) and then we take the difference between predicted and actual values.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#the-fundamental-problem",
    "href": "Causal-Inference.html#the-fundamental-problem",
    "title": "Causal Inference",
    "section": "",
    "text": "The fundamental problem of causal inference is that we don’t have a time machine. By that I mean we would love to have a data frame like this where we can get the individual level causal effect. Meaning we can observe what would happen if we assigned people the treatment and what would happen if people never received the treatment.\n\ntime_machine_world = {\n    'Treated Outcome': [50, 60, 70],\n    'Untreated Outcome': [10, 25, 23],\n\n}\n\npl.DataFrame(time_machine_world).with_columns(\n    (pl.col('Treated Outcome') - pl.col('Untreated Outcome')).alias('Treatment_Effect')\n)\n\n\nshape: (3, 3)\n\n\n\nTreated Outcome\nUntreated Outcome\nTreatment_Effect\n\n\ni64\ni64\ni64\n\n\n\n\n50\n10\n40\n\n\n60\n25\n35\n\n\n70\n23\n47\n\n\n\n\n\n\nTo do this we would need a time machine to go back in time and either give them the treatment or take away the treatment. We only ever observe one state of an individual. We are missing half of the information to get this individual result. Instead we have to come up with a framework that lets us bridge these worlds. The most common framework and kind of the working model that motivates the observational causal inference framework is the Randomized Control Trial/ A/B test framework. We kind of all intuitevely understand this framework. We have a, more or less, exchangeable group of people, villages etc and then we randomize treatment status then take the average of the outcome. Effectively we can think of it as\n\\[\n\\text{Average Treatement Effect} = (\\bar{Y} | \\text{Treated}) - (\\bar{Y} | \\text{Untreated})\n\\]\nIn a DAG framework we can think of it like this.\n\n\n\n\n\n\n\n\n\nNotice that the randomizer is not effected by anything and the only effect of randomization is through the treatment status. Importantly any things that make us special don’t actually determine our potential outcomes. It eliminates a lot of potential avenues for selection or and treatment status is not determined by anything else in our model. This is a pretty powerful and magical idea, but it comes at a cost. In this case we mean money. Just because we can randomize doesn’t mean we will magically find the causal effect of something on something else. In the preceding sections we will spend some time on how to do this and potential areas for trouble.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#rcts",
    "href": "Causal-Inference.html#rcts",
    "title": "Causal Inference",
    "section": "",
    "text": "Technically in industry they are called A/B tests because well reasons. The general idea is that this follows the canonical RCT. We have an intervention we want to test so we will run an experiment. This is straightforward idea but we have to make a lot of design considerations. Since experiments have an air of magic to them there is lots of room for them to go wrong. There are lots of statistical avenues to explore including Multi-Armed Bandit, problems staggered rollout, conjoint experiments, and whole host of other things. However, before we start with those its going to be pretty important to talk about power.\nStatistical power is one thing that a variety of designs have in common since they kind of broadly fall under the null hypothesis significance testing framework. Very basically the idea of power comes down to the idea that we want our experiment to have a good shot of rejecting the null hypothesis if the alternative is true. Basically we want to reduce the probability of Type II error or the false negative rate. Importantly when power is low we tend to estimate a treatement effect that is larger than the ‘true’ treatment effect and/or the sign of the effect may be wrong (Gelman and Carlin 2017).\nSo what are the components of statistical power. Political scientists tend to think of power and what we are interested a bit differently then psychologists do. We tend to think of treatment effects on the scale of our oiutcome and through a regression lens. This is important because Psychologist tend to deal with standardized effect sizes and ANOVA/ANCOVA analysis which is just regression in a trench coat, but thats a conversation for another time. Much of the following section is just written notes to Carlise Rainey’s excellent power primer. I should mention that there are bespoke tools to simulate a variety of designs and identifying some potential sticking points. The setup of the paper is that we have a some experiment where we assume a null hypothesis of no treatment effect and an alternative of H1 &gt; 0 implying a positive treatment effect. Although we can extend this to a two-tailed and/or a negative treatment effect.\n\n\nThe main takeaway from this section is that we can think of power as\n\\[\n\\text{Statistical Power} = \\frac{\\text{Treatment}}{\\text{Standard Error}}\n\\]\nLets first start with the intuition. If we could rerun our trial infinite times and collect the treatment effects and count how we frequently we observe the treatment effect we would get our canonical Gaussian distribution. The center of the distribution will be centered over the treatment effect because of the CLT. How far away a single point is from the treatment effect is just the variance. However, if we run the trial a thousand times than that would be the sample treatment effect. The distance measure in the sample is we just refer to as the standard error. To reject the null hypothesis of Treatment Effect &gt; 0 than we would want our sampling distribution to look something like this\n\ndat = data.frame(treat = rnorm(1000, mean = 5, sd = 1))\n\nggplot(dat, aes(x = treat)) +\n    geom_histogram() +\n    theme_minimal() +\n    geom_vline(xintercept = 0) +\n    labs(x = 'Treatment Effect')\n\n\n\n\n\n\n\n\nKind of by extension we are powering the experiment to be able to adjudicate this. What we are saying in words that an experiment has “80% power to detect a treatment effect of ___” if \\(\\text{Treatment effect} - 1.64 \\times \\hat{SE}_{\\hat{\\text{TE}}} &gt; 0\\) in 80% of repeated experiments. Whats noticeable is that we are inserting the critical value of a 90% confidence level in this equation. Obviously as change our significants level then that changes the critical value. If we subsitute in the moment conditions of the normal CDF than we get back to\n\\[\n\\text{Statistical Power} = \\frac{\\text{Treatment}}{\\text{Standard Error}}\n\\]\nEffectively when we are doing a priori analysis we are making an assumption a plausible treatment effect and the variability of that estimate. However, this is pretty challenging because if we knew the treatment effect ahead of time we either simulated the data or we wouldn’t need to spend time and money on an experiment!\n\n\n\nFrom a statistical power perspective there are a few things we can think about. What our best guess is of the effect, the smallest plausible effect, and the smallest substantively meaninful effect. We may have run a gajillion mailer experiments so we may have a pretty good sense of what the effect could be. That would be great but thats not neccessarily generalizeable to the rest of us. Instead we should use our substantive expertise to think about what would be a smallest plausible effect of our treatment or the smallest meaningful effect of our treatment. This will be important later because we are going to use it in our power equation.\n\n\n\nWhen we go to actually estimate our treatment effect we are going to do something like this\n\\[\nOutcome = \\alpha + \\beta \\times Treatment + \\varepsilon\n\\]\nWe make the assumption that \\(\\varepsilon\\) is distributed \\(N(0, \\sigma^2)\\) which gets us the classic standard error of\n\\[\nSE^{Classic}_t = \\sqrt{\\frac{\\sigma^2}{N \\times \\bar{D} (1-\\bar{D})}}\n\\]\nWhere N is our total sample size and D is the fraction assigned to each condition. This \\(\\bar{D}\\) will be important as we start adding conditions. However, lets remain focused on what this tells us. We can further decompose this which gets to the idea that \\(\\sigma\\) is just the standard deviation within the outcome. It makes sense because what we are really doing is stratifying Y by our treatment conditions. To get a good guess of what our \\(\\sigma\\) to plug into our treatment condition is we should find a suitable reference population. The chances are that the company or somebody else is working this area. What this gets us is that we can use features of a reference population to predict a plausible standard error. Where it looks like\n\\[\n\\frac{2 \\times SD(Y)}{\\sqrt{2 \\times N}}\n\\]\nWhere N is the sample size per each condition. What this intuition gets us is that we can think of ways to reduce \\(\\varepsilon\\) by including covariates. This is taking things out of the error term reducing the SE.\n\n\n\nSo this is a long way to get to this idea. If we set an alpha level of 1.90 and a target of 80% power than we know a two things. We know that the arms of either side of a 90% confidence interval are 1.64 standard errors wide thus power is the percent of the sampling distribution that is largert than 1.64 standard errors. To find the MDE we need to solve for the treatment effect that positions 80% of the sampling distribution above 1.64. So after passing it through the inverse of the standard normal CDF and centering it we get the result that for 80% power we can multiply the SE by 2.5 to get the MDE. First, the arms of the 90% confidence interval are 1.64 standard errors wide, so the power of the study is the percent of the sampling distribution that is larger than 1.64 standard errors.\nCarlise uses the example of a ANES experiment where the outcome is a feelings thermometer for respondents PID. If we were to want to run a similar experiment we could take the SD of 20.8 and plug it into our numerator. The reference experiment has two conditions with more or less 500 experiments. So we can back out the minimal detectable effect size like this\n\nsd_ref = 20.8 \n\n\nnumerator = sd_ref * 2 \n\ndenominator = math.sqrt(2 * 500)\n\n\nmde = (numerator / denominator) * 2.5\n\nprint(f'The minimal detectable effect is for 80% power {mde}')\n\nThe minimal detectable effect is for 80% power 3.2887687665751146",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#getting-to-the-sample-size",
    "href": "Causal-Inference.html#getting-to-the-sample-size",
    "title": "Causal Inference",
    "section": "",
    "text": "Cool we could do some more stuff to bring down the standard error. However, how do we move from mde and SE to a good sample size? When we combine our rules we get something like this\n\\[\n\\text{Sample Size} = 2 \\times (\\frac{2.5 \\times \\hat{SD}(y)}{\\hat{Treatment}})^2\n\\]\nIf we are using an existing studies we can grab the standard errors and",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#rules-of-thumb-according-to-jpal",
    "href": "Causal-Inference.html#rules-of-thumb-according-to-jpal",
    "title": "Causal Inference",
    "section": "",
    "text": "A larger sample increases the statistical power of the evaluation\nIf the effect size of the program is small, the evaluation needs a large sample to achieve a given level of power\nAn evaluation of a program with low take-up needs a large sample\nIf the underlying population has high vartiation in outcomes, the evalution eneds a large sample\nFor a given sample size power is maximized when the sample s equally split between the treatment and control group\nFor a given sample size, randomizing at the cluster level as opposed to the individual level reduces the power of the evaluation. The more similar the outcomes of individuals within clusters are, the larger the sample needs to be.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#observational-methods",
    "href": "Causal-Inference.html#observational-methods",
    "title": "Causal Inference",
    "section": "",
    "text": "These are certainly less popular then in academic social sciences there are certainly lots of opportunties for example Recast uses an augmented synthetic control model for its lift tests. Netflix uses a lot of observational causal inference methods outside of just A/B tests so its certainly worth going over some of them.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#difference-in-difference",
    "href": "Causal-Inference.html#difference-in-difference",
    "title": "Causal Inference",
    "section": "",
    "text": "DiD dates back to as early as Snow in 1840’s London. The idea is fairly simple something happens that we don’t have any control over causing a change in the system. In the original study the Lambeth water company moved their water supply upstream of London while other water suppliers remained downstream of London. During this time Cholera was running rampant in London and most of the sewers either dumped into the Thames, people dumped things in the Thames, and then just commerical activity. The move kind of came out of nowwhere and was not influenced by any other factor other than the fact that an act of parliament forced them to move the production upstream. So if we wanted to estimate the effect of moving the water source we would follow a fairly simple formula\n\\[\nATE = (\\text{Mean Post-Treatment}_{Treated} - \\text{Mean Pre-Treatment}_{Treated}) - (\\text{Mean Post-Treatment}_{Control} - \\text{Mean Post-Treatment}_{Control})\n\\]\nLets take the mock data from that period and then get the treatment effect.\n\ncholera = tribble(~`Region Supplier`,   ~`Death Rates 1849`,    ~`Death Rates 1854`,\n'Non-Lambeth Only (Dirty)', 134.9,  146.6,\n'Lambeth + Others (Mix Dirty and Clean)',   130.1,  84.9) |&gt;\n    mutate(treatment = ifelse(`Region Supplier` == 'Non-Lambeth Only (Dirty)', 'Control', 'Treated')) |&gt;\n    pivot_longer(cols = !c(`Region Supplier`, treatment)) |&gt;\n    mutate(name = str_extract(name, '\\\\d+')) |&gt;\n    rename(year = name)\n\n(pull(cholera |&gt;\n    filter(treatment == 'Treated', year == '1854')) - pull(cholera |&gt;\n    filter(treatment == 'Treated', year == '1849'))) - (pull(cholera |&gt;\n    filter(treatment == 'Control', year == '1854')) - pull(cholera |&gt;\n    filter(treatment == 'Control', year == '1849')))\n\n[1] -56.9\n\n\nWhich gives us our treatment effect. The reason we have to incorporate all this information simply boils down to the fact is that we need to ensure that we are making the proper comparisions. If we simply did the simple difference in means between the treated group we are not really getting the correct estimate because we can’t discount that the treated group would have improved over time. If we did the means post treatment and pre treatment we can’t discount that this is just a time effect. This is pretty neat and we can recover the ATE as long as we meet some identifying assumptions.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#no-anticipation",
    "href": "Causal-Inference.html#no-anticipation",
    "title": "Causal Inference",
    "section": "",
    "text": "One assumption that gets a little less attention in courses and texts is the no anticipation assumption. This one is a bit harder to test explicitly. You have to have a much more convincing story to justify that this assumption is not violated. In the Jon Snow water case this is a bit more straightforward to for me to think about than the organ donation example. Anticipation in this context may look like people strategically switching who they are getting their water from. If we remember the canoncal Braod Street pump from the Jon Snow story\n\nHistData::SnowMap()\n\n\n\n\n\n\n\n\nThere are a lot of oddities in the Jon Snow data but for now we will just focus on a few cases that are instructive. There are deaths in the dataset that don’t map onto the Broad Street water pump well. Some cases we would suspect are due to other water pumps. However, due to the impressive work done by Jon Snow we know that some of these deaths are school children or workers that drank from the pump on their way home. These cases are not significant for the anticipation assumption.\nThere are several interesting cases where the anticipation assumption could have come into play. One women who used to live near the Broad Street water pump actually really liked the taste of the water and had the water delivered to her house. This resulted in her death and the deaths of some of her nieces. Lets say that with the annoucement of the Parliamentary law people started actively switching who they got their water from because they liked the taste and were worried about the move affecting the taste of the water. If this happens post-treatment no harm no fowl really. If this happens before treatment turns on then we have kind of a big problem. People jumping to get treated or avoid treatment violates our no anticipation assumption.\nHow do we detect this? Well there are still some graphical measures we can leverage. In this case we can use pre-trends plots\n\nlibrary(fixest) \nlibrary(broom)\n\nod = causaldata::organ_donations |&gt;\n    mutate(for_pretends = State == 'California',\n           treated = ifelse(State == 'California' & Quarter %in% c('Q32011','Q42011','Q12012'), TRUE, FALSE),\n           year = str_extract(Quarter, '\\\\d{4}$'),\n           quarter_num = str_extract(Quarter, 'Q\\\\d{1}'),\n           quarter_num = str_remove(quarter_num, 'Q'),\n           quarter_num = paste0('Q0', quarter_num),\n           quarter = paste0(year,quarter_num),\n           quarter = zoo::as.yearqtr(quarter))\n\npre_trends_plots = feols(Rate ~ i(Quarter_Num, for_pretends, ref = 3) | State + Quarter_Num , data = od) \n   \niplot(pre_trends_plots)\n\n\n\n\n\n\n\n\nWhere we can see a significant pretrend. Is this the best way to use an event-study plot? Ehh not really for the most part people tend to use these in support of the parallel trends assumption. We may want to start assigning placebos to see if we can detect an effect prior to treatment. But, this is also us trying to find evidence to the contrary.\n\n\nThe parallel trends assumption is the biggest one that has lots of literature dedicated to it. The parallel trends assumption simply states that if treatment would have never occured we would have seen similar trends post treatment. An unconditional parallel trends assumption is a bit hard to justify so we tend to make a conditonal parallel trends assumption.\n\nlibrary(geomtextpath)\nlibrary(patchwork)\n\nod = causaldata::organ_donations |&gt;\n    mutate(for_pretends = State == 'California',\n           treated = ifelse(State == 'California' & Quarter %in% c('Q32011','Q42011','Q12012'), TRUE, FALSE),\n           year = str_extract(Quarter, '\\\\d{4}$'),\n           quarter_num = str_extract(Quarter, 'Q\\\\d{1}'),\n           quarter_num = str_remove(quarter_num, 'Q'),\n           quarter_num = paste0('Q0', quarter_num),\n           quarter = paste0(year,quarter_num),\n           quarter = zoo::as.yearqtr(quarter))\n\npt_trends = od |&gt;\n    group_by(treated, quarter) |&gt;\n    summarise(rate = mean(Rate)) |&gt;\n    ungroup() |&gt;\n    mutate(treated = ifelse(treated == TRUE, 'Treated', 'Control'))\n\n\nggplot(pt_trends, aes(x =  as.factor(quarter),\n                      y = rate,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_textvline(xintercept = '2011 Q3', label = 'Treatment Turns On', linetype = 'dashed') +\nlabs(y = 'Organ Donation Rate', x = NULL) +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nIn Figure 1 we are simply plotting the difference between the trajectories of the treated group and the control group. This is nice but what should we look for?\nThere are visual inspections and there are answers that we must come to that are a little bit outside the data but the data can inform our judgements. The key idea of DiD is that our control is a valid counterfactual for our treated group. This requires us to do some shoe leather investigation on whether your counterfactuals are plausible. In our organ donation example lets say that by some unforseen miracle lots of states in the control group somehow build high quality transits in major cities and high quality trains between major cities. Maybe less dramatic and possibly more plausible is there is a ‘sin’ tax of sorts on motorcycles making them much more expensive to buy and/or more green spaces like the Beltline are opened providing a free and safe bike path. All these have implications on organ donation rate because there are less situations where cars can collide with eachother, motorcycles, or bikes. What this would imply is that there are less traumatic brain injuries leading to brain death causing a decrease in organ donations. This might result in very dramatic shift in the trends.\n\n\nCode\npt_trends_violations = pt_trends |&gt;\n    mutate(\n           dramatic = ifelse(treated == 'Control' & quarter &gt;= '2011 Q2', (rate/2.25), rate),\n           subtle = ifelse(treated == 'Control' & quarter &gt;= '2011 Q2', rate/1.001, rate))\n\n\nog = ggplot(pt_trends, aes(x =  as.factor(quarter),\n                    y = rate,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3') +\nlabs(y = 'Organ Donation Rate', x = NULL, title = 'Original Plot') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\ndramatic = ggplot(pt_trends_violations, aes(x =  as.factor(quarter),\n                      y = dramatic,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3') +\nlabs(y = NULL, x = NULL, title = 'Dramatic Violation') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\nslight = ggplot(pt_trends_violations, aes(x =  as.factor(quarter),\n                    y = subtle,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3')+\nlabs(y = NULL, x = NULL, title = 'Subtle Violation') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\nog/dramatic/slight\n\n\n\n\n\n\n\n\n\nAs we can see when we reduce the donation rate dramatically visually we see that there is likely a problem with our counterfactual. For the subtle violation it is a lot harder to tell that the control group is changing at a much different rate than the treated group. If we are just evaluating the parallel trends visually this is likely to give us a different impression than we may have initially thought. As a first cut visually a simple 2 X 2 is maybe a good idea, but we might also want to dissegregate the plot. Just because in the aggregate these trends aren’t violated there may be some underlying policy that reduces our pool of candidate counterfactuals. So lets say NY implements congestion pricing reducing traffic into NYC from CT, NYC, and NJ. We may not pick this up but may not show up in our 2x2 plot.\n\nlibrary(ggrepel)\n\n\npt_weirdness = od |&gt;\n    mutate(rate_changed = ifelse(State %in% c('New Jersey', 'New York', 'Connecticut') & quarter &gt;= '2011 Q2', Rate/2.25, Rate),\n          nice_labs = ifelse( State %in% c('New Jersey', 'New York', 'Connecticut', 'California'),State ,NA_character_))\n\n\ntwo_by_two = pt_weirdness |&gt;\n    group_by(treated, quarter) |&gt;\n    summarise(rate = mean(Rate))\n\n\ntwo_by_two_plot = ggplot(two_by_two, aes(\n                    x = as.factor(quarter),\n                    y = rate,\n                    label = treated, \n                    color = treated,\n                    group = treated)) +\n    geom_textpath(hjust = 0.8) +\n    geom_vline(xintercept = '2011 Q3') +\nlabs(y = 'Organ Donation Rate', x = NULL, title = '2X2 with NYC policy change') +\nscale_color_met_d(name = 'Troy') +\ntheme_minimal() +\ntheme(legend.position = 'none')\n\n\ndisagg = ggplot(pt_weirdness, aes(x = as.factor(quarter),\n                         y = rate_changed,\n                         color = State,\n                         group = State)) +\n    geom_line() +\n    geom_vline(xintercept = '2011 Q3')+\n    geom_text_repel(data = filter(pt_weirdness, !is.na(nice_labs), quarter == '2012 Q1'),\n                     aes(label = nice_labs), nudge_x = 1) +\n    scale_color_manual(values = c('grey70', 'California' = '#f2ad22', 'New York' = 'darkblue', 'New Jersey'='darkorange', 'Connecticut' = 'darkorchid' )) +\n    labs(y = 'Organ Donation Rate', x = NULL) +\n    theme_minimal() +\n    theme(legend.position = 'none')\n\ntwo_by_two_plot / disagg\n\n\n\n\n\n\n\n\nIn this case we see that the pre-trends in the affected states disappear however when we dissagregate the plot we can clarly see that we have some states that are moving a lot quicker. These states trajectories are not a great counterfactual group to use. One other thing to think about is that traffic rates in CT, and NJ are declining not due to any policy that those states enacted put because New York is affecting their traffic flows suggesting that they may not be candidates anyways. This is definently where a well defined estimand is uber important.\nRecently Rambachan and Roth (2023) proposes a bounds method for testing parallel trends add a more transparent way to test for violations of the PT assumption and a well powered way to do it.\n\n# code taken from vignette \nlibrary(HonestDiD)\n\ndata('LWdata_EventStudy', package = \"HonestDiD\")\n\n# Number of pre-periods\nLW_numPrePeriods = length(LWdata_EventStudy$prePeriodIndices)\nLW_numPostPeriods = length(LWdata_EventStudy$postPeriodIndices)\n\nLW_l_vec = basisVector(15 - (-2), LW_numPostPeriods)\n\n# Construct robust confidence intervals for Delta^{SD}(M) for 15 years of exposure\nLW_DeltaSD_RobustResults = createSensitivityResults(betahat = LWdata_EventStudy$betahat,\n                                                    sigma = LWdata_EventStudy$sigma,\n                                                    numPrePeriods = LW_numPrePeriods,\n                                                    numPostPeriods = LW_numPostPeriods,\n                                                    l_vec = LW_l_vec,\n                                                    Mvec = seq(from = 0, to = 0.04, by = 0.005))\n\n\n# Construct dataframe with OLS confidence interval for theta\nLW_OriginalResults = constructOriginalCS(betahat = LWdata_EventStudy$betahat,\n                                         sigma = LWdata_EventStudy$sigma,\n                                         numPrePeriods = LW_numPrePeriods,\n                                         numPostPeriods = LW_numPostPeriods,\n                                         l_vec = LW_l_vec )\n\n# Construct sensitivity plot\nLW_DeltaSD_SensitivityPlot = createSensitivityPlot( #\n    robustResults = LW_DeltaSD_RobustResults,\n    originalResults = LW_OriginalResults)\n\n\nLW_DeltaSD_SensitivityPlot\n\n\n\n\n\n\n\n\nIn this case we can visualize the effect of different shocks on the outcome of interest. The blue coefficient is really just the OLS estimator and as we increase M we are introducing more and more non-linearity the estimates from the model look a little bit more defenseble if you think that strict linearity is not totally reasonable than woohooo you can defend your estimate a bit more coherently.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#staggered-adoption",
    "href": "Causal-Inference.html#staggered-adoption",
    "title": "Causal Inference",
    "section": "",
    "text": "The problem we often run into is that we often don’t have all units turning treatment on at one point. Often times we have some kind of staggered adoption. While not neccessarily a problem we used to solve this with the TWFE estimator, but as Goodman-Bacon (2021) finds that is a problem.\n\n\nCode\n# code taken from Baker, Larcker, and Wang\ndata &lt;- tibble(\n  Y = c(2, 5, 1, 2),\n  Unit = c(\"Treat\", \"Treat\", \"Control\", \"Control\"),\n  T = c(0, 1, 0, 1)\n)\n\n# Make plot 1 - change in trends over time\nplot1a &lt;- data %&gt;% \n  ggplot(aes(x = T, y = Y, group = Unit, color = Unit)) + \n  geom_point() + \n  geom_line() + \n  geom_label(aes(label = Y), hjust = 0.5, vjust = -0.5, color = \"black\") + \n  labs(x = \"\", y = \"Outcome \\n Variable\") + \n  scale_color_manual(values = c(\"#A7473A\", \"#4B5F6C\")) +\n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"Pre\", \"Post\")) + \n  ggtitle('Panel A') + \n  ylim(c(0, 6)) + \n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(hjust = 0.5, vjust = 0.5, angle = 360),\n        legend.title = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust = 0.5)) \n\n# Make plot 2 - remove baseline differences\n# make a dataset that removes first difference\ndata_fd &lt;- data %&gt;% \n  group_by(Unit) %&gt;% \n  mutate(Y = Y - Y[which(T == 0)])\n\n# plot\nplot1b &lt;- data %&gt;% \n  ggplot(aes(x = T, y = Y, group = Unit, color = Unit)) + \n  geom_point(alpha = 1/2) + \n  geom_line(alpha = 1/2, linetype = \"dashed\") + \n  geom_point(data = data_fd, aes(x = T, y = Y, group = Unit, color = Unit)) +\n  geom_line(data = data_fd, aes(x =  T, y = Y, group = Unit, color = Unit)) +\n  annotate(\"label\", x = 1, y = 1, label = \"1\") + \n  annotate(\"label\", x = 1, y = 3, label = \"3\") + \n  # these colors correspond to certain birds native to new zealand. This is the \n  # only asymptotically correct manner to pick color palettes.\n  scale_color_manual(values = c(\"#A7473A\", \"#4B5F6C\")) +\n  labs(x = \"\", y = \"\") + \n  ylim(c(0, 6)) + \n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"Pre\", \"Post\")) + \n  ggtitle('Panel B') + \n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(hjust = 0.5, vjust = 0.5, angle = 360),\n        legend.title = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust = 0.5)) \n\n# Make plot 3 - difference in differences\n# dataset with the first differences\ndata_dd &lt;- tribble(\n  ~Unit, ~Diff,\n  \"Treat\", 3, \n  \"Control\", 1\n)\n\n# plot\nplot1c &lt;- data_dd %&gt;% \n  ggplot(aes(x = Unit, y = Diff, group = Unit, fill = Unit)) + \n  geom_col() + \n  scale_fill_manual(values = c(\"#A7473A\", \"#4B5F6C\")) +\n  labs(x = \"\", y = expression(Delta)) + \n  ylim(0, 3.5) + \n  geom_label(aes(label = Diff), fill = \"white\", vjust = -0.2) + \n  annotate(\"segment\", x = 2, xend = 2, y = 1, yend = 3, color = \"white\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 2, xend = 2, y = 3, yend = 1, color = \"white\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 1.5, xend = 2.5, y = 1, yend = 1, color = \"white\") +\n  annotate(\"label\", x = 2, y = 2, label = \"Treatment Effect \\n = 2\") + \n  ggtitle('Panel C') + \n  labs(y = expression(Delta), x = \"\") + \n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(hjust = 0.5, vjust = 0.5, angle = 360, size = 14),\n        legend.title = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(hjust = 0.5))\n\n# combine subplots\nplot1 &lt;- plot1a + plot1b + plot1c\n\n# save\n\n\n# Plot - GB1 ---------------------------------------------------------\n# Goodman-bacon decomp, overall information\ndata &lt;- tibble(\n  time = 0:100,\n  U = seq(5, 12, length.out = 101),\n  l = seq(10, 17, length.out = 101) + c(rep(0, 85), rep(15, 16)),\n  k = seq(18, 25, length.out = 101) + c(rep(0, 34), rep(10, 67))\n) %&gt;% \n  pivot_longer(-time, names_to = \"series\", values_to = \"value\")\n\n# plot\nGB1 &lt;- data %&gt;% \n  ggplot(aes(x = time, y = value, group = series, color = series, shape = series)) + \n  geom_line(size = 2) + geom_point(size = 2) +\n  geom_vline(xintercept = c(34, 85)) +\n  labs(x = \"Time\", y = \"Units \\n of y\") +\n  scale_x_continuous(limits = c(0, 100), breaks = c(34, 85), \n                     labels = c(expression('t'['k']^'*'), expression('t'['l']^'*')), \n                     expand = c(0, 0)) + \n  annotate(\"text\", x = 10, y = 21, label = expression('y'^'k'), size = 9) +\n  annotate(\"text\", x = 50, y = 16, label = expression('y'^'l'), size = 9) +\n  annotate(\"text\", x = 90, y = 14, label = expression('y'^'U'), size = 9) +\n  annotate('label', x = 17, y = 4, label = 'T1') +\n  annotate('label', x = 60, y = 4, label = 'T2') +\n  annotate('label', x = 93, y = 4, label = 'T3') +\n  annotate(\"segment\", x = 1, xend = 33, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 33, xend = 1, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 35, xend = 84, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 84, xend = 35, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) + \n  annotate(\"segment\", x = 86, xend = 99, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  annotate(\"segment\", x = 99, xend = 86, y = 2, yend = 2, color = \"black\", \n           arrow = arrow(length = unit(0.1, \"inches\"))) +\n  scale_y_continuous(limits = c(0, 40), expand = c(0, 0)) +\n  scale_color_manual(values = c(\"#A7473A\", \"#4B5F6C\", \"#51806a\")) + \n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 16),\n        legend.position = 'none',\n        plot.background = element_blank(),\n        axis.title.y = element_text(angle = 0, hjust = 0.5, vjust = 0.5),\n        axis.ticks.y = element_blank())\n\n# save  \n\n\n# Plot - GB2 ---------------------------------------------------------\n\n# function to make subplots\nmake_subplot &lt;- function(omit, keep_dates, colors, breaks, break_expressions, series, \n                         series_x, series_y, break_names, break_loc, arrow_start, arrow_stop, title){\n  \n  data %&gt;% \n    filter(series != omit & time &gt;= keep_dates[1] & time &lt;= keep_dates[2]) %&gt;% \n    ggplot(aes(x = time, y = value, group = series, color = series, shape = series)) + geom_line() + geom_point() +\n    geom_vline(xintercept = breaks) + \n    labs(x = \"Time\", y = \"Units \\n of y\") +\n    scale_x_continuous(limits = c(0, 105), breaks = breaks, \n                       labels = break_expressions, \n                       expand = c(0, 0)) + \n    annotate(\"text\", x = series_x[1], y = series_y[1], label = series[1]) +\n    annotate(\"text\", x = series_x[2], y = series_y[2], label = series[2]) +\n    annotate('label', x = break_loc[1], y = 5, label = break_names[1]) +\n    annotate('label', x = break_loc[2], y = 5, label = break_names[2]) +\n    annotate(\"segment\", x = arrow_start[1], xend = arrow_stop[1], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = arrow_stop[1], xend = arrow_start[1], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = arrow_start[2], xend = arrow_stop[2], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"segment\", x = arrow_stop[2], xend = arrow_start[2], y = 2, yend = 2, color = \"black\", \n             arrow = arrow(length = unit(0.1, \"inches\"))) + \n    scale_y_continuous(limits = c(0, 40), expand = c(0, 0)) +\n    scale_color_manual(values = c(colors[1], colors[2])) +  \n    ggtitle(title) + \n    theme(axis.title = element_text(size = 18),\n          axis.text = element_text(size = 16),\n          legend.position = 'none',\n          plot.background = element_blank(),\n          plot.title = element_text(hjust = 0.5),\n          axis.title.y = element_text(angle = 0, hjust = 0.5, vjust = 0.5),\n          axis.ticks.y = element_blank())\n }\n\n# make the four subplots\np1 &lt;- make_subplot(omit = \"l\", keep_dates = c(0, 100), colors = c('#A7473A', '#51806a'), breaks = 34, \n                   break_expressions = expression('t'['k']^'*'), \n                   series = c(expression('y'^'k'), expression('y'^'U')),\n                   series_x = c(10, 90), series_y = c(23, 16), \n                   break_names = c('T1', 'T2 + T3'), break_loc = c(17, 66), \n                   arrow_start = c(1, 35), arrow_stop = c(33, 99), \n                   title = bquote(paste('A. Early Group vs. Untreated Group')))\n\np2 &lt;- make_subplot(omit = \"k\", keep_dates = c(0, 100), colors = c('#4B5F6C', '#51806a'), breaks = 85, \n                   break_expressions = expression('t'['l']^'*'), \n                   series = c(expression('y'^'l'), expression('y'^'U')),\n                   series_x = c(50, 90), series_y = c(18, 16), \n                   break_names = c('T1 + T2', 'T3'), break_loc = c(50, 95), \n                   arrow_start = c(1, 86), arrow_stop = c(84, 99), \n                   title = bquote(paste('B. Late Group vs. Untreated Group')))\n\np3 &lt;- make_subplot(omit = \"U\", keep_dates = c(0, 84), colors = c('#A7473A', '#4B5F6C'), breaks = c(34, 85), \n                   break_expressions = c(expression('t'['k']^'*'), expression('t'['l']^'*')), \n                   series = c(expression('y'^'k'), expression('y'^'l')),\n                   series_x = c(10, 50), series_y = c(23, 18), \n                   break_names = c('T1', 'T2'), break_loc = c(17, 60), \n                   arrow_start = c(1, 35), arrow_stop = c(33, 84), \n                   title = bquote(paste('C. Early Group vs. Late Group, before ', 't'['l']^'*', sep = \" \")))\n\np4 &lt;- make_subplot(omit = \"U\", keep_dates = c(34, 100), colors = c('#A7473A', '#4B5F6C'), breaks = c(34, 85), \n                   break_expressions = c(expression('t'['k']^'*'), expression('t'['l']^'*')), \n                   series = c(expression('y'^'k'), expression('y'^'l')),\n                   series_x = c(60, 50), series_y = c(36, 18), \n                   break_names = c('T2', 'T3'), break_loc = c(60, 95), \n                   arrow_start = c(35, 86), arrow_stop = c(84, 99), \n                   title = bquote(paste('D. Late Group vs. Early Group, after ', 't'['k']^'*', sep = \" \")))\n\n# combine plots\np1 + p2 + p3 + p4 + plot_layout(nrow = 2)\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nPut simply the TWFE estimator is just where we include intercepts for the time and the spatial unit (typically). There is nothing inherently ‘wrong’ with this the problem is that the ATT we are identifying is not comparing treated and non-treated groups. If we think about this our heads this makes sense! As Figure 2 demonstrates there are a variety of comparisions that the matrix algebra can make. We can compare:\n\nNever Treated vs Early Groups\nEarly Group vs Late Group\nLate Group vs Early Group\nLate Group vs Never Treated\n\nWhat this means is that the \\(\\beta\\) from our is really just weighted average of all these comparisions. What influences the weights is the sample size meaning what share of units are in the treatment group and the comparision group, the subsample variance of the treatment, and the timing. Functionally what is happening is the model is getting confused by what comparisions to make. Lets take another plot from Baker, Larcker, and Wang (2021) where we really great looking PT with staggered treatment timings.\n\n\nCode\nlibrary(lfe)\nlibrary(fastDummies)\nlibrary(ggthemes)\nlibrary(did)\n\n#----------------------------------------------------------------------------\niseed  = 20201221\nnrep &lt;- 100  \ntrue_mu &lt;- 1\nset.seed(iseed)\n\n#----------------------------------------------------------------------------\n## Generate data - treated cohorts consist of 250 obs each, with the treatment effect still = true_mu on average\nmake_data &lt;- function(nobs = 1000, \n                      nstates = 40) {\n  \n  # unit fixed effects (unobservd heterogeneity)\n  unit &lt;- tibble(\n    unit = 1:nobs,\n    # generate state\n    state = sample(1:nstates, nobs, replace = TRUE),\n    unit_fe = rnorm(nobs, state/5, 1),\n    # generate instantaneous treatment effect\n    #mu = rnorm(nobs, true_mu, 0.2)\n    mu = true_mu\n  )\n  \n  # year fixed effects (first part)\n  year &lt;- tibble(\n    year = 1980:2010,\n    year_fe = rnorm(length(year), 0, 1)\n  )\n  \n  # Put the states into treatment groups\n  treat_taus &lt;- tibble(\n    # sample the states randomly\n    state = sample(1:nstates, nstates, replace = FALSE),\n    # place the randomly sampled states into four treatment groups G_g\n    cohort_year = sort(rep(c(1986, 1992, 1998, 2004), 10))\n  )\n  \n  # make main dataset\n  # full interaction of unit X year \n  expand_grid(unit = 1:nobs, year = 1980:2010) %&gt;% \n    left_join(., unit) %&gt;% \n    left_join(., year) %&gt;% \n    left_join(., treat_taus) %&gt;% \n    # make error term and get treatment indicators and treatment effects\n    # Also get cohort specific trends (modify time FE)\n    mutate(error = rnorm(nobs*31, 0, 1),\n           treat = ifelse(year &gt;= cohort_year, 1, 0),\n           tau = ifelse(treat == 1, mu, 0),\n           year_fe = year_fe + 0.1*(year - cohort_year)\n    ) %&gt;% \n    # calculate cumulative treatment effects\n    group_by(unit) %&gt;% \n    mutate(tau_cum = cumsum(tau)) %&gt;% \n    ungroup() %&gt;% \n    # calculate the dep variable\n    mutate(dep_var = (2010 - cohort_year) + unit_fe + year_fe + tau_cum + error)\n  \n}\n#----------------------------------------------------------------------------\n# make data\ndata &lt;- make_data()\n\n# plot\n data %&gt;% \n  ggplot(aes(x = year, y = dep_var, group = unit)) + \n  geom_line(alpha = 1/8, color = \"grey\") + \n  geom_line(data = data %&gt;% \n              group_by(cohort_year, year) %&gt;% \n              summarize(dep_var = mean(dep_var)),\n            aes(x = year, y = dep_var, group = factor(cohort_year),\n                color = factor(cohort_year)),\n            size = 2) + \n  labs(x = \"\", y = \"Value\", color = \"Treatment group   \", title  = \"One draw of the DGP with homogeneous effects across cohorts \\n and with all groups being eventually treated\") + \n  geom_vline(xintercept = 1986, color = '#E41A1C', size = 2) + \n  geom_vline(xintercept = 1992, color = '#377EB8', size = 2) + \n  geom_vline(xintercept = 1998, color = '#4DAF4A', size = 2) + \n  geom_vline(xintercept = 2004, color = '#984EA3', size = 2) + \n  scale_color_brewer(palette = 'Set1') + \n  theme_minimal() +\n  theme(legend.position = 'bottom',\n        #legend.title = element_blank(), \n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12))  +\n  theme(plot.title = element_text(hjust = 0.5, size=12))\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nWe may think to ourselves well we can either our canonical TWFE estimator or some withcraft with leads and lags or some reviewer or stakeholder is going to want to see both so lets do both.\n\n\nCode\nkeepvars &lt;- c(\"`rel_year_-5`\",  \"`rel_year_-4`\",  \"`rel_year_-3`\",  \"`rel_year_-2`\",\n              \"rel_year_0\", \"rel_year_1\", \"rel_year_2\", \"rel_year_3\", \"rel_year_4\", \"rel_year_5\")\n\nrun_ES_DiD &lt;- function(...) {\n  \n  # resimulate the data\n  data &lt;- make_data()\n  \n  # make dummy columns\n  data &lt;- data %&gt;% \n    # make dummies\n    mutate(rel_year = year - cohort_year) %&gt;% \n    dummy_cols(select_columns = \"rel_year\") %&gt;% \n    # generate pre and post dummies\n    mutate(Pre = ifelse(rel_year &lt; -5, 1, 0),\n           Post = ifelse(rel_year &gt; 5, 1, 0))\n  \n  # estimate the model\n  mod &lt;- lfe::felm(dep_var ~ Pre + `rel_year_-5` + `rel_year_-4` + `rel_year_-3` + `rel_year_-2` + \n                `rel_year_0` + `rel_year_1` + `rel_year_2` + `rel_year_3` + `rel_year_4` + \n                `rel_year_5` + Post | unit + year | 0 | state, data = data, exactDOF = TRUE)\n  \n  # grab the obs we need\n  mod2 &lt;- tibble(\n    estimate = mod$coefficients,\n    term1 = rownames(mod$coefficients)\n    )\n  \n es &lt;-\n   mod2 %&gt;% \n    filter(term1 %in% keepvars) %&gt;% \n    mutate(t = c(-5:-2, 0:5)) %&gt;% \n    select(t, estimate)\n es\n}\n\ndata_classical &lt;- map_dfr(1:nrep, run_ES_DiD)\n\ncolors &lt;- c(\"True Effect\" = \"red\", \"Estimated Effect\" = \"blue\")\n\nES_plot_classical &lt;- data_classical %&gt;% \n  group_by(t) %&gt;% \n  summarize(avg = mean(estimate),\n            sd = sd(estimate),\n            lower.ci = avg - 1.96*sd,\n            upper.ci = avg + 1.96*sd) %&gt;% \n  bind_rows(tibble(t = -1, avg = 0, sd = 0, lower.ci = 0, upper.ci = 0)) %&gt;% \n  mutate(true_tau = ifelse(t &gt;= 0, (t + 1)* true_mu, 0)) %&gt;% \n  ggplot(aes(x = t, y = avg)) + \n  #geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), color = 'darkgrey', size = 2) + \n  geom_ribbon(aes(ymin = lower.ci, ymax = upper.ci), color = \"lightgrey\", alpha = 0.2) +\n  geom_point(color = 'blue', size = 3) + \n  geom_line(aes(color = 'Estimated Effect'), size = 1) + \n  geom_line(aes(x = t, y = true_tau, color = 'True Effect'), linetype = \"dashed\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  scale_x_continuous(breaks = -5:5) + \n  labs(x = \"Relative Time\", y = \"Estimate\") + \n  theme(axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12)) +\n  ggtitle(\"TWFE event-study regression with binned end-points\")+\n  scale_color_manual(values = colors) + \n  theme(plot.title = element_text(hjust = 0.5, size=12),\n        legend.position = \"bottom\", \n        legend.title = element_blank())\n\nrun_ES_DiD_sat &lt;- function(...) {\n  \n  # resimulate the data\n  data &lt;- make_data()\n  \n  # make dummy columns\n  data &lt;- data %&gt;% \n    # make relative year indicator\n    mutate(rel_year = year - cohort_year)\n  \n  # get the minimum relative year - we need this to reindex\n  min_year &lt;- min(data$rel_year)\n  \n  # reindex the relative years\n  data &lt;- data %&gt;% \n    mutate(rel_year = rel_year - min_year) %&gt;% \n    dummy_cols(select_columns = \"rel_year\")\n  \n  # make regression formula \n  indics &lt;- paste(\"rel_year\", (1:max(data$rel_year))[-(-1 - min_year)], sep = \"_\", collapse = \" + \")\n  keepvars &lt;- paste(\"rel_year\", c(-5:-2, 0:5) - min_year, sep = \"_\")  \n  formula &lt;- as.formula(paste(\"dep_var ~\", indics, \"| unit + year | 0 | state\"))\n  \n  # run mod\n  mod &lt;- felm(formula, data = data, exactDOF = TRUE)\n  \n  # grab the obs we need\n  mod2 &lt;- tibble(\n    estimate = mod$coefficients,\n    term1 = rownames(mod$coefficients)\n    )\n  \n es &lt;-\n   mod2 %&gt;% \n    filter(term1 %in% keepvars) %&gt;% \n    mutate(t = c(-5:-2, 0:5)) %&gt;% \n    select(t, estimate)\n es\n}\n\ndata_sat &lt;- map_dfr(1:nrep, run_ES_DiD_sat)\n\nES_plot_sat &lt;- data_sat %&gt;% \n  group_by(t) %&gt;% \n  summarize(avg = mean(estimate),\n            sd = sd(estimate),\n            lower.ci = avg - 1.96*sd,\n            upper.ci = avg + 1.96*sd) %&gt;% \n  bind_rows(tibble(t = -1, avg = 0, sd = 0, lower.ci = 0, upper.ci = 0)) %&gt;% \n  mutate(true_tau = ifelse(t &gt;= 0, (t + 1)* true_mu, 0)) %&gt;% \n  ggplot(aes(x = t, y = avg)) + \n  #geom_linerange(aes(ymin = lower.ci, ymax = upper.ci), color = 'darkgrey', size = 2) + \n  geom_ribbon(aes(ymin = lower.ci, ymax = upper.ci), color = \"lightgrey\", alpha = 0.2) +\n  geom_point(color = 'blue', size = 3) + \n   geom_line(aes(color = 'Estimated Effect'), size = 1) + \n   geom_line(aes(x = t, y = true_tau, color = 'True Effect'), linetype = \"dashed\", size = 2) + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  scale_x_continuous(breaks = -5:5) + \n  labs(x = \"Relative Time\", y = \"Estimate\") + \n  theme(axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12)) +\n  ggtitle(\"TWFE event-study regression with 'all' leads and lags\")+\n  scale_color_manual(values = colors) + \n  theme(plot.title = element_text(hjust = 0.5, size=12),\n        legend.position = \"bottom\", \n        legend.title = element_blank())\n        \n\nES_plot_sat/ES_plot_classical\n\n\n\n\n\n\n\n\n\nSo what do we do? Thankfully there are huge number of estimators that try to tackle this problem. Unfortunately there are a lot of estimators to choose from. I will focus on Callaway and Sant’Anna (2020) since it is a bit more straight forward and Pedro gives excellent talks. They focus on how we can create rigorous comparisions using a variety of double robust methods, but they allow for comparisions either with the ‘first-treated’ and ‘not yet treated’ or ‘never treated’. This will really depend on your application.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Causal-Inference.html#synthetic-control",
    "href": "Causal-Inference.html#synthetic-control",
    "title": "Causal Inference",
    "section": "",
    "text": "Synthetic Control(SCM) is one of the youngest of the canonical observational causal inference models. In a way we are mixing some of the best of both worlds with IPW and DiD. So let take the canonical example of the California smoking tax. So in this example CA imposes a cigarette tax kind of out of nowhere and we are interested in the effect of this tax on smoking. The problem is that there are not a lot of clean counterfactuals for CA. This happends for a variety of reasons but the problem arises that many of the candidate estimators not going to produce accurate estimates of the effect of a program.\nAs Athey and Imbens (2017) argues this is one of the major breakthroughs in the last 15 years. Basically if draw out a DAG of pre-treatment controls and they are within the convex hull we can literally just create a fake CA. Convex hull is a mathy way of saying that the some combination of the unaffected units can approximate the pre-intervention charecteristic of the unaffected units. In a very simple example\n\nfake_intervention = tibble(\n    treated_unit = seq(5.6, 6.5, length.out = 17),\n    donor_1 = seq(4.5,4.7, length.out = 17),\n    donor_2 = seq(4.0,4.4, length.out = 17),\n    donor_3 = seq(3.5, 4.9, length.out = 17),\n    time_relative_to_treatment = -12:4) |&gt;\n        pivot_longer(-time_relative_to_treatment) |&gt;\n    mutate(name = str_to_title(str_replace(name, '_', ' ')))\n\nggplot(fake_intervention, aes(x = time_relative_to_treatment, y = value, color = name)) +\n    geom_line() +\n    geom_vline(xintercept = 0, linetype = 'dashed') +\n    geom_hline(yintercept = 5) +\n    geom_hline(yintercept = 3.4) +\n    annotate(geom = 'text',x = -2 , y = 5.1, label = 'Outside Convex Hull') +\n    annotate(geom = 'text',x = -2 , y = 3.3, label = 'Outside Convex Hull') +\n    annotate(geom = 'text',x = -2 , y = 4.5, label = 'Synthetic unit will fall \\n somewhere in here') +\n    scale_x_continuous(limits = c(-12,4), breaks = seq(-12,4, by = 1)) +\n    labs(color = NULL) +\n    theme_classic()\n\n\n\n\n\n\n\n\nWhen applied with the official algorithm that looks like this\n\n\nCode\n# code taken from the tidysynth vignette\n\nlibrary(tidysynth)\ndata('smoking')\n\nsmoking_out &lt;-\n  \n  smoking %&gt;%\n  \n  # initial the synthetic control object\n  synthetic_control(outcome = cigsale, # outcome\n                    unit = state, # unit index in the panel data\n                    time = year, # time index in the panel data\n                    i_unit = \"California\", # unit where the intervention occurred\n                    i_time = 1988, # time period when the intervention occurred\n                    generate_placebos=T # generate placebo synthetic controls (for inference)\n                    ) %&gt;%\n  \n  # Generate the aggregate predictors used to fit the weights\n  \n  # average log income, retail price of cigarettes, and proportion of the\n  # population between 15 and 24 years of age from 1980 - 1988\n  generate_predictor(time_window = 1980:1988,\n                     ln_income = mean(lnincome, na.rm = T),\n                     ret_price = mean(retprice, na.rm = T),\n                     youth = mean(age15to24, na.rm = T)) %&gt;%\n  \n  # average beer consumption in the donor pool from 1984 - 1988\n  generate_predictor(time_window = 1984:1988,\n                     beer_sales = mean(beer, na.rm = T)) %&gt;%\n  \n  # Lagged cigarette sales \n  generate_predictor(time_window = 1975,\n                     cigsale_1975 = cigsale) %&gt;%\n  generate_predictor(time_window = 1980,\n                     cigsale_1980 = cigsale) %&gt;%\n  generate_predictor(time_window = 1988,\n                     cigsale_1988 = cigsale) %&gt;%\n  \n  \n  # Generate the fitted weights for the synthetic control\n  generate_weights(optimization_window = 1970:1988, # time to use in the optimization task\n                   margin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 # optimizer options\n  ) %&gt;%\n  \n  # Generate the synthetic control\n  generate_control()\n\n\nsmoking_out |&gt;\n    plot_trends()\n\n\n\n\n\n\n\n\n\nThere is a bit more to it than my general intuition of ‘this is just a fancy weighted average’ but realistically we are kind of just creating a fancy weighted average for CA. Really this is just a prediction problem where we are using various estimators to produce ‘CA’ where are reducing the distance between real CA and fake CA by the MSPE.\n\\[\n\\begin{aligned}\n\\hat{weights} = \\arg\\min \\sum^{t-1}_{t=1}(Y_{nt} - \\sum^{n-1}_{i=1}weights_{i}Y_{it})^2 \\\\\nwith \\sum^{n-1}_{i=1} \\hat{weights}=1 \\\\\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\hat{weights} \\ge0 \\\\\n\\mu = 0\n\\end{aligned}\n\\]\nWe also put the additional restrictions that the weights are greater than zero and sum to one. We do this to prevent interpolation. Effectively what mu is saying is that we are not letting averages across the group.\nThis is a really cool idea! However, this comes at a really great cost. To create a realistic synthetic version of CA we need lots of periods prior to treatment or we need lots of periods post treatment for the algorithm to get a handle on the trajectory of the trends. This is because the weighted average is trying to leverage past information to balance the trajectory of the synthetic outcome against the treated unit.\nHow do we justify a fancy weighted average? The main motivating justifaction is the factor analytic model where\n$$\n\\[\\begin{aligned}\n\nY_{it} = \\mathit{T}_{it}D_{it} + \\Theta^\\prime_{t}Z_{i} + \\xi_{t} + \\lambda_i^{\\prime}f_t + \\varepsilon_{it}\n\\end{aligned}\\]\n$$\n\\[\n\\begin{aligned}\nY^{0}_it = \\Theta^{\\prime}_t Z_{i} + \\xi_{t} + \\lambda_i^{\\prime}f_t + \\varepsilon_{it} \\\\\nY^{1}_{it} = Y^{0}_it + \\mathit{T}_{it}\n\\end{aligned}\n\\]\nThis allows the control to be a function of time fixed effects a vector of covariates plus some time-invariant confounders to be flexible across time. Broken down a little bit more suppose there are some number of time-varying signals. Each unit will pick up some fixed linear combination of these time varying signals and the differences. Since theese confounders are picked up in the pre-treatment outcomes for both treated and controls we can try to use this information to ‘balance on’ these confounders. We want to learn \\(\\lambda_i^{\\prime}f_t\\) from the pre-treatment controls so this term will be balanced out. If we get a good synthetic unit we can effectively not worry about \\(\\lambda_i^{\\prime}f_t\\) which if left to its own devices will behave/is an unobserved time-varying confounders.\nThe restrictions of weights summing to one, the weights being non-negative, and only dealing with one unit is somewhat of an arbitrary limitation. Innovations on the SCM the Generalized SCM (GSCM) (Xu 2017), the Augmented SCM (ASCM)(Ben-Michael, and and Rothstein 2021), or SynthDiD (Arkhangelsky et al. 2021) in someways relax these constraints.\nRecast uses the Augmented SCM which deals with non-negativity and the convex hull restrictions. We may have a lot of donor units but a limited number of covariates that may result in no unique solution. We could also have fewe control units wiht non-zero weights. These all pose problems for the traditional SCM where we do not have a unique solution. Effectively what happens underneath the hood is we have our SCM model that we estimate our synthetic unit on. If the pre-treatment fit is somewhat poor or is a little bit outside of the convex hull this is an estimation problem rather than a failure of the method. The estimator for the ASCM is\n\\[\nY_{it} = \\sum\\lambda^{scm}_i Y_{it} + \\hat{m}_{it} - \\sum \\lambda^{scm}_i \\hat{m}_{it}\n\\]\nWhere \\(\\hat{m}_{it} - \\sum \\lambda^{scm}_i \\hat{m}_{it}\\) is the augmentation to the original SCM model. Effectively we are adding a small correction to account for estimation problems. In a IPW setting there could be remaining imbalances between the treatment and control groups. To deal with this problem we would simply use G-computation to adjust for these residual imbalances. That is we would estimate the propensity score model with the pre-treatment covariates to predict treatment status and if there are remaining imbalances then we can include them in the outcome modeling to handle these imbalances. We can use a similar logic to improve the fit of our synthetic unit. In statistical learning framework we can kind of think of this as a boosting problem.\nIf the pre-treatment fit is poor we can use the augmented estimator to upweight units that are more predictive of the post-treatment outcomes. The ASCM can also handle cases where we may need weights to be negative and rely on extrapolation to get a good synthetic unit. We can relax the constraint of non-negative weights by applying a ridge penalty to the end of the ASCM model. This will let us control the amount of extrapolation that we allow.\nThe GSCM and SynthDiD view the problem not entirely differently but view SCM and DiD as matrix completion methods. Basically the idea is that based on the structure of our data we want to impute the missing potential outcomes based on what our matrix looks like. If we have a setting where N &gt; T or a ‘tall’ then we would want to use cross-sectional information or vice-versa. The advantage of these approaches is that they are a bridge to the DiD stuff we did earlier and we can incorporate intercept shifts for units or time and can generalize to cases where lots of units are treated, there is staggered treatment timing, or poor pre-treatment fit.\nThese work based on the idea of the interactive fixed effect models where we are really stratifying our model by treated and control. We are losing some information we don’t let the model see the both the treated and control groups during training that way we aren’t worried about negative weighting. The reason this is happening is that the model can’t actually use early adopters as comparisions. So we have a model that tries makes predictions about Y so we get \\(\\hat{Y}\\) and then we take the difference between predicted and actual values.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html",
    "href": "Bayesian-Stats.html",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Bayesianism has been on the to do list for awhile since well they make the prettiest plots. More importantly we get more coherent measures of uncertainty and are closer to how we actually think of the world. There are lots of broad applications for Bayesianism in the private industry. The betting market for sports and how we think about teams is effectively Bayesian. We incorporate a lot of information about the observable parts of a team like schedule, roster talent, coaching, and guesses about the probablity that the roster will stay healthy. These are all priors we have about things that relate to the success of the team going into the season. As the season progresses and we get data our prior will update. In the case of the Saints this year our prior updated to make them a good team after one game moving our guess about the number of games they would win this year to probably more than their projected win total. With the second win this may have not moved our prior much at all. However, as attrition set in then our prior shifted a little bit back toward the pre-season total so on and so forth.\n\n\nFormally Bayes rule is usually denoted by this equation.\n\\[\nP(A|B) = \\frac{P(A) P(B|A)}{P(B)}\n\\]\nWhile uber important in respect to Bayesian statistics this not always been the easiest way for me to think of Bayes rules with respect to logic of how we do Bayesian statistics. I think its hard to ground it for me because we can just rewrite it a bit and get the law of total probability which doesn’t make it inherently Bayesian. For me I think it is easier to clarify it as\n\\[\nPosterior = \\frac{Prior \\times \\text{Probability of Hypothesis being true | Data}}{\\text{Mean probability of Data}}\n\\]\nOr we are just counting the number of instances that we observed this data and then we average over the prior which get us our average. Practically this is really just so everything adds to 1. To be a Bayesian is to incorporate our beliefs and then update them as the data meets our hypothesis meets the data or as Rich McCelreth argues\n\nThe rules of probability tell us that the logical way to compute the plausibilities, after accounting for the data, is to use Bayes’ theorem.\n\nWe then sample from the posterior to communicate something interesting or important about our model. Like where is the mass of the probability distribution, where is the center of this distribution, etc\nBayesianism, in a way, is a formalization of this process by encoding our beliefs in probability distributions. Our prior is really just our belief about the distribution and plausible range of values for that variable before ever seeing the data. So if we were to set a prior for a coin we would set the prior as somthing kind of loosy goosy as this\n\nprior_heads = rbinom(n = 2, size = 1000, prob = 0.5)\n\nprior_heads[1]/sum(c(prior_heads[1], prior_heads[2]))\n\nSo in this case we are just stating that the probability of heads is about 50 percent over a thousand trials assuming a fair coin. However, lets say that we know the coin is biased in a known way that makes it come up heads 61% of the time. We could then set our prior that it willl come up heads as this\n\nprior_heads_biased = rbinom(n = 2, size = 1000, prob = .61)\n\nThis is not neccessarily all that unique from a standard null hypothesis which we covered in the opening “chapter” of this book. It is the combination of the prior and the posterior which makes the Bayesian machine go brrr.\nThe posterior can we operationalized along these lines\n\\[\nPosterior = \\frac{\\text{Probability of Data} \\times Prior}{\\text{Mean probability of Data}}\n\\]\nWhere we are averaging the probability of the prior. If we derive this out more formally we will see that the posterior is actually proportional to the prodct of the prior and the probability of the data. The posterior distribution contains the relative plausibility of different parameter values, conditional on the data and model. We can overcome a bad prior given an infinite amount of data, but this process will be inefficient and critically we will never reach infinite data. Priors in Bayesian inference are important computationally as well as substantively.\n\n\nSetting a prior is one of the hardest things in Bayesian statistics and the subject a large and rich part of the literature. I think one really important thing to adjudicate is what a prior really is. That way we can reinforce the importance of setting a good one, what to check, and how to check it.\nAccording to Gelman et al. (2013) we can conceptualize the prior distribution along two different lines. There is the population interpretation. This is a little frequentisty but is definitely helpful. Lets say that we have some pseudo population of parameters that the candidate parameter \\(\\beta\\) is drawn from. For simplicity sake lets draw \\(\\beta\\) from as standard normal where we overlay the overall distribution\n\nlibrary(tidyverse)\n\n\nsim_data = \\(beta_val = 0, beta_variance = 1, n = 100){\n  \n  sim_dat = tibble(beta_values = rnorm(n, mean = beta_val, sd = beta_variance),\n                   beta_mean = rep(beta_val, length.out = n),\n                   beta_variance = rep(beta_variance, length.out = n))\n\n  return(sim_dat)\n    \n}\n\npop_sims = map_dfr(c(5, 10, 15), \\(x) sim_data(beta_val = x)) \n\nwide_version = pop_sims |&gt; \n    mutate(id = row_number()) |&gt;\n    pivot_wider(names_from = beta_mean, values_from = beta_values, names_glue = 'mean_{beta_mean}') \n\nplot_dat = wide_version |&gt;\n    mutate(pop_total = pop_sims$beta_values) |&gt;\n    pivot_longer(cols = mean_5:mean_15) |&gt;\n    mutate(nice_labs = as.factor(str_to_title(str_replace(name, '_', \" \"))), \n           nice_labs  = fct_relevel(nice_labs, 'Mean 5', 'Mean 10', 'Mean 15'))\n\n\nggplot(plot_dat) +\ngeom_density(mapping = aes(x = value, y = after_stat(ncount),   fill = nice_labs), \n             stat = \"bin\",  size = 0.5,\n             alpha = 0.7) + \ngeom_density(mapping = aes(x = pop_total, y = after_stat(ncount)), \n                 alpha = 0.9,\n                 color = \"gray30\", size = 0.6, \n             stat = \"bin\",\n             direction = \"mid\") +\n   facet_wrap(vars(nice_labs)) +\n   MetBrewer::scale_fill_met_d(name = 'Lakota') +\n   theme_minimal() +\n   labs(fill = NULL, y = 'Scaled Count') +\n   theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nSo in the population interpretation of priors we have kind of weird population by design. Depending on what ‘draw’ we get our prior could be N(5,1) or it could be N(10,1) where plausible values center around 5 or 10 with a standard deviation of 1. In this setting we are thinking about given the data what is a reasonable set of values that we would expect to see. In this setting we are kind of explicitly using a bit of frequentist logic to bridge the gap. However, instead of assuming that all values between 5-15 are equally plausible we are expliciltly stating that the mass of the distribution will be around some value and the amount of variation that we will have. This could be useful if we have a huge amount of experiments or results banked and we can imagine our plausible beta values as drawn from some distribution of the experiments.\nIn the state of knowledge interpretation of priors we are still using our subject matter expertise of the phenomena, but there may not be a good population to ground our priors on. Say we are launching a completly new product or expanding our product into a new market. We may have some idea about what we are likely to see but we don’t have the same reference population that we can draw from, but we have at least some idea of what our expectation should be.\nAn important thing to note about Bayes Rule is that we can think of Bayes rules along these lines\n\\[\nPosterior = \\frac{\\overbrace{P(A)}^{\\text{Prior}} \\times \\overbrace{\\text{Probability of Data}}^{Likelihood}}{\\text{Average Probability of Data}}\n\\]\nWhat this means is that effectively the posterior is a bit of a compromise between our data and our beliefs as the size of the data get bigger our prior will have less of an influence on our posterior distribution, but it will never completely evaporate.\n\n\n\nMost of the time people say just use your substantive knowledge. But that’s generally not helpful if you haven’t done that in a statistical setting. Lets take it step by step. First we generally outline what we link the distribution of our dependent variable would be. For our outcome variable we may think that its a normal distribution so we would write it like this\n\\[\nOutcome \\sim Normal(\\mu, \\sigma)\n\\]\nIf it is a binary outcome we would write it like this\n\\[\nOutcome \\sim Bernouli(\\text{Number of Trials},\\text{Probability true})\n\\]\nThe next step is is we have to think about our generative model. So to ground our analysis it helps to start from a DAG. We generally have beliefs about an intervention in the world. That way we can start thinking about the potential relationships in our data. We then have to think of the plausible ranges for these values and what the uncertainty around them.\n\\[\n\\begin{align}\n\\textbf{Outcome} \\sim N(\\mu, \\sigma) \\\\[8pt]\n\\textbf{Predictor One} \\sim N (3, 1)\n\\end{align}\n\\]\nWhere we are shifting the mean a bit and putting a somewhat conservative prior. This is generally fine but there are lots of priors we can set.\nLets say that out in the real world by some miracle of god we have a true normal distribution where the mean is zero and the standard deviation is 1. We can set informative, weakly informative, flat/non-informative prior, and a conjugate prior. Conjugate priors are a little bit harder to visualize with the below schema because a conjugate prior is something that also relies on the posterior. Meaning that if it turns out the posterior of our mean comes from the same family then it turns out that our prior is conjugate.\n\nn = 1000\n\nprior_df = tibble(\n    pop = rnorm(n),\n    `Kinda Flat` = rnorm(n, sd = 5), \n    informative = rnorm(n, mean = 0.5, sd = 1)\n) |&gt;\n    pivot_longer(everything(),\n        names_to = 'prior',\n        values_to = 'value'\n    )\n\n\nggplot(prior_df, aes(x = value, fill = prior)) +\n    geom_density(alpha = 0.5) +\n    scale_fill_manual(values = c('Kinda Flat' = '#a40000', 'informative' = '#00b7a7', 'pop' = '#ffcd12')) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis could be better but you kind of get the idea. A flat/uninformative prior is at best a bit like saying “the parameter of interest is somewhere between \\(-\\infty\\) and \\(\\infty\\).” My flat prior in the plot is more akin to a weakly informative prior where we are ruling out impossible values, but not really ruling out extreme values. A weakly informative prior is more akin to like we wouldn’t expect the treatment effect to be all that big but it is not outside of the realm of possibility that due to a weird draw of the experimental population people really take to the treatment. Priors can get pretty crazy because well the real world is messy and making simplyfying assumptions is hard. This is why (most) Bayesians will simulate the world first before even touching the data.\n\n\n\n\nSimulations are superpowerful because we get to play god in a way that we don’t normally get to do as social scientists. We can really simply simulate that you have roughly a 50/50 chance of getting heads or tails.\n\ncoin_flips = replicate(10000, sample(c('heads', 'tails'), 1))\n\ncoin_flips |&gt;\n    as_tibble() |&gt;\n    group_by(value) |&gt;\n    summarise(counts = n()) |&gt;\n    ungroup() |&gt;\n    mutate(probs = counts/sum(counts))\n\nWhile this is intuitive we can see how this varies by number of flips.\n\nnum_flips = 100\n\nflips = sample(c('heads', 'tails'), size = num_flips, replace = TRUE)\n\ncoin_flips = tibble(\n    heads_frequency = cumsum(flips == 'heads')/1:num_flips,\n    flip_number = 1:num_flips\n\n)\n\n\nggplot(coin_flips, aes(x = flip_number, y = heads_frequency)) +\n    geom_line() +\n    geom_hline(yintercept = 0.5) +\n    scale_y_continuous(limits = c(0,1)) +\n    labs(x = 'Flip Number', y = 'Proportion of Heads') +\n    theme_minimal()\n\n\n\n\n\n\n\n\nIn expectation we start getting closer and closer to 50% heads. Neat, but why should we care? Well we can test how well our prior does on various situations with a known truth.\nLets take a randomized control trial with unobserved confounding using the really excellent DeclareDesign package.\n\noptions(scipen = 999)\nlibrary(DeclareDesign)\n\nrct &lt;-\n  declare_model(N = 100,\n                U = rnorm(N),\n                potential_outcomes(Y ~ 0.2 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\nfake_data = draw_data(rct)\n\nSo now we have some fake data where we can display the ‘truth’ or in this case 0.2\n\ndiagnose_design(rct)\n\nWhen we go and estimate it on some fake data we can see how keeping or omitting the unobserved confounding.\n\ncontrol_for_confounding = lm(Y ~ Z + U, data = fake_data)\n\nno_controls = lm(Y ~ Z, data = fake_data)\n\n\nmodelsummary::modelsummary(list('Controls Added' = control_for_confounding,\n                                 'No Controls' = no_controls),\n                          gof_map = 'nobs',\n                          stars = TRUE)\n\nThis is pretty cool to see how this can go if we don’t account for things that should be accounted for. The same general principle applies in Bayesian analysis. The reason we simulate out a RCT is that they are super expensive! We want to diagnose what could go wrong before we tell our partners what to do. The same idea applies for Bayes because we have two separate problems that can make it hard to tell what is going on. We have computational problems that can arise due to how Bayesian models are fit and we have modeling problems which are really just scientific problems. Conceptually these are somewhat distinct but practically these two run into each other all the time. We can isolate some of the computational mechanics of fitting a bad model on data we know is ‘good’. This makes fitting lots of models easier.\nHow should we simulat the data? Typically we will define a parameter that seems reasonable! So if we are trying measure. So if we were trying to model the impact of a treatment on conversion rate aka how often do we move from a free user to a subscriber setting simulating a uniform distribution across treatment and control.\n\nlibrary(patchwork)\nlibrary(brms)\nlibrary(modelsummary)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\nconversions_tibble = tibble(\n    Control = runif(100, min = 0, max = 100),\n    Treatment = runif(100, min = 0, max = 100),\n) |&gt;\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none') \n\n\n\n\n\n\n\n\nWe wouldn’t expect that by doing nothing that conversion rate is uniformly distributed between 0 and 100 percent. Conversion rate for Netflix or established streaming services is probably closer to something that looks like this.\n\n\nCode\nconversions_tibble_reasonable = tibble(\n    # most in the control group don't convert \n    Control = rbeta(n = 100, shape1 = 3, shape2 = 7) * 10,\n    # people in the control group are slightly more likely to convert\n    Treatment = rbeta(n = 100, shape1 = 6, shape2 = 4) * 10,\n) |&gt;\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble_reasonable, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none') \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are lots of ways to check our model! One way that is fairly common because Gelman reccomends it is prior predictive checks. The first is a prior predictive check which is just a way for us to check on what influence the prior is going to have on the posterior distribution. As our data grows the influence of our prior is going to decrease and the likelihood function is going to start to have more influence on the posterior distribution. Or in other words how probable the observed data is given different value of the model. However, they are not totally irrelevant the prior can impose a form of regularization shrinking the posterior predictions back towards the ‘true value.’ This is to say that even in large-n settings a well calibrated prior is still important in simple models. In much more complex models the prior is going to do a lot more.\nIn a sense we did something similar but what we are doing is modeling the problem only using the priors.\n\ntitanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\ntitanic &lt;- subset(titanic, PClass != \"*\")\n\nf &lt;- Survived ~ SexCode + Age + PClass\n\n\n\nmod_prior &lt;- brm(PClass ~ SexCode + Age,\n    data = titanic,\n     prior = c(\n        prior(normal(0, 3), class = b, dpar = \"mu2nd\"),\n        prior(normal(0, 3), class = b, dpar = \"mu3rd\")),\n    family = categorical(link = logit),\n    sample_prior = \"only\")\n\n\npp_check(mod_prior)   \n\n\n\n\n\n\n\n\nIn this case we the prior does okay matching the observed data. It could definitely be better\n\n\n\n\n\n\nLets break down some brms syntax first.\n\nprior - somewhat self explantory\nnormal - somewhat self explantory\nclass = b - Set the prior at the population level.\ndpar = 'class' - Here we are telling it that we are making an assumption about the probability of being in second or 3rd class relative to 1st class\n\n\nmod_prior |&gt;\n    modelsummary()\n\nOne important thing to note is that brms is based off of lme4 which is one of the premier multilevel modeling packages for frequentist. As a political scientist we aren’t really multilevel model people we are more throw OLS at everything people so some of the terminology around is different.\nIn OLS political science land we would say we include ‘class fixed effects’ to refer to a model that does something like this.\n\n# this is a bad model don't judge \nlogit_fe = glm(Survived ~ Age + SexCode + factor(PClass), data = titanic, family = binomial(link = 'logit'))\n\nmodelsummary(logit_fe)\n\nWhen we say class fixed effects we really just mean including an intercept per classs. In multilevel land what we mean by a fixed effect is really just saying that these effects are constant across groups. So think of the effect of school on classroom performance or the effect of product popularity on ROI on adverstising channels. The way we would replicate the this is something to the effect of\n\nbrm(Survived ~ Age + (1|PClass), data = titanic) |&gt;\n   modelsummary()\n\nAdmittedly also a really bad model but you get the idea.\n\n\n\nWe can then check our priors. We can also set priors on the intercept in our Titanic example if we stripped away age, class, and sex what would our baseline expectation of survival be? In an RCT this is really just what is a reasonable expectation for our control group. For other more complex models we could put priors on the effect of time, space, interactions between groupings and so much more.\n\n\n\nAs an R Bayesian we use brm mostly. We are going to walk through a variety of models that you have fit before using frequentist methods using brms. As the excercise progressed it reallly just became redoing the Statistical Rethinking\n\n\nSo as a good Bayesian we are going to set some priors. First we are going to set a prior for the height in cm and then we are going to set a scale parameter for the weight in kg’s and stretches the distribution. The sigma parameter must be positive. The alpha parameter we are going to think about well what if all the covariates were zero? What would a reasonable set of values be? In this case what would be the weight of an individual be if we never took into account height? Well a somewhat reasonable prior is just taking the average of the weight and spreading it out.\nStan really likes when you scale things because well it makes everything easier to compute. Rescaling in this case gives us the benefit of making the intercept interpretable. Inside the regression the intercept parameter now just becomes the expected weight of an individual is when they are of average height.\n\ndata('Howell1', package = 'rethinking')\n\njust_adults = Howell1 |&gt;\n    filter(age &gt;= 18) |&gt;\n    mutate(height_z = scale(height))\n\n\n\npriors = c(# this is just weight in kilos so approx 132 lbs \n           prior(normal(60, 10), class = Intercept),\n           # this is just centering on the new variable. Now we are also \n           # this is kilos/cm with a spread of 10^2 \n           prior(normal(0, 10), class = b, coef = 'height_z'),\n           prior(uniform(0,50), class = sigma, ub = 50))\n\nfirst_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors,\n    data = just_adults\n)\n\n\nplot(first_cut)\n\n\n\n\n\n\n\n\nSo far the chains look really nice there is great mixing. Everything you would want! However, if we simulate our prior we see an interesting phenomena\n\nsample_prior = first_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors,\n    data = just_adults,\n    sample_prior = 'only'\n)\n\nheight_scale &lt;- attributes(just_adults$height_z) %&gt;%\n  set_names(janitor::make_clean_names(names(.)))\n\ndraws_prior &lt;- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 1000)) |&gt;\n                add_epred_draws(sample_prior, ndraws = 1000) |&gt;\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot(draws_prior, aes(x = height_unscaled, y = .epred, group = .draw)) +\n    geom_line(alpha = 0.2) +\n    coord_cartesian(xlim = c(130, 170), ylim = c(10, 100)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis plot looks super wonky we wouldn’t expect that as height increases weight would decrease which seems to be the case for a fair number of cases. What we are seeing is that not out maliciousness we have set up a weird world for our model to calibrate to.\n\npp_check(sample_prior)\n\n\n\n\n\n\n\n\nIf we look a the posterior predictions they look a little weird. We have some draws that go negative implying there are negative weights. To recalibrate our model a bit we need to simply choose a prior that constrains our world a bit more.\n\npriors2 = c(prior(normal(60, 10), class = Intercept),\n            # here we are actually just constraining \n            # beta to be positive \n           prior(lognormal(0, 1), class = b, lb = 0 ),\n            prior(uniform(0,10), class = sigma, ub = 10))\n\nsample_prior2 =  brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors2,\n    data = just_adults,\n    sample_prior = 'only'\n)\n\n\n\ndraws_prior2 &lt;- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 1000)) |&gt;\n                add_epred_draws(sample_prior2, ndraws = 1000) |&gt;\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot(draws_prior2 ,aes(x = height_unscaled,  y = .epred, group = .draw)) +\n    geom_line(alpha = 0.2) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis is definitely a lot better! Out of 1000 draws we get only two weird samples, but that happens. For the most part we can live with this.\n\npp_check(sample_prior2)\n\n\n\n\n\n\n\n\nThis is definitely at least encouraging! We are not getting really really weird predictions. There are definitely some extreme ones in there, but I wouldn’t be like to shocked.\n\nfinal_model = first_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors2,\n    data = just_adults\n)\n\n\nfinal_draws = tibble(height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 500)) |&gt; \n  add_predicted_draws(final_model, ndraws = 100) |&gt;\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\n\nggplot(just_adults, aes(x = height, y  = weight)) +\n    geom_point(alpha = 0.5) +\n    stat_lineribbon(data = final_draws,\n    aes(x = height_unscaled, y = .prediction), .width = 0.95, alpha = 0.2, inherit.aes = FALSE) +\n    theme_minimal() +\n    theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nShe looks great! What about the posterior predictions?\n\npp_check(final_model)\n\n\n\n\n\n\n\n\nShe is beautiful. However if we look at the data\n\nggplot(just_adults, aes(x = height, y = weight, color = as.factor(male))) +\n    geom_point() +\n    theme_minimal()\n\n\n\n\n\n\n\n\nAs you would expect we see that there group differences. So if our dag looks like this.\n\nlibrary(ggdag)\nlibrary(dagitty)\n\ncoords = list(x = c(Height = 0, Sex = 1, Weight = 2),\n              y = c(Height = 0, Sex  = 1, Weight = 0))\n\nlabs = c(Height = 'Height', Sex = 'Sex', Weight = 'Weight')\n\ndagify(Weight ~ Height + Sex,\n        Height ~ Sex,\n        outcome = 'Weight',\n        exposure = 'Height',\n        labels = labs,\n        coords = coords) |&gt;\n    ggdag_status(use_labels = 'label', text = FALSE) +\n    guides(fill = 'none', color = 'none') +\n    theme_dag()\n\n\n\n\n\n\n\n\nRich advises using index variables for things we would normally make a factor or an indicator variable. His argument is that this makes it easier to set a prior on each the different levels of the category. What this does is split apply our prior to both levels of sex. If we wanted to get crazy we could actually estimate a multilevel model. Will it outperform a linear model? ¯_(ツ)_/¯. In this case we could create a fairly neutral prior\n\njust_adults = just_adults |&gt;\n    mutate(sex = factor(male),\n           height_z = as.numeric(height_z))\n\nsex_priors = c(\n    # we do be because we are going to specify a model sans intercept\n    prior(normal(60, 10), class = b),\n    prior(uniform(0,10), class = sigma, lb = 0, ub = 10)\n)\n\nsex_model = brm(weight ~ 0 + sex,\n                prior = sex_priors,\n                data = just_adults)\n\nsex_model\n\nNow we have our indicator variables.\n\nsw_post_means = sex_model |&gt;\n    gather_draws(b_sex0, b_sex1)\n\n\nggplot(sw_post_means, aes(x = .value, fill = .variable)) +\n    stat_halfeye() +\n    labs(fill = NULL, x = 'Posterior mean weight') +\n    theme_minimal() +\n    theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\nSo we could just subtract the the variables like this.\n\ndiffs_manual = sw_post_means |&gt;\n    pivot_wider(names_from = .variable, values_from = .value) |&gt;\n    mutate(diff = b_sex1 - b_sex0)\n\ndiffs_brms = sex_model |&gt;\n    spread_draws(b_sex0, b_sex1) |&gt;\n    mutate(diff = b_sex1 - b_sex0)\n\ntinytest::expect_equal(diffs_brms, diffs_manual)\n\nWe could also use marginaleffects.\n\nlibrary(marginaleffects)\n\nme_contrasts = get_draws(avg_comparisons(sex_model))\n\ntinytest::expect_equal(me_contrasts$draw, diffs_brms$diff)\n\nggplot(me_contrasts, aes(x = draw)) +\n    stat_halfeye() +\n    labs(x = 'posterior contrasts') +\n    theme_minimal()\n\n\n\n\n\n\n\n\nWe can also get posterior prediced draws with marginal effects via\n\npreds =  tibble(sex = c(\"0\", \"1\")) |&gt; \n  add_predicted_draws(sex_model, ndraws = 1000)\n\nggplot(preds, aes(x = .prediction, fill = sex)) +\n    stat_halfeye() +\n    theme_minimal() +\n    theme(legend.position = 'bottom') \n\n\n\n\n\n\n\n\nJust to be sure lets make sure the model makes sense\n\ncheck_sex_priors = brm(\n    weight ~ 0 + sex,\n    prior = sex_priors,\n    data = just_adults,\n    sample_prior = 'only'\n)\n\n\npp_check(check_sex_priors)\n\n\n\n\n\n\n\n\nEhh it somewhat hard to tell but in all honesty it seems like we are a little bit streching the issue a bit. Now we can use a little parlor trick to avoid some of the the wonky syntax of the nl argument of brms. It will kick a stink but she still works.\n\npriors &lt;- c(prior(normal(60, 10), class = b, coef = 'sex1'),\n            prior(lognormal(0, 1), class = b, coef = 'sex1:height_z'),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex &lt;- brm(\n  bf(weight ~ 0 + sex + sex:height_z),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors\n)\n\npriors2 &lt;- c(prior(normal(60, 10), class = b, nlpar = a),\n            prior(lognormal(0, 1), class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex_wonky =brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors2,\n  chains = 4, cores = 4\n)\n\n# model_height_sex \n# Regression Coefficients:\n#               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sex0             45.15      0.45    44.27    46.00 1.00     2560     2372\n# sex1             45.15      0.46    44.22    46.04 1.00     2734     2716\n# sex0:height_z     5.08      0.48     4.14     6.02 1.00     2846     2773\n# sex1:height_z     4.65      0.42     3.82     5.47 1.00     2716     2618\n# Further Distributional Parameters:\n#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sigma     4.27      0.16     3.96     4.60 1.00     4054     2961\n\n# model_height_sex_wonky\n# Regression Coefficients:\n#               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sex0             45.15      0.45    44.27    46.00 1.00     2560     2372\n# sex1             45.15      0.46    44.22    46.04 1.00     2734     2716\n# sex0:height_z     5.08      0.48     4.14     6.02 1.00     2846     2773\n# sex1:height_z     4.65      0.42     3.82     5.47 1.00     2716     2618\n# \n# Further Distributional Parameters:\n#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sigma     4.27      0.16     3.96     4.60 1.00     4054     2961\n\nSome of the logic of brms with the posterior is a little weird for me but there is not neccessarily a good comparisions in marginaleffects that I am aware of, but is probably going to be obvious if I just ask Vincent.\n\nsex_height_weight_post_pred &lt;- expand_grid(\n  height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 50),\n  sex = 0:1\n) |&gt; \n  add_predicted_draws(model_height_sex, ndraw = 4000) |&gt;\n  compare_levels(variable = .prediction, by = sex, comparison = list(c(\"0\", \"1\"))) |&gt; \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\n\nggplot(sex_height_weight_post_pred, aes(x = .prediction)) +\n  stat_halfeye(fill = 'red') +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHow do we interpret this kind of wonky thing? Well we are really justing talking about the distribution of the predicted differences between men and women’s weight. If we want to talk about the differences meaningfully between two different distributions then we have to take the differences. T\n\n\n\nReally what we want is one model to represent the entire causal system to get the effect of sex on weight without a ton of additional work. What we are going to do is basically estimate the effect of sex on weight through height. It is a little bit easier to think of the effect of like this.\n\ndagify(Weight ~ Height,\n       Height ~ Sex) |&gt;\n    ggdag() +\n    theme_dag()\n\n\n\n\n\n\n\n\nTo do this we have to set our priors accordingly to account for the path through height.\n\npriors &lt;- c(prior(normal(60, 10), resp = weight, class = b, nlpar = a),\n            prior(lognormal(0, 1), resp = weight, class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), resp = weight, class = sigma, lb = 0, ub = 10),\n            # prior(normal(160, 10), resp = height, class = b),\n            prior(normal(0, 1), resp = heightz, class = b),\n            prior(uniform(0, 10), resp = heightz, class = sigma, lb = 0, ub = 10))\n\nmodel_luxury &lt;- brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE) + \n    bf(height_z ~ 0 + sex) + \n    set_rescor(TRUE),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors\n)\n\n\nluxury_post_mean_diff &lt;- expand_grid(\n  height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 50),\n  sex = 0:1\n) |&gt; \n  add_epred_draws(model_luxury) |&gt;\n  compare_levels(variable = .epred, by = sex, comparison = list(c(\"1\", \"0\")))\n\nluxury_post_mean_diff |&gt; \n  filter(.category == \"weight\") |&gt; \n  ggplot(aes(x = .epred)) +\n  stat_halfeye(fill = 'pink') +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBrms has lots of ways to draw predictions from the posterior distribution! This is great, but they are a little bit different in subtle ways that can make it a hard to grasp at first. As with lots of things we are just going to follow an excellent Andrew Heiss blog post to understand the differences.\n\n\nCode\nlibrary(palmerpenguins)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(ggtext)\n\ntheme_pred &lt;- function() {\n  theme_minimal(base_family = \"Roboto Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 0),\n          legend.title = element_text(face = \"bold\"))\n}\n\ntheme_pred_dist &lt;- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          axis.text.y = element_blank(),\n          panel.grid.major.y = element_blank(),\n          panel.grid.minor.y = element_blank())\n}\n\ntheme_pred_range &lt;- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          panel.grid.minor.y = element_blank())\n}\n\nclrs &lt;- MetBrewer::met.brewer(\"Java\")\n\npenguins =   penguins |&gt;\ndrop_na(sex) |&gt; \n  mutate(is_gentoo = species == \"Gentoo\") |&gt; \n  mutate(bill_ratio = bill_depth_mm / bill_length_mm)\n\n\nmodel_normal &lt;- brm(\n  bf(body_mass_g ~ flipper_length_mm),\n  family = gaussian(),\n  data = penguins\n)\n\n\n\npenguins_avg_flipper &lt;- penguins |&gt; \n  summarize(flipper_length_mm = mean(flipper_length_mm))\n\n# Extract different types of posteriors\nnormal_linpred &lt;- model_normal |&gt; \n  linpred_draws(newdata = penguins_avg_flipper)\n\nnormal_epred &lt;- model_normal |&gt; \n  epred_draws(newdata = penguins_avg_flipper)\n\nnormal_predicted &lt;- model_normal |&gt; \n  predicted_draws(newdata = penguins_avg_flipper,\n                  seed = 12345)  \n\n\np1 &lt;- ggplot(normal_linpred, aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[3]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Linear predictor** &lt;span style='font-size: 14px;'&gt;*µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist() +\n  theme(plot.title = element_markdown())\n\np2 &lt;- ggplot(normal_epred, aes(x = .epred)) +\n  stat_halfeye(fill = clrs[2]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] and *µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_epred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\np3 &lt;- ggplot(normal_predicted, aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[1]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(2900, 5500)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Normal(*µ*, *σ*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\n(p1 / plot_spacer() / p2 / plot_spacer() / p3) +\n  plot_layout(heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\n\n\nSo both posterior_linpred and posterior_epred are lookig at the \\(\\mu\\) part of the model or effectively\n\\[\n\\mu = \\alpha + \\beta\\text{Flipper Length}\n\\]\nFor posterior_linpred what we are doing are we going to take the individual predictions from our model, the intercept and multiply them by the average flipper length.\n\nlinear_predictions_manual = model_normal |&gt;\nspread_draws(b_Intercept, b_flipper_length_mm) |&gt;\n mutate(mu = b_Intercept + \n           (b_flipper_length_mm * penguins_avg_flipper$flipper_length_mm))\n\ntinytest::expect_equal(linear_predictions_manual$mu, normal_linpred$.linpred)\n\nReally importanly what you will notice is that we don’t incorporate any inforamtion from \\(\\sigma\\) or the posterior or the part of the model that remains uncertain after modeling the outcome. As you might imagine that may have a some important implications on what the predictions from the posterior look like!\nposterior_predict uses this sigma term to effectively take samples from the normal around the mu with the sigma as a standard deviation. So the only thing we change in our code, is this\n\npostpred_manual = model_normal |&gt;\nspread_draws(b_Intercept, b_flipper_length_mm, sigma) |&gt;\n mutate(mu = b_Intercept + \n        (b_flipper_length_mm * penguins_avg_flipper$flipper_length_mm),\n       predictions = rnorm(n(), mu, sigma))\n\ntinytest::expect_equal(postpred_manual$predictions, normal_predicted$.prediction)\n\nThat makes sense since we are taking some samples. However, if we plot them they are going to look the same.\n\nmanual = ggplot(postpred_manual, aes(x = predictions)) + \n    stat_halfeye(fill = 'hotpink') +\n    labs(title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Normal(*µ*, *σ*)&lt;/span&gt;\", subtitle = \"rnorm(b_Intercept + (b_flipper_length_mm * 201), sigma)\") +\n    theme_pred_dist() +\n    theme(plot.title = element_markdown())\n\nmanual /p3\n\n\n\n\n\n\n\n\nSince we are incorporating the remaining uncertainty from the model we are going to have a little bit of a larger spread.\nepreds is a little weird. In many cases they can be the same as the linear predcitons. In fact if we did epredswe would get just get the linear predicitions returned back to us. epreds is short for Expected predicitions. This will combine the E[y] and \\(\\mu\\). Thankfully we have a really easy case to look at\n\nlogit_mod = brm(\n    is_gentoo ~ flipper_length_mm,\n    family = bernoulli(link = 'logit'),\n    data = penguins\n)\n\n\nepreds_logit = logit_mod |&gt;\n    epred_draws(newdata = penguins_avg_flipper)\n\nlinear_pres_logit = logit_mod |&gt;\n    linpred_draws(newdata = penguins_avg_flipper)\n\nepreds_logit_plot = ggplot(epreds_logit, aes(x = .epred)) +\n    stat_halfeye() +\n    labs(title = 'Expected Predictions') +\n    theme_minimal()\n\nlogit_linpreds = ggplot(linear_pres_logit, aes(x = .linpred)) +\n    stat_halfeye() +\n    labs(title = 'Linear Predicitions')+ \n    theme_minimal()\n\n\nepreds_logit_plot/logit_linpreds\n\n\n\n\n\n\n\n\nThese are super different! In this case epreds is going to take the predictions on the probability scale while linpreds is going to use the logit scales or the log odds scale(ewww). One of the things about epreds is it will shift a lot. For a beta regression the expectation will effectively be the average of the posterior’s average.\n\n\n\nWhen we adjust for a continous variable we are going adjust for the actual values of the continous variables and the slope because every value produces a different relationshiop between the treatment and the ouctcome. Scaling helps because we can think of big effects in terms of the mean and the sd.\n\ndata('WaffleDivorce', package = 'rethinking')\n\n\n\n\nwaffle_clean = WaffleDivorce |&gt;\n    janitor::clean_names() |&gt;\n    mutate(across(c(marriage, divorce, median_age_marriage),  \\(x) scale(x), .names = \"{col}_scaled\")) |&gt;\n    mutate(across(c(marriage, divorce, median_age_marriage), \\(x)  as.numeric(scale(x)), .names = \"{col}_z\"))\n\nwaffle_priors = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z'),\n    prior(normal(0,0.5), class = b, coef = 'marriage_z'),\n    prior(exponential(1), class = sigma))\n\n\nscaled_waffles = brm(\n    divorce_z ~ marriage_z + median_age_marriage_z,\n    data = waffle_clean,\n    prior = waffle_priors,\n    sample_prior = 'only' \n)\n\n# range(waffle_clean$median_age_marriage_z)\n\ncheck_priors = tibble(median_age_marriage_z = seq(-3, 3),\n                     marriage_z = 0) |&gt;\n                add_epred_draws(scaled_waffles, ndraws = 100)\n\nggplot(check_priors, aes(x = median_age_marriage_z, y = .epred, group = .draw)) +\n    geom_line(alpha = 0.5)\n\n\n\n\n\n\n\n\nThese are some of the implied regression lines from the model. For the most part we some pretty plausible regression lines. We have some weird ones where as the age of marriage increases the divorce rate decreases. Other regression lines we have mostly flat which I guess would make sense since we are not stratifying by state. We can then go ahead and estimate the model. I will spare myself the tedium of specifying two models. Instead we will estimate the effect of age of marriage through marriage rate. I suppose I didn’t explain it when I did it earlier because I was getting to angry at getting the : syntax to work correctly.\nSo the idea is that we really have two models. The effect of Age on Divorce and the effect of Age on marriage. So we have to tell brms how do to this because simply doing\n\nbrm(divorce_z ~ marriage_z + age_z, data = d)\n\nWould just adjust for the age of marriage in our model. This is great if we are only interested in the effect of marriage rate on divorce but is not our stated goal. There are lots of ways to invoke this in brms. I will probably just separate these into two different objects just for ease of reading.\n\nwaffle_priors_two = c(\n    prior(normal(0, 0.2), class = Intercept, resp = divorcez),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z', resp = divorcez),\n    prior(normal(0,0.5), class = b, coef = 'marriage_z', resp = divorcez),\n    prior(exponential(1), class = sigma, resp = divorcez),\n    prior(normal(0, 0.2), class = Intercept, resp = marriagez),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z', resp = marriagez),\n    prior(exponential(1), class = sigma, resp = marriagez))\n\nSince we technically have two regression models smooshed together we have to add priors for this model as well. To make sure brms doesn’t get mad at us we feed just use resp to tell it what equation the prior is for. We could obviously change the prior around age of marriage. This could get super complicated if we needed it, but for now we are gonna k.i.s.s.\nIn the gender example that we worked on earlier we did set_rescor(TRUE) which is going to allow correlation betwee the two different equations. This makes sense because we are making the assumption that there are probably other factors that influence this relationship that we don’t observe. In our waffle divorce model we would also want to include the effect of region/state. But in our DAG we don’t include it for pedagogical purposes\n\ndivorce_model = bf(divorce_z ~ median_age_marriage_z + marriage_z)\n\nmarriage_rate_model = bf(marriage_z ~ median_age_marriage_z)\n\nconflicted::conflict_prefer('stanfit', 'rstan')\n\nfull_luxury_model = brm(\n    divorce_model + marriage_rate_model + set_rescor(FALSE),\n    prior = waffle_priors_two,\n    data = waffle_clean,\n    cores = 4\n)\n\nWhats cool about this approach is we have modeled a few effects. We can get the effect of age on divorce and age on marriage rate. All we need to do is extract the predictions from the posterior and set the correct response variable.\n\ndivorce_predictions = tibble(median_age_marriage_z = seq(-2, 2, length.out = 40 ),\n                             # make sure we don't use a grid of marriage rates \n                             marriage_z = 0) |&gt;\n                       add_predicted_draws(full_luxury_model, resp = 'divorcez')\n\nmarriage_predictions = tibble(median_age_marriage_z = seq(-2, 2, length.out = 40 )) |&gt;\n                       add_predicted_draws(full_luxury_model, resp = 'marriagez')\n\n\n\n\n\nRegularization in Bayes works a bit differently then in frequentism/machine learning. Instead of simply adding a penalty term to the sum of the squared residuals we can impose some what ‘stronger weakly informative priors’ or there are priors that are kid of just used for regularization. To pick a model that performs well we should probably have an idea about how to pick a good model.\nA good model is one that balances good fit with flexibility to incorporate new data and describes the world realistically. This is probably the most straightforward way to describe the bias variance tradeoff. We will use the example from the book of brain size vs body mass\n\n# getting tired of  + theme_minmal\n\ntheme_set(theme_minimal())\n\n d &lt;- \n  tibble(species = c(\"afarensis\", \"africanus\", \"habilis\", \"boisei\", \"rudolfensis\", \"ergaster\", \"sapiens\"), \n         brain   = c(438, 452, 612, 521, 752, 871, 1350), \n         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))\nggplot(d, aes(x = mass, y = brain)) +\n    geom_point()\n\n\n\n\n\n\n\n\nThis will also be a useful way to learn how to use the update function. We are standardizing the variables becaus\n\n\n\n\n\n\nwe want to standardize body mass–give it mean zero and standard deviation one–and rescale the outcome, brain volume, so that the largest observed value is 1. Why not standardize brain volume as well? Because we want to preserve zero as a reference point: No brain at all. You can’t have negative brain. I don’t think.\n\n\n\n\nd = d |&gt;\n    mutate(mass_std  = (mass - mean(mass)) / sd(mass),\n         brain_std = brain / max(brain))\n\nbrain_model_priors = c(\n    prior(normal(0.5, 1), class = Intercept),\n    prior(normal(0,10), class = b),\n    prior(lognormal(0,1), class = sigma))\n\n\nbrain_model = brm(\n    brain_std  ~  mass_std,\n    prior = brain_model_priors,\n    data = d,\n    file = 'fits/brain_model',\n    chains = 4, cores = 4\n)\n\nbrain_sqr = update(\n    brain_model,\n    newdata = d,\n    brain_std ~ mass_std + I(mass_std^2),\n    chains =4, cores =4,\n    file = 'fits/brain_model_sqr'\n)\n\nbrain_cubed = update(\n    brain_model,\n    newdata = d, \n         formula = brain_std ~ mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4),\n         iter = 2000, warmup = 1000, chains = 4, cores = 4,\n         seed = 7,\n         control = list(adapt_delta = .995)\n)\n\nI am less dedicated (to this excerise than Soloman Kurz)[https://bookdown.org/content/4857/ulysses-compass.html#the-problem-with-parameters] so I am not going to go through the trouble of messing with stan directly. But we can see the logic of what is going on\n\nbrain_loo_lins =\\(mod, ylim = range(d$brain_std)){\n\n nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200))\n  \n  # simulate and wrangle\n  fitted(mod, newdata = nd, probs = c(.055, .945)) |&gt; \n    data.frame() |&gt; \n    bind_cols(nd) |&gt; \n    \n    # plot!  \n    ggplot(aes(x = mass_std)) +\n    geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5)) +\n    geom_point(data = d,\n               aes(y = brain_std)) +\n    labs(\n         x = \"body mass (std)\",\n         y = \"brain volume (std)\") +\n             coord_cartesian(xlim = c(-1.2, 1.5),\n                    ylim = c(0.3, 1.0))\n  \n\n}\n\nmod_list = list(brain_model, brain_sqr, brain_cubed)\n\nplots = purrr::map(mod_list, \\(x) brain_loo_lins(x)) \n\nwrap_plots(plots, ncol =1)\n\n\n\n\n\n\n\n\nObviously fitting a bunch of terms to like 4 data points is not an ideal way to model data. But this kind of mimics how we do a lot of stats in data science. One way to can shrink our estimates is to use regularization. To do this we can just set a more restrictive prior.\n\ndat = tibble(x = seq(from = - 3.5, to = 3.5, by = 0.01)) |&gt;\n  mutate(a = dnorm(x, mean = 0, sd = 0.2),\n         b = dnorm(x, mean = 0, sd = 0.5),\n         c = dnorm(x, mean = 0, sd = 1.0)) |&gt; \n  pivot_longer(-x) \n\n\nggplot(dat, aes(x = x, y = value, color = name, fill = name)) +\n    geom_area(alpha = 0.4, linewidth = 0.5, position = 'identity') +\n    theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nWhat is really strange if you are coming from a pure machine learning background is that we don’t neccesasrily need to implement a pure version of Ridge regression in Bayes. We can simply place more restrictive priors on the model using the good old fashion \\(\\beta \\approx N(0,1)\\) or placing a Laplace prior on the coefficients to achieve the same effect.\n\nn_sim   &lt;- 1e3\nkseq    &lt;- 1:5\n\nmake_sim &lt;- function(n, b_sigma) {\n  sapply(kseq, function(k) {\n    print(k);\n    r &lt;- replicate(n_sim, rethinking::sim_train_test(N = n, k = k, b_sigma = b_sigma));c(mean(r[1, ]), mean(r[2, ]), stats::sd(r[1, ]), stats::sd(r[2, ])) \n    }\n    ) |&gt; \n    \n    # this is a new line of code\n    data.frame()\n}\n\ns &lt;-\n  crossing(n  = c(20, 100),\n           b_sigma = c(1, 0.5, 0.2)) |&gt; \n  mutate(sim = map2(n, b_sigma, make_sim)) |&gt; \n  unnest(sim)\n\n\nn_sim   &lt;- 1e3\nn_cores &lt;- 8\nkseq    &lt;- 1:5\n\n# define the simulation function\nmy_sim &lt;- function(k) {\n  \n  print(k);\n  r &lt;- replicate(n_sim, rethinking::sim_train_test(N = n, k = k));\n  c(mean(r[1, ]), mean(r[2, ]), stats::sd(r[1, ]), stats::sd(r[2, ]))\n  \n}\n\n\n# here's our dev object based on `N &lt;- 20`\nn      &lt;- 20\ndev_20 &lt;-\n  sapply(kseq, my_sim)\n\n# here's our dev object based on N &lt;- 100\nn       &lt;- 100\ndev_100 &lt;- \n  sapply(kseq, my_sim)\n\n\n\n\ndev_tibble &lt;-\n  rbind(dev_20, dev_100) |&gt; \n  data.frame() |&gt; \n  mutate(statistic = rep(c(\"mean\", \"sd\"), each = 2) |&gt; rep(x = _, times = 2),\n         sample    = rep(c(\"in\", \"out\"), times = 2) |&gt; rep( x = _, times = 2),\n         n         = rep(c(\"n = 20\", \"n = 100\"), each = 4)) |&gt; \n  pivot_longer(-(statistic:n)) |&gt; \n  pivot_wider(names_from = statistic, values_from = value) |&gt;\n  mutate(n     = factor(n, levels = c(\"n = 20\", \"n = 100\")),\n         npar = str_extract(name, \"\\\\d+\") |&gt; as.double()) |&gt; \n  mutate(npar = ifelse(sample == \"in\", npar - .075, npar + .075))\n\n\n\n\n\n\n\n\n\n\nThe argument that Rich makes is that regularization is often a bit skeptical in Bayesian statistics. The priors are bit harder to tune. In frequentist statistics we search a large number of very small values of the hyperparameter. This can take a fair amount of time. Now think about searching through the posterior distribution for a ton of different models for the ideal prior. This can take a long time. However, there are a lot of work going into regularization priors since we use them for BART and other estimation methods.\n\n\n\nAll the core algorithms have different ways to sample from the posterior. Stan uses a Hamiltonian Monte Carlo (HMC)\n\nlibrary(animation)\n\nmyU2 &lt;- function( q , a=0 , b=1 , k=0 , d=0.5 ) {\n    s &lt;- exp(q[2]) # sigma on log latent scale\n    mu &lt;- q[1]\n    U &lt;- sum( dnorm(y,mu,s,log=TRUE) ) + dnorm(mu,a,b,log=TRUE) + dnorm(q[2],k,d,log=TRUE)\n    return( -U )\n}\n\n# gradient function\n# need vector of partial derivatives of U with respect to vector q\nmyU_grad2 &lt;- function( q , a=0 , b=1 , k=0 , d=0.5 ) {\n    mu &lt;- q[1]\n    s &lt;- exp(q[2])\n    G1 &lt;- sum( y - mu ) * exp(-2*q[2]) + (a - mu)/b^2 #dU/dmu\n    G2 &lt;- sum( (y - mu)^2 ) * exp(-2*q[2]) - length(y) + (k-q[2])/d^2 #dU/ds\n    return( c( -G1 , -G2 ) ) # negative bc energy is neg-log-prob\n}\n\n# test data\nset.seed(7)\ny &lt;- abs(rnorm(50))\ny &lt;- c( y , -y ) # ensure mean is zero\n\n###########\n# example paths\nlibrary(shape) # for good arrow heads\n# blank(bty=\"n\")\n\n# priors\npriors &lt;- list()\npriors$a &lt;- 0\npriors$b &lt;- 1\npriors$k &lt;- 0\npriors$d &lt;- 0.3\n\n#ss &lt;- ss + 1\nset.seed(42) # seed 9 for examples\n\n# init\nn_samples &lt;- 4\nQ &lt;- list()\nQ$q &lt;- c(-0.4,0.2)\nxr &lt;- c(-0.6,0.6)\nyr &lt;- c(-0.25,0.4)\n\nstep &lt;- 0.02\nL &lt;- 12 # 0.02/12 okay sampling --- 0.02/20 is good for showing u-turns\nxpos &lt;- c(4,2,1,2) # for L=20\n#xpos &lt;- c(2,3,2,1) # for L=55\npath_col &lt;- col.alpha(\"black\",0.5)\n\ndraw_bg &lt;- function() {\n    plot( NULL , ylab=\"log_sigma\" , xlab=\"mu\" , xlim=xr , ylim=yr )\n    # draw contour of log-prob\n    cb &lt;- 0.2\n    mu_seq &lt;- seq(from=xr[1]-cb,to=xr[2]+cb,length.out=50) \n    logsigma_seq &lt;- seq(from=yr[1]-cb,to=yr[2]+cb,length.out=50)\n    z &lt;- matrix(NA,length(mu_seq),length(logsigma_seq))\n    for ( i in 1:length(mu_seq) )\n        for ( j in 1:length(logsigma_seq) )\n            z[i,j] &lt;- myU2( c( mu_seq[i] , logsigma_seq[j] ) , a=priors$a , b=priors$b , k=priors$k , d=priors$d )\n    cl &lt;- contourLines( mu_seq , logsigma_seq , z , nlevels=30 )\n    for ( i in 1:length(cl) ) lines( cl[[i]]$x , cl[[i]]$y , col=col.alpha(\"black\",0.5) , lwd=1 )\n}\n\n\nQ &lt;- list()\nQ$q &lt;- c(-0.4,0.2) # start point\nxr &lt;- c(-0.4,0.4) # x range in plot\nyr &lt;- c(-0.25,0.3) # y range in plot\n\ndraw_bg()\n\nn_samples &lt;- 10\n# points( Q$q[1] , Q$q[2] , pch=4 , col=\"black\" )\npts &lt;- matrix(NA,nrow=n_samples,ncol=3)\n\nfor ( i in 1:n_samples ) {\n\n    Q &lt;- HMC2( myU2 , myU_grad2 , step , L , Q$q , a=priors$a , b=priors$b , k=priors$k , d=priors$d )\n\n    draw_bg()\n\n    # draw previous points\n    if ( i &gt; 1 ) {\n        for ( j in 1:(i-1) ) {\n            V &lt;- 0.9\n            points( pts[j,1] , pts[j,2] , pch=ifelse( pts[j,3]==1 , 1 , 16 ) , col=grau(V) , lwd=2 )\n        }\n    }\n\n    # draw trajectory\n    for ( l in 1:L ) {\n        lines( Q$traj[l:(l+1),1] , Q$traj[l:(l+1),2] , col=\"white\" , lwd=8 )\n        lines( Q$traj[l:(l+1),1] , Q$traj[l:(l+1),2] , col=4 , lwd=5 )\n        ani.record()\n    }\n    #points( Q$traj[2:L+1,] , pch=16 , col=\"white\" , cex=0.3 )\n\n    # draw new point\n    pts[i,1:2] &lt;- Q$traj[L+1,]\n    pts[i,3] &lt;- Q$accept\n\n    #Arrows( Q$traj[L,1] , Q$traj[L,2] , Q$traj[L+1,1] , Q$traj[L+1,2] , arr.length=0.3 , arr.adj = 0.7 , col=4 )\n    #text( Q$traj[L+1,1] , Q$traj[L+1,2] , i , cex=1.2 , pos=1 , offset=0.4 )\n    \n    points( Q$traj[L+1,1] , Q$traj[L+1,2] , pch=ifelse( Q$accept==1 , 1 , 16 ) , col=ifelse( Q$accept==1 , 4 , 2 ) , lwd=2 )\n\n    invisible( replicate( 3 , ani.record() ) )\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a way we are effectively just using gradient descent to get our posterior. What we are doing is just throwing a marble into a bowl with n-dimensions and then just recording where it lands. Since it is a bowl the marble is going to end towards the center of the bowl. We are going to use step size to tell our arm how hard to throw the marble. In gradient descent we really just use these throws to give the marble a hint what path would be faster to minimize a loss function.\nIn two or three dimensions this is hard when we start adding in the complexities of sampling from a huge dimensional space lots of problems can arise. As stated early on problems with computing or divergent chains are some indicators that our model may not be working all that well. When taming our markov chains it can help to set more informative priors. Even Stan doesn’t get mad at you its important to inspect the health of the markov chains. To make this easier on ourselves lets just simulate a model.\n\ndata('Wines2012', package = 'rethinking')\n\n\nwine_clean = Wines2012 |&gt;\n    mutate(score_z = scale(score),\n           wine = as.numeric(wine))\n\npriors = c(\n    prior(normal(0,1), class = b, coef = 'wine'),\n    prior(exponential(1), class = sigma)\n)\n\nwine_score_model = brm(score_z ~ 0 + wine,\nprior = priors,\n data = wine_clean)\n\n\n\nplot(wine_score_model)\n\n\n\n\n\n\n\n\n\n\nMost of the libraries have some built in method of trace plots. Visual diagnostics of MCMC chains tend towards the idea of ‘this looks like a jumbly mess’ so when it looks like a random mess that is good.\n\nlibrary(MetBrewer)\nmanual_trace_data = as_draws_df(wine_score_model)\n\nggplot(manual_trace_data, aes(x = .iteration, y  = b_wine, color = as.factor(.chain))) +\n    geom_line(alpha = 0.5) +\n    scale_color_met_d(name = 'Lakota')\n\n\n\n\n\n\n\n\nThis is kind of what you want it should look like lots of random lines. Lets make some bad priors to see what happens\n\npriors_bad = c(\n    prior(lognormal(0,10), class = b, coef = 'wine'),\n    prior(exponential(4), class = sigma)\n)\n\nbad_wine_score_model = brm(score_z ~ 0 + wine,\nprior = priors_bad,\n data = wine_clean)\n\nbad_chains = as_draws_df(bad_wine_score_model)\n\nggplot(bad_chains, aes(x = .iteration, y = b_wine,\n                       color = as.factor(.chain),\n                       group = as.factor(.chain))) +\n    geom_line() +\n scale_color_met_d(name = 'Lakota')\n\n\n\n\n\n\n\n\nI played up the priors a big but as you can see these chains are really bad. We have what looks to be clear trends in the first and fourth chains. We cahn’t really even see the second and third chain. Overall this is a mess.\n\n\n\n\nWe can also use rank orders to get a sense of the health of the chains. Basically no chain should consistently look the best. They should look like some jumbled up mess.\n\nas_draws_df(bad_wine_score_model) |&gt;\n    bayesplot::mcmc_rank_overlay()\n\n\n\n\n\n\n\n\nAs you can see it looks like a few chains are actually broken while others just seem to be doing bettter than others. Good trank plots look more like this:\n\nas_draws_df(wine_score_model) |&gt;\n    bayesplot::mcmc_rank_overlay() +\n    scale_y_continuous(limits = c(30,65)) +\n    scale_color_met_d(name = 'Lakota')\n\n\n\n\n\n\n\n\nwhere no one chain consistently outranks any other chain consistently.\n\n\n\n\n\nWhen chains converge the end of the chain and the beggining of a chain are exploring the same region. Independent chains explore the same or similar region. \\(\\hat{R}\\) is effectively a ratio of variances so we can think of it as\n\\[\n\\hat{R} = \\frac{\\text{Within Chain Variance}}{\\text{Total Variance of All Chains}}\n\\]\nIn practice this means that values closer to give us some indication of how well are chains are doing. Values like 1.01 could tell us that we just need to run the chains for longer or that we need to go back and rethink our priors.\n\n\n\nThis is an estimate of the number of effective samples what this is kind of telling us is that\n\n\n\n\n\n\nImportant\n\n\n\nHow long would the chain be if each sample was independent of the one before it\n\n\nMcElreath argues that we can think of rnorm as the ideal behavior of n_eff where each draw from a normal distribution is independent of the one before it. So our number of n_eff would be equal to what ever you feed the n argument. The same idea applies to n_eff where we are going to get some number that is generally less than the number of samples we are taking, but sometimes Stan will find places where we can grow the length of the chain. This is not a bug because, well, when we are traversing multidimensional spaces we are not going to be as clever about that as some may think. We would see the number off effective samples shrink if we had autocorrelation because past values of the variables are giving us more information about the variable we are modeling so the length of the chain will be much shorter. The toy bad model\n\n\n\n\nsummary(bad_wine_score_model)\n\nAs we can see in the basd wine model we have really really short chains and Rhat values that tell us that the chains aren’t mixing all that well! When we made the trace plot we saw that some chains were actually broken so the short length of chains make a ton of sense. When we compare this to a good model the difference is pretty stark\n\nsummary(wine_score_model)\n\n\n\n\nOften times when we have computational problems with our model we have issues with our scientific problems with our model. Calculating the posterior is hard thats why the racist Sir Ronald Fisher was a coward and took the log of the likelihood. When we are first building a model we will often have problems getting the chains to work well because our assumptions about the world get passsed onto our model. The trouble that we are having sampling from the posterior distribution maybe because it doesn’t make any sense.\n\n\n\n\nWe often have conditional hypothesis especially in the social sciences. Political science in particular uses a lot of interaction terms. Some of our most cited statistics articles are interaction terms. Importanly for multilevel models we often times have interactions with lots of factors. Inidividuals are going to interact with where they are from or what party they identify with.\n\ndata('rugged', package = 'rethinking')\n\nrugged = rugged |&gt;\n    drop_na(rgdppc_2000) |&gt;\n    mutate(log_gdp = log(rgdppc_2000),\n           log_gdp_z = log_gdp/mean(log_gdp),\n           rugged_std = rugged/max(rugged),\n           rugged_std_c = rugged_std - mean(rugged_std))\n\nLets take a crack at it with some bad priors\n\npriors_first_cut = c(\n    prior(normal(1, 1), class = Intercept),\n    prior(normal(0,1), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_first_cut,\n    sample_prior = 'only'\n)\n\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |&gt;\n    add_linpred_draws(check_priors, ndraws = 100) |&gt;\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n\n\n\n\n\n\n\n\nHere we are getting some pretty crazy looking regression lines when we move back to the unscaled version. By scaling things we can make lots of priors look pretty reasonable but when we put them back on the og scale we are going to run into some issues.\nLets constrain the priors a bit more to the defaultish values Rich suggests\n\npriors_rich = c(\n    prior(normal(1, 0.1), class = Intercept),\n    prior(normal(0,0.5), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_rich,\n    sample_prior = 'only'\n)\n\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |&gt;\n    add_linpred_draws(check_priors, ndraws = 100) |&gt;\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n\n\n\n\n\n\n\n\nHere we are getting slightly more plausible lines but we still have more than our fair share of weird worlds. We could make it a little bit better by tuning the prior a bit more.\n\npriors_rich = c(\n    prior(normal(1, 0.1), class = Intercept),\n    prior(normal(0,0.3), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_rich,\n    sample_prior = 'only'\n)\n\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |&gt;\n    add_linpred_draws(check_priors, ndraws = 100) |&gt;\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n\n\n\n\n\n\n\n\nCool now we just need to add an interaction with Rich’s index approach. Now we can add the priors to each of the ‘intercept terms’. The annoying part is that the nlpar are really just place holders where we are going to set priors for each level of the interaction. This would be fine but for now this would be incredibly annoying.\n\nrugged = rugged |&gt;\n    mutate(cid = ifelse(cont_africa == 1, '1', '2'))\n\nint_priors = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n                prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n                prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n                prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n                prior(exponential(1), class = sigma))\n\n\nint_model_binary = brm(\n    bf(log_gdp_z ~ 0 + a  + b * rugged_std_c,\n    a ~ 0 + cid,\n    b ~ 0 + cid,\n    nl = TRUE),\n    data = rugged,\n    prior = int_priors)\n\nI have never directly interpreted an interaction term in my life and have been told never too. So I am just going to run it through marginaleffects.\n\nprds  = predictions(int_model_binary,\n                    newdata = datagrid(cid = c('1', '2'),\n                                       rugged_std_c = mean))\n\npreds_plot = get_draws(prds) |&gt;\n    transform(type = 'response') |&gt;\n    mutate(cid = ifelse(cid == '1', 'Africa', 'Not Africa'))\n\n\nggplot(preds_plot, aes(x = draw, fill = cid)) +\n    stat_halfeye() +\n    scale_fill_met_d('Lakota') +\n    labs(y = 'Density', x = 'Predicted Values of Log GDP')\n\n\n\n\n\n\n\n\nHere are computing predictions across each level of our binary indicator while holding ruggedness at its mean value. Here we are seeing that the predictionns for continents outside of Africa cluster around above one indicating we would have above average GDP per capita when ruggedness is held at its mean values. This is a little bit weird to think about since we don’t really have a good intuition about the values of the data.\n\nunscaled = preds_plot |&gt;\n    mutate(draw = exp(draw * mean(rugged$log_gdp)))\n\n\nggplot(unscaled, aes(x = draw, fill = cid)) +\n    stat_halfeye() +\n    scale_fill_met_d('Lakota') +\n    labs(y = 'Density', x = 'Predicted Values GDP')\n\n\n\n\n\n\n\n\nTo replicate the work done in Rethinking we can simply do:\n\nnd &lt;- \n  crossing(cid= 1:2,\n           rugged_std = seq(from = -0.2, to = 1.4, length.out = 30)) |&gt; \n  mutate(rugged_std_c = rugged_std - mean(rugged$rugged_std))\n\ncountries &lt;- c(\"Equatorial Guinea\", \"South Africa\", \"Seychelles\", \"Swaziland\", \"Lesotho\", \"Rwanda\", \"Burundi\", \"Luxembourg\", \"Greece\", \"Switzerland\", \"Lebanon\", \"Yemen\", \"Tajikistan\", \"Nepal\")\n\nfit = fitted(\n    int_model_binary,\n    newdata = nd, \n    probs = c(0.015, 0.985)\n) |&gt;\n    data.frame() |&gt;\n    bind_cols(nd) |&gt;\n    mutate(cont_africa = ifelse(cid == 1, 'African Nations', 'Non-African Nations'))\n\ncleaned = rugged |&gt;\n    mutate(cont_africa = ifelse(cid == 1, 'African Nations', 'Non-African Nations'))\n\n\nggplot(cleaned, aes(x = rugged_std, y = log_gdp_z, fill = cont_africa, color = cont_africa)) +\n    geom_smooth(data = fit,\n                aes(y = Estimate, ymin = Q1.5, ymax = Q98.5)) +\n    coord_cartesian(xlim = c(0, 1)) +\n    theme(legend.position = \"none\") +\n    facet_wrap(vars(cont_africa))\n\n\n\n\n\n\n\n\n\n\n\n\ndata(tulips, package = 'rethinking')\n\nd = tulips |&gt;\n    mutate(blooms_std  = blooms/max(blooms),\n           water_cent = water - mean(water),\n           shade_cent = shade - mean(shade))\n\nFortunately in the case of continous by continous interactions we really only need to put a prior on one term so we can just do.\n\ncont_priors = c(\n    prior(normal(0.5, 0.25), class = Intercept),\n    prior(normal(0, 0.25), class = b, coef = water_cent),\n    prior(normal(0, 0.25), class = b, coef = shade_cent),\n    prior(normal(0, 0.25), class =b, coef = 'water_cent:shade_cent'),\n    prior(exponential(1), class = sigma)\n)\n\ncont_inter = brm(\n    blooms_std ~ water_cent:shade_cent + water_cent + shade_cent,\n    data = d,\n    prior = cont_priors\n)\n\nCool now lets try to recreate the plot in Rethinking using marginaleffects\n\npreds = get_draws(\n    predictions(cont_inter, newdata = datagrid(\n        shade_cent = -1:1,\n        water_cent = -1:1\n    ))\n) |&gt;\n    mutate(nice_labs = glue::glue('Shade (centered) = {shade_cent}'))\n\nggplot(preds, aes(x = water_cent, y = draw, group = drawid)) +\n    geom_smooth(alpha = 1/5) +\n    geom_point(alpha = 0.5) +\n    facet_wrap(vars(nice_labs)) +\n    labs(y = 'Blooms (Standardized)', x = 'Water (centered)')\n\n\n\n\n\n\n\n\n\n\n\nIt would be nice if everything was linear. It is nice that OLS generally performs pretty well in a lot of cases where it has no reason behaving that way. However, modeling the DGP correctly often is helpful if anything to give us some sanity checks.\n\n\n\n\ndata(UCBadmit,package = 'rethinking')\n\nadmit_data = UCBadmit\n\nCounts and events are inherently bounded either by force of de facto bounded. We have been using the exponential distribution in a lot of our priors. Effectively the exponetial distribution is just a distribution that tells us about a time to an event that has a constant rate. We have really just been using it to constrain the unexplained variance to be postive. The difficulty with setting priors for a logit is that we are doing it on the log odds scale so they don’t always have an intuitive mapping as they did in OLS land.\nWe can kind of think of it as anything +4 = almost always and anything -4=almost never so when we stretch out the prior or compress it on the logit scale it will create probability distributions we may not have a good intuition of what the prior should be. Fortunately we can get a handle on this using the qlogis and plogis function from R to get a sense of how our priors are going to translate on the log-odds scale and on the probability scale. The function link function can shrink turn somewhat reasonable priors into really extreme values. This kind of makes since in logit world we our outcome is constrained to 1 and 0 while OLS can take on really any value as long it can be computed.\n\n\nCode\ndata(weather_perth, package = 'bayesrules')\n\nweather = weather_perth |&gt;\n    select(day_of_year, raintomorrow, humidity9am, humidity3pm, raintoday) |&gt;\n   mutate(across(c(humidity9am, humidity3pm), \n                ~scale(., scale = FALSE), .names = \"{col}_centered\")) |&gt; \n  mutate(across(c(humidity9am, humidity3pm), \n                ~as.numeric(scale(., scale = FALSE)), .names = \"{col}_c\")) |&gt; \n  mutate(raintomorrow_num = as.numeric(raintomorrow) - 1)\n\nextract_attributes &lt;- function(x) {\n  attributes(x) %&gt;%\n    set_names(janitor::make_clean_names(names(.))) %&gt;%\n    as_tibble() %&gt;%\n    slice(1)\n}\n\nunscaled &lt;- weather %&gt;%\n  select(ends_with(\"_centered\")) |&gt; \n  summarize(across(everything(), ~extract_attributes(.))) |&gt; \n  pivot_longer(everything()) |&gt; \n  unnest(value) |&gt; \n  split(~name)\n\n\nSo if we think that the baseline rate of rain is about 20% we can get see what a reasonable prior on the log-odds scale would look like this:\n\nqlogis(.2)\n\nSo if we think that there is a range between 15% to 25% of rain is normal than we can translate it to roughly around .5 in either direction. It is a little hard to think of what that would translate to so we can use plogis to convert it back to the probablity scale. So we have a range between 8% and 40% chance of rain. This honestly seems fine to me since I don’t have any good sense of how likely or unlikely this is for Peth.\n\nplogis(qlogis(0.2) - (2 * 0.5))\n\nplogis(qlogis(0.2) + (2 * 0.5))\n\nThe priors for how humidity will shape the weather are a bit hard to get my head around because I don’t have a good idea about what makes sense. The Bayes rules book makes the argument that 0.07 makes a lot of sense? If we plug this into the exp ratio\n\nexp(0.07 - 0.07)\n## [1] 1\nexp(0.07 + 0.07)\n\nSo the odds of rain the next will increase by 0-15% for each percentage point increase in humidity\n\npriors_first = c(prior(normal(-1.39, 0.5), class = Intercept),\n                 prior(normal(0.07, 0.035), class = b, coef = 'humidity9am_c'))\n\nclrs &lt;- MetBrewer::met.brewer(\"Lakota\", 6)\n\ncheck_weather_priors = brm(\n    raintomorrow ~ humidity9am_c,\n    data = weather, \n    family = bernoulli(link = 'logit'), \n    prior = priors_first,\n    sample_prior = 'only'\n)\n\np1 &lt;- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |&gt; \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |&gt; \n  add_epred_draws(check_weather_priors, ndraws = 100) |&gt; \n  ggplot(aes(x = humidity9am, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.5, size = 0.5, color = clrs[6]) +\n  labs(x = \"9 AM humidity\", y = \"Probability of rain tomorrow\")\n\np2 &lt;- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |&gt; \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |&gt; \n  add_predicted_draws(check_weather_priors, ndraws = 100) |&gt; \n  group_by(.draw) |&gt; \n  summarize(proportion_rain = mean(.prediction == 1)) |&gt; \n  ggplot(aes(x = proportion_rain)) +\n  geom_histogram(binwidth = 0.02, color = \"white\", fill = clrs[1]) +\n  labs(x = \"Proportion of rainy days in each draw\", y = \"Count\")\n\np1 | p2\n\n\n\n\n\n\n\n\nI think we can feel pretty good about the worlds we have drawn. Lets go ahead and check the posteriors\n\nreal_model =  brm(\n    raintomorrow ~ humidity9am_c,\n    data = weather, \n    family = bernoulli(link = 'logit'), \n    prior = priors_first\n)\n\npp_check(real_model, ndraws = 50)\n\n\n\n\n\n\n\n\nThat is a pretty good looking posterior predictive check. We desire posterior predicitive checks this pretty.\n\n\n\n\n\nConfession these guys scare me a bit. Knowing what to call them is fraught they have weird syntax and also just have a ton of machinery behind them. The general idea is that categories tell us something interesting and important about observations in the category and across categories. We are basically using partial pooling where we are not fully treating each category as independent like we would with a fixed effect model.\n\ndata('reedfrogs', package = 'rethinking')\n\nadd_tank = reedfrogs |&gt;\n    mutate(tank = row_number(),\n           preds = ifelse(pred == 'no' ,1L, 2L),\n           group = ifelse(size == 'small', 1L, 2L),\n           log_density_sd = scale(log(density)))\n\nWhen thinking about models with multiple levels like we have in the reed frog example is we need to ask ourselves is how much will each of the clusters vary? In this notation we are trying to learn \\(\\bar{a}\\) which is effectively the average tank and then try to learn how variable the tanks are.\n\\[\n\\begin{aligned}\n\\text{Survived} \\sim Binomial(D_{i}, p_{i}) \\\\\nlogit(p_i) = \\alpha_{T[i]} \\\\\n\\alpha_j \\sim Normal(\\bar{\\alpha, ?}) \\\\\n\\bar{a} \\sim Normal(0, 1.5)\n\\end{aligned}\n\\]\nIt is kind of hard to get an idea we can go through and use cross validation to get an idead of what sigma should be. We are going to have to use some wonky(to me) syntax. In this case if we just didn’t include the | trials(density) it would make the assumption that surv was binary which is mostly fine if that is the case. We want to model the proportion that survived from each tank. So we have to tell brms how many survivd.\n\n\nCode\nwith_trials &lt;- \n  brm(data = add_tank, \n      family = binomial,\n      surv | trials(density) ~  (1 | tank),\n      prior = c(prior(normal(0, 1.5), class = Intercept),  # alpha bar\n                prior(exponential(1), class = sd)),        # sigma\n      iter = 5000, warmup = 1000, chains = 4, cores = 4)\n\ncheck_mod_priors = \\(sigma_values = 1.5){\n   \n   new_prior = c(set_prior(paste0(\"normal(0, \", sigma_values, \")\"), class = 'Intercept'),  # alpha bar\n                 set_prior('exponential(1)', class = 'sd'))\n   \n   m = brm(data = add_tank, \n      family = binomial,\n      surv | trials(density) ~ 1 +  (1 | tank),\n      prior = new_prior,        # sigma\n      iter = 5000, warmup = 1000, chains = 4, cores = 10)\n\n  crit = add_criterion(m, criterion = 'loo') \n\n  return(crit)\n}\n\nsig_values = seq(0.1, 5, by = 0.1)\n\nnames_vec = paste('Sigma = ', sig_values)\n\nnames(sig_values) = names_vec\n\n\ncheck_models = map(sig_values, check_mod_priors)\n\nget_psis = map(check_models, \\(x){\n    psis(x)\n})\n\n\nThis is kind of an expensive task computationally and not all that fun to wrangle. We can actually learn the optimal posterior distribution of sigma from modeling. If we look at the summary of the model we can see that the model learns the optimal value of sigma or the global variation of tanks.\n\ntank_priors = c(prior(cauchy(0, 1.5), class = Intercept),  \n                prior(exponential(1), class = sd))\nwith_trials &lt;- \n  brm(data = add_tank, \n      family = binomial,\n      surv | trials(density) ~  (1 | tank),\n      prior = tank_priors,        \n      iter = 5000, warmup = 1000, chains = 4, cores = 4)\n\n\nwith_trials |&gt;\n    tidy()\n\n\n\n\nCool well Rich doesn’t actually go into how to use this immediately which is always fun. The issue we are going to run into is that when we add features is tha varying effects or effects per intercept can introduce confounding. To side step some of the issues of following along week by week I am going to use the Bayes Rules book.\n\ndata('cherry_blossom_sample', package = 'bayesrules')\n\nrunning = cherry_blossom_sample |&gt;\n    select(runner, age, net) |&gt;\n    drop_na() |&gt;\n    mutate(runner_nice = glue::glue('Runner {runner}'),\n           runner_nice = fct_inorder(runner_nice),\n           across(c(net, age), \\(x) scale(x, scale = FALSE), .names = '{col}_c'))\n\nunscaled = running |&gt;\n    select(ends_with('c')) |&gt;\n    summarise(across(everything(), \\(x) extract_attributes(x))) |&gt;\n    pivot_longer(everything()) |&gt;\n    unnest(value) \n\nWhats nice is that all the runners are in one age group. We would expect that there would be interactions between the runner specific intercepts and the age specific intercepts. Subsequently we would need priors for those age specific intercepts.\n\nggplot(running, aes(x = net)) +\n    geom_histogram(color = 'white')\n\n\n\n\n\n\n\n\nAs we would kind of expect we see spikes around specific times eg some people really want to finish aroun the 90 minute mark or under 2 hours. Since every runner is different in their own way there are some stuff that is going to be different. Some people are naturally faster or have been training for longer. However, we can use information from each of the runners to learn something about the other runners. So tune the varying intercepts part of the prior.\n\nrunning_priors = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(exponential(1), class = sd))\n\n\nfirst_cut = brm(\n    net ~ (1| runner),\n    data = running,\n    prior = running_priors,\n    sample_prior = 'only'\n)\n\np1 = running |&gt;\nadd_predicted_draws(first_cut)  |&gt;\nggplot(aes(x = .prediction, group = .draw)) +\ngeom_density() +\nlabs(title = 'SD = 1')\n\nThis doesn’t look like totally awful but it seems like we have a.\n\nrunning_priors2 = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(exponential(0.5), class = sd))\n\n\nfirst_cut = brm(\n    net ~ (1| runner),\n    data = running,\n    prior = running_priors2,\n    sample_prior = 'only'\n)\n\n\nplot_dat = running |&gt;\nadd_predicted_draws(first_cut) \n\n\np2 = ggplot(plot_dat,aes(x = .prediction, group = .draw)) +\ngeom_density() +\nlabs(title = 'SD = 0.5')\n\nThat is nice we are getting slightly less impossible draws with more and more looking like our real sample. We can definitely going to tune this a bit more since we seem to have lots of deviation in the spikes. We would imagine that members of the same group are far less variable than this. Lets take a look at the differences between the real and predicitions from our prior.\n\nplot_dat |&gt;\n    ungroup() |&gt;\n    reframe(actual_avg = mean(net),\n              predicted_avg = mean(.prediction), \n              actual_range = range(net),\n              predicted_range = range(net))\n\nThe ranges look pretty good! There is a pretty big difference in the actual and predicted average so maybe a little bit more tuning of the standard deviation will do us some good.\n\nrunning_priors3 = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(exponential(0.1), class = sd))\n\n\nfirst_cut = brm(\n    net ~ (1| runner),\n    data = running,\n    prior = running_priors3,\n    sample_prior = 'only'\n)\n\nplot_dat = running |&gt;\nadd_predicted_draws(first_cut) \n\n\np3 = ggplot(plot_dat,aes(x = .prediction, group = .draw)) +\ngeom_density() +\nlabs(title = 'SD = 0.1')\n\nNow lets look at the changes summary statistics.\n\nplot_dat |&gt;\n    ungroup() |&gt;\n    reframe(actual_avg = mean(net),\n              predicted_avg = mean(.prediction), \n              actual_range = range(net),\n              predicted_range = range(net))\n\nLets look at the plots next to each other.\n\np1 / p2/ p3\n\n\n\n\n\n\n\n\nIt is somewhat hard to tell what is the best prior would be. The argument that they make in the book is that that we wouldn’t expect a ton of variation between and within runners in the same age group. Lets simultate out the prior\n\nsd_1 = ggplot() +\n    stat_function(fun = ~dexp(.,1/10), geom = 'area') +\n    xlim(c(0, 60)) \nsd_5 = ggplot() +\n    stat_function(fun = ~dexp(., 5/10), geom = 'area') +\n    xlim(c(0, 60)) \n\nsd_un = ggplot() +\n    stat_function(fun = ~dexp(., 1), geom = 'area') +\n    xlim(c(0, 60)) \n\nsd_1 | sd_5 | sd_un\n\n\n\n\n\n\n\n\nThe exponetial of 1 and 0.5 seems to think that there is a lot smaller variation I think the more informative prior seems to letting the sigmas vary a bit more. Which makes sense if we have a just signup kind of race. An exponential of 1 or 0.5 might make more sense if we were looking at the times for professional racers where the margins are a lot slimmer. So lets go with the 0.1. Next we are going to think about the relationship between age and time. Or slightly rephrased, as you get older how many minutes do you lose off your mile? That is a somewhat hard question to answer. Lets start with a slightly agressive prior. The reason I say agressive is that we are making the argument that as age increases we lose about two and half minutes on average give or take a minute.\n\npriors_runners = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(normal(2.5, 1), class = b),\n    prior(exponential(0.1), class = sigma),\n    prior(exponential(0.1), class = sd)\n)\n\nrunner_model = brm(\n    net ~ age_c + (1|runner), \n    data = running, \n    prior = priors_runners,\n    sample_prior = 'only'\n)\n\nrunning |&gt;\n    add_linpred_draws(runner_model, ndraws = 8)  |&gt;\n    ggplot(aes(x = age, y = net)) +\n    geom_line(aes(y = .linpred, group = paste(runner, .draw))) +\n    facet_wrap(vars(.draw))\n\n\n\n\n\n\n\n\nThese look pretty good! Now its time to actually incorporate the data\n\nrunner_model_final = brm(\n    net ~ age_c + (1|runner), \n    data = running, \n    prior = priors_runners,\n    iter = 4000, cores = 10, threads = 5\n)\n\nrunner_predictions = \n     predictions(runner_model_final, re_formula = NA, ndraws = 200, type = 'prediction') |&gt;\n     get_draws() |&gt;\n     mutate(age = (age_c + mean(running$age)))\n\n\n me = ggplot(runner_predictions, aes(x = age, y = draw)) +\n     stat_lineribbon(alpha = 0.5) +\n     labs(x = 'Age', y = 'Race Time(Minutes)', title = 'Made with marginaleffects') +\n     theme(legend.position = 'none') + \n     scale_y_continuous(breaks = seq(80, 100, by = 5))\n\nlpd = runner_model_final |&gt;\n    linpred_draws(running, ndraws = 200, re_formula = NA) |&gt;\n    ggplot(aes(x = age, y = net)) +\n    stat_lineribbon(aes(y = .linpred)) +\n    labs(x = 'Age', y = 'Race time', title = 'made with linepredraws') +\n    theme(legend.position = 'none')\n\nThese are pretty close to the same!\n\nme + lpd \n\n\n\n\n\n\n\n\nBasically all this is telling us as that effectively as age increases so does race time for the typical runner. The next step is to look at the runner specific analysis or the ‘fixed effect’ in MLM.\n\nrunner_effects = runner_model_final |&gt;\n    spread_draws(b_Intercept, r_runner[runner,]) |&gt;\n    mutate(runner_intercepts = b_Intercept + r_runner) |&gt;\n    ungroup() |&gt;\n    mutate(runner = fct_reorder(factor(runner), runner_intercepts, .fun = mean))\n\nglobal_effect = runner_model_final |&gt;\n    tidy(effects = c('fixed'), conf.level = 0.89) |&gt;\n    filter(term == '(Intercept)')\n\nggplot(runner_effects, aes(x = runner_intercepts, y = runner)) +\n    stat_pointinterval() +\n    annotate(geom = 'rect', ymin = -Inf, ymax = Inf,\n              xmin = global_effect$conf.low, xmax = global_effect$conf.high, alpha = 0.5) +\n    geom_vline(xintercept = global_effect$estimate)\n\n\n\n\n\n\n\n\n\n\n\nThis is all well and dandy but this is what it looks like when we just fit an OLS model per runner\n\nrunning |&gt; \n  ggplot(aes(x = age, y = net, group = runner)) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5, color = clrs[1]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = \"Race time\",\n       title = \"Observed data\",\n       subtitle = \"Basic per-runner OLS models\")\n\n\n\n\n\n\n\n\nThis makes sense because time effects us all differently if we think about the effect of age on football positions we get a different relationship to production. A QB can be uber productive in their 30 and above seasons versus a wide receiver or corner. To account for these relationships we need to model a specific intercept and a particular slope for each runner.\nWe need a more robust language for these kinds of models because they are uber complicated because now both the intercept and the slope per each runner are going to move together. We could theoretically write them as distributions but the tinkering is going to be a bear. Instead we can use multi-dimensional priors to describe the correlations between the features. There are effectively three things we need to think about\n\nthe feature means: the mean of the intercepts, the global intercept etc\nthe correlation matrix: what is the expected correlation\nHow much to they vary?\n\nThe tricky part is how do we set a prior for a correlation matrix? We can use an LKJ prior. This prior is a effectively a shape. Bigger values are more skeptical of large correlation coefficients.\n\nwithr::with_seed(123, {\n  rho_plots &lt;- tibble(rho = c(-0.99, 0, 0.99)) |&gt; \n    mutate(title = glue::glue(\"ρ = {rho}\"),\n           subtitle = c(\"Strong negative correlation\\nbetween slope and intercept\",\n                        \"No correlation\\nbetween slope and intercept\",\n                        \"Strong positive correlation\\nbetween slope and intercept\")) |&gt; \n    mutate(Sigma = purrr::map(rho, ~matrix(c(1, .x, .x, 1), 2, 2))) |&gt; \n    mutate(data = purrr::map(Sigma, ~{\n      MASS::mvrnorm(n = 100, mu = c(2, 3), Sigma = .x) |&gt; \n        as_tibble() |&gt; \n        rename(b0 = V1, b1 = V2)\n    })) |&gt; \n    mutate(plot = pmap(list(data, title, subtitle), ~{\n      ggplot(..1) +\n        geom_abline(aes(intercept = b0, slope = b1, color = b0),\n                    size = 0.3) +\n        scale_color_viridis_c(option = \"rocket\", begin = 0.1, end = 0.85,\n                              limits = c(-1, 5)) +\n        labs(title = ..2, subtitle = ..3, color = \"β&lt;sub&gt;0&lt;/sub&gt;\") +\n        lims(x = c(0, 3), y = c(0, 10)) +\n        theme(axis.line = element_line(),\n              legend.title = element_markdown())\n    }))\n})\n\nwrap_plots(rho_plots$plot) + \n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nIn this case when the correclation is large and negative big intercepts have smaller slopes this would mean that individual runners who have longer baseline race times would see less of an age effect. When p is bigger and positive bigger intercepts have steeper slopes meaning runners with longer baseline race times would have more of an age effect. Whats nice about the LKJ prior in brms is that if we don’t want to touch the means and variance parts we don’t have to we can just set the regularization parts. The breakdown looks like this\n\n\\(\\eta\\) &lt; 1 extreme correlations are more likely\n\\(\\eta\\) = 1, all correlations are equally likely\n\\(\\eta\\) &gt; 1 central correlations are more likely, and the larger the \\(\\eta\\) the narrower the distribution is around 0\n\nHere we will start with a fairly bland prior.\n\nvarying_slope_priors_1 = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(2.5, 1), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(exponential(0.1), class = sd),\n            prior(lkj(1), class = cor))\n\n\nvar_mod1 = brm(\n    net ~ age_c + (1 + age_c | runner),\n    data = running,\n    prior = varying_slope_priors_1,\n    iter = 4000, threads = threading(5), cores = 10, \n    control = list(adapt_delta = 0.9),\n    sample_prior = 'only' \n)\n\nLets look at the sims\n\nsim1 = running |&gt;\n    add_predicted_draws(var_mod1) \n\n\nggplot(sim1, aes(x = .prediction, group = .draw)) +\n    geom_density() +\n    coord_cartesian(xlim = c(0, 300))\n\n\n\n\n\n\n\n\nThis seems broadely reasonable! What if we bump up the regularization a bit?\n\nvarying_slope_priors_2 = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(2.5, 1), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(exponential(0.1), class = sd),\n            prior(lkj(1.5), class = cor))\n\n\nvar_mod2 = brm(\n    net ~ age_c + (1 + age_c | runner),\n    data = running,\n    prior = varying_slope_priors_2,\n    iter = 4000, threads = threading(5), cores = 10, \n    control = list(adapt_delta = 0.9),\n    sample_prior = 'only' \n)\n\nNow lets look at the prediction from the model\n\nrunning |&gt;\n    add_predicted_draws(var_mod2) |&gt;\n    ggplot(aes(x = .prediction, group = .draw)) +\n    geom_density() +\n    coord_cartesian(xlim = c(0, 300))\n\n\n\n\n\n\n\n\nTo my eye these look pretty similar so lets fit both and then compare the three models. Now lets check the corss validation statistics\n\nvar_mod1 = update(\n    var_mod1,\n    sample_prior = 'no'\n)\n\nvar_mo2 = update(var_mod2,\n    sample_prior = 'no')\n\nNow we should go and look at how far off each model is\n\np1 = pp_check(runner_model_final, ndraws = 25) +\nlabs(title = 'Random Intercepts only')\n\np2 = pp_check(var_mod1, ndraws = 25) +\n    labs(title = 'Random Intercepts & Slopes', subtitle =  'ETA = 1')\n\np3 = pp_check(var_mo2, ndraws = 25) +\n    labs(title = 'Random Intercepts & Slopes', subtitle = 'ETA = 1.5')\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\nThese all look vaguely the same so now we should check how well they predict out of sample\n\nloo_stats = tribble(~model_name, ~model,\n                    'Random Intercepts', runner_model_final,\n                    'Random Intercepts & Slopes \\n ETA = 1', var_mod1,\n                    'Random Intercepts & Slopes \\n ETA = 1.5', var_mo2) |&gt;\n            mutate(loo = purrr::map(model, \\(x) loo(x))) |&gt;\n            mutate(loo_stuff = purrr::map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |&gt;\n            select(model_name, loo_stuff) |&gt; \n  unnest(loo_stuff) |&gt; \n  filter(statistic == \"elpd_loo\") |&gt; \n  arrange(desc(Estimate))\n\nloo_stats\n\nMy eyes do not deceive effectively we aren’t extracting more predicitive accuracy out of the model. Which is great we can use the random intercepts safely. Now lets play around with the cauchy prior on the random intercepts model and then see how this effects things.\n\npriors_runners = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(normal(2.5, 1), class = b),\n    prior(cauchy(0,10), class = sigma),\n    prior(cauchy(0,10), class = sd)\n)\n\nrunner_model = brm(\n    net ~ age_c + (1|runner), \n    data = running, \n    prior = priors_runners,\n    sample_prior = 'only'\n)\n\ncheck_models_cauchy = running |&gt;\n    add_predicted_draws(runner_model) \n\nggplot(check_models_cauchy, aes(x = .prediction, group = .draw)) +\n    geom_density() +\n    coord_cartesian(xlim = c(0, 300))\n\n\n\n\n\n\n\n\nWow umm the predictions look uber wonky. Lets see what happens when we fit the model\nWhen we plot the posterior predictions we see that we have slightly heavier tails which is kind of the point of using the cauchy distribution.\n\npp_check(runner_model_cauchy, ndraws = 25)\n\n\n\n\n\n\n\n\nNow lets go and grab the posterior predictions.\n\ncauchy_preds = running |&gt;\nadd_linpred_draws(runner_model_cauchy, ndraws = 200, re_formula = NA) |&gt;\nggplot(aes(x = age, y = net)) +\n     stat_lineribbon(aes(y = .linpred)) +\n     labs(title = 'Runner Model with Cauchy Priors') \n\n\nlpd + cauchy_preds\n\n\n\n\n\n\n\n\nThese are vaugely the same in terms of posterior prdictions. The credible intervals for the cuachy priors seem to be a little bit different.\n\nloo_stats = tribble(~model_name, ~model,\n                    'Random Intercepts exponential prior', runner_model_final,\n                    'Random Intercepts Cauchy prior', runner_model_cauchy,\n                    'Random Intercepts & Slopes \\n ETA = 1', var_mod1,\n                    'Random Intercepts & Slopes \\n ETA = 1.5', var_mo2) |&gt;\n            mutate(loo = purrr::map(model, loo)) |&gt;\n            mutate(loo_stuff = purrr::map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |&gt;\n            select(model_name, loo_stuff) |&gt; \n  unnest(loo_stuff) |&gt; \n  filter(statistic == \"elpd_loo\") |&gt; \n  arrange(desc(Estimate))\n\nloo_stats |&gt;\n    tinytable::tt()\n\nThe cauchy prior does slightly bettter than the random intercepts model with a exponential prior but not so dramatic that I would be sweating about it.\n\n\n\n\nAs you might imagine there are lots of structures that we can model. A particularly relevant setup is the mountain climbers example. We often have nested structure so in this case we have climber in expedition teams. For MMM’s we have people within channels and within that we have additional structures that we may know based off of demographic information or clusters that we can model. In this case we will start small and then we can build on these fundamentals because we also have expeditions within season and then age.\n\ndata('climbers_sub', package = 'bayesrules')\n\nclimbers = climbers_sub |&gt;\n    select(expedition_id, member_id, success, year, season,\n         age, expedition_role, oxygen_used) |&gt; \n  mutate(age_c = scale(age, scale = FALSE))\n\nextract_attributes &lt;- function(x) {\n  attributes(x) %&gt;%\n    set_names(janitor::make_clean_names(names(.))) %&gt;%\n    as_tibble() %&gt;%\n    slice(1)\n}\n\nunscaled_climbers &lt;- climbers %&gt;%\n  select(ends_with(\"_c\")) |&gt; \n  summarize(across(everything(), ~extract_attributes(.))) |&gt; \n  pivot_longer(everything()) |&gt; \n  unnest(value) |&gt; \n  split(~name)\n\nLets get some basic summary statistics\n\nclimber_expedition = climbers |&gt;\n    group_by(expedition_id) |&gt;\n    summarise(total = n(),\n              prop_success = mean(success))\n\nsuccess_by_size = ggplot(climber_expedition, aes(x = total, y = prop_success)) +\n    geom_count() +\n    labs(x = 'Expedition Team Size', y = 'Proportion of Expedition Team That Finished')\n\ndist_of_team_finished = ggplot(climber_expedition, aes(x = prop_success)) +\n    geom_histogram() +\n    labs(x = 'Proportion of Expedition Team That Finished')\n\nsuccess_by_size + dist_of_team_finished\n\n\n\n\n\n\n\n\nNow lets look at the success by size by season\n\nclimbers |&gt;\n    mutate(total = n(),\n              prop_success = mean(success), .by = expedition_id) |&gt;\n          ggplot( aes(x = total, y = prop_success)) +\n    geom_count() +\n    labs(x = 'Expedition Team Size', y = 'Proportion of Expedition Team That Finished') +\n    facet_wrap(vars(season))      \n\n\n\n\n\n\n\n\nThis is an interesting partial pooling thing that we will look at bit later. Most of the expeditions happen during Spring and Autumn with only a few happening in winter with close to no success and one during summer with close to 100 percent success. So seasons are definitely something that we would want to model.\nIf we look at the model in the book we expect that whether or not you use oxygen is importanat\n\nclimbers |&gt;\n    group_by(age, oxygen_used) |&gt;\n    summarise(success_rate = mean(success)) |&gt;\n    ggplot(aes(x = age, y = success_rate, color = oxygen_used)) +\n    geom_point()\n\n\n\n\n\n\n\n\nAs we can see there age is a pretty poor predictor of a success rate compared to oxygen. So now lets set some priors.\n\nclimbing_priors = c(\n    prior(normal(0, 2.5), class = Intercept),\n    prior(normal(0, 2.5), class = b),\n    prior(exponential(1), class = sd)\n)\n\nclimbing_prior_mod = brm(\n    success ~ age_c + oxygen_used + (1 | expedition_id),\n    data = climbers, \n    family = bernoulli(link = 'logit'),\n    prior = climbing_priors,\n    sample_prior = 'only'\n)\n\ncheck_prds = climbers |&gt;\n    add_predicted_draws(climbing_prior_mod, ndraws = 1000)\n\nggplot(check_prds, aes(x = age, y = .prediction, color = oxygen_used)) +\n    geom_dots(aes(side = ifelse(.prediction == '1', 'bottom', 'top')), pch = 19, scale = 0.2)\n\n\n\n\n\n\n\npp_check(climbing_prior_mod)\n\n\n\n\n\n\n\n\nHmmm it seems that we could probably do a bit better. If we check the coefficients the implication is that if we use oxygen then it will decrease our likelihood of survival which intuitively we know is crazy! Once again working with\n\n\n\nLets say we have some ordered predictor like education. We would expect that the difference between elementary and college is not linear. The effect would look something like this\n$$\n\\[\\begin{aligned}\n1(Elementary) \\phi_i = 0 \\\\\n\n2(\\text{Middle School}) \\phi_i = \\delta_1 \\\\\n\n3(\\text{High School})  \\phi_i = \\delta_1 + \\delta_2 \\\\\n\n4 (\\text{Some College}) \\phi_i = \\delta_1 + \\delta_3\n\\end{aligned}\\]\n$$\nAnd so on and so forth. Effectively we are adding the effect current level of education completed plus the effect of the previous level. In our example Elementary school does not really count because it is the first level so the effect of completing elementary school plus the effect of completing middle school is just the effect of middle school. For ordered categories we are sampling from a Dirichlet distribution. Funnily enough these can be thought of as weird beta distributions.\n\n\n\nThese seem to be uber popular with Bayesians who do anything related to time and space. The idea kind of boils down to Tobler’s first law of geography “things that are close to each other are more similar than things that are far away from each other.”\nA Gaussian process is “an inifinte dimensional generalization of multivariate distributions.” In English this says that we use a kernel function that generalizes to infinite dimensions/observations/predictions instead of a conventional variance-covariance matrix. If you think of the varying slopes and intercepts models that we have worked with before there isn’t any ordering to the categories. I have a hierarcical ranking of seasons but that is not a constraint that I would ever impose on a model. If we wanted to include something like distance or time the variance-covariance matrix would be huge because well it is a continous variable. Instead of the standard matrix we just sub in a kernel to take the place of this big unruly matrix. A kernel function willl generalize to infinite dimensions. The kernel gives the covariance between any pair of points as function of their distance. As always we set a prior which is effectively the rate of decay between the tips. Like 0 distance all the way to the max of the distance and the maximum covariance. These are kind of just regularization priors in some respect.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#what-is-bayes-rule",
    "href": "Bayesian-Stats.html#what-is-bayes-rule",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Formally Bayes rule is usually denoted by this equation.\n\\[\nP(A|B) = \\frac{P(A) P(B|A)}{P(B)}\n\\]\nWhile uber important in respect to Bayesian statistics this not always been the easiest way for me to think of Bayes rules with respect to logic of how we do Bayesian statistics. I think its hard to ground it for me because we can just rewrite it a bit and get the law of total probability which doesn’t make it inherently Bayesian. For me I think it is easier to clarify it as\n\\[\nPosterior = \\frac{Prior \\times \\text{Probability of Hypothesis being true | Data}}{\\text{Mean probability of Data}}\n\\]\nOr we are just counting the number of instances that we observed this data and then we average over the prior which get us our average. Practically this is really just so everything adds to 1. To be a Bayesian is to incorporate our beliefs and then update them as the data meets our hypothesis meets the data or as Rich McCelreth argues\n\nThe rules of probability tell us that the logical way to compute the plausibilities, after accounting for the data, is to use Bayes’ theorem.\n\nWe then sample from the posterior to communicate something interesting or important about our model. Like where is the mass of the probability distribution, where is the center of this distribution, etc\nBayesianism, in a way, is a formalization of this process by encoding our beliefs in probability distributions. Our prior is really just our belief about the distribution and plausible range of values for that variable before ever seeing the data. So if we were to set a prior for a coin we would set the prior as somthing kind of loosy goosy as this\n\nprior_heads = rbinom(n = 2, size = 1000, prob = 0.5)\n\nprior_heads[1]/sum(c(prior_heads[1], prior_heads[2]))\n\nSo in this case we are just stating that the probability of heads is about 50 percent over a thousand trials assuming a fair coin. However, lets say that we know the coin is biased in a known way that makes it come up heads 61% of the time. We could then set our prior that it willl come up heads as this\n\nprior_heads_biased = rbinom(n = 2, size = 1000, prob = .61)\n\nThis is not neccessarily all that unique from a standard null hypothesis which we covered in the opening “chapter” of this book. It is the combination of the prior and the posterior which makes the Bayesian machine go brrr.\nThe posterior can we operationalized along these lines\n\\[\nPosterior = \\frac{\\text{Probability of Data} \\times Prior}{\\text{Mean probability of Data}}\n\\]\nWhere we are averaging the probability of the prior. If we derive this out more formally we will see that the posterior is actually proportional to the prodct of the prior and the probability of the data. The posterior distribution contains the relative plausibility of different parameter values, conditional on the data and model. We can overcome a bad prior given an infinite amount of data, but this process will be inefficient and critically we will never reach infinite data. Priors in Bayesian inference are important computationally as well as substantively.\n\n\nSetting a prior is one of the hardest things in Bayesian statistics and the subject a large and rich part of the literature. I think one really important thing to adjudicate is what a prior really is. That way we can reinforce the importance of setting a good one, what to check, and how to check it.\nAccording to Gelman et al. (2013) we can conceptualize the prior distribution along two different lines. There is the population interpretation. This is a little frequentisty but is definitely helpful. Lets say that we have some pseudo population of parameters that the candidate parameter \\(\\beta\\) is drawn from. For simplicity sake lets draw \\(\\beta\\) from as standard normal where we overlay the overall distribution\n\nlibrary(tidyverse)\n\n\nsim_data = \\(beta_val = 0, beta_variance = 1, n = 100){\n  \n  sim_dat = tibble(beta_values = rnorm(n, mean = beta_val, sd = beta_variance),\n                   beta_mean = rep(beta_val, length.out = n),\n                   beta_variance = rep(beta_variance, length.out = n))\n\n  return(sim_dat)\n    \n}\n\npop_sims = map_dfr(c(5, 10, 15), \\(x) sim_data(beta_val = x)) \n\nwide_version = pop_sims |&gt; \n    mutate(id = row_number()) |&gt;\n    pivot_wider(names_from = beta_mean, values_from = beta_values, names_glue = 'mean_{beta_mean}') \n\nplot_dat = wide_version |&gt;\n    mutate(pop_total = pop_sims$beta_values) |&gt;\n    pivot_longer(cols = mean_5:mean_15) |&gt;\n    mutate(nice_labs = as.factor(str_to_title(str_replace(name, '_', \" \"))), \n           nice_labs  = fct_relevel(nice_labs, 'Mean 5', 'Mean 10', 'Mean 15'))\n\n\nggplot(plot_dat) +\ngeom_density(mapping = aes(x = value, y = after_stat(ncount),   fill = nice_labs), \n             stat = \"bin\",  size = 0.5,\n             alpha = 0.7) + \ngeom_density(mapping = aes(x = pop_total, y = after_stat(ncount)), \n                 alpha = 0.9,\n                 color = \"gray30\", size = 0.6, \n             stat = \"bin\",\n             direction = \"mid\") +\n   facet_wrap(vars(nice_labs)) +\n   MetBrewer::scale_fill_met_d(name = 'Lakota') +\n   theme_minimal() +\n   labs(fill = NULL, y = 'Scaled Count') +\n   theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nSo in the population interpretation of priors we have kind of weird population by design. Depending on what ‘draw’ we get our prior could be N(5,1) or it could be N(10,1) where plausible values center around 5 or 10 with a standard deviation of 1. In this setting we are thinking about given the data what is a reasonable set of values that we would expect to see. In this setting we are kind of explicitly using a bit of frequentist logic to bridge the gap. However, instead of assuming that all values between 5-15 are equally plausible we are expliciltly stating that the mass of the distribution will be around some value and the amount of variation that we will have. This could be useful if we have a huge amount of experiments or results banked and we can imagine our plausible beta values as drawn from some distribution of the experiments.\nIn the state of knowledge interpretation of priors we are still using our subject matter expertise of the phenomena, but there may not be a good population to ground our priors on. Say we are launching a completly new product or expanding our product into a new market. We may have some idea about what we are likely to see but we don’t have the same reference population that we can draw from, but we have at least some idea of what our expectation should be.\nAn important thing to note about Bayes Rule is that we can think of Bayes rules along these lines\n\\[\nPosterior = \\frac{\\overbrace{P(A)}^{\\text{Prior}} \\times \\overbrace{\\text{Probability of Data}}^{Likelihood}}{\\text{Average Probability of Data}}\n\\]\nWhat this means is that effectively the posterior is a bit of a compromise between our data and our beliefs as the size of the data get bigger our prior will have less of an influence on our posterior distribution, but it will never completely evaporate.\n\n\n\nMost of the time people say just use your substantive knowledge. But that’s generally not helpful if you haven’t done that in a statistical setting. Lets take it step by step. First we generally outline what we link the distribution of our dependent variable would be. For our outcome variable we may think that its a normal distribution so we would write it like this\n\\[\nOutcome \\sim Normal(\\mu, \\sigma)\n\\]\nIf it is a binary outcome we would write it like this\n\\[\nOutcome \\sim Bernouli(\\text{Number of Trials},\\text{Probability true})\n\\]\nThe next step is is we have to think about our generative model. So to ground our analysis it helps to start from a DAG. We generally have beliefs about an intervention in the world. That way we can start thinking about the potential relationships in our data. We then have to think of the plausible ranges for these values and what the uncertainty around them.\n\\[\n\\begin{align}\n\\textbf{Outcome} \\sim N(\\mu, \\sigma) \\\\[8pt]\n\\textbf{Predictor One} \\sim N (3, 1)\n\\end{align}\n\\]\nWhere we are shifting the mean a bit and putting a somewhat conservative prior. This is generally fine but there are lots of priors we can set.\nLets say that out in the real world by some miracle of god we have a true normal distribution where the mean is zero and the standard deviation is 1. We can set informative, weakly informative, flat/non-informative prior, and a conjugate prior. Conjugate priors are a little bit harder to visualize with the below schema because a conjugate prior is something that also relies on the posterior. Meaning that if it turns out the posterior of our mean comes from the same family then it turns out that our prior is conjugate.\n\nn = 1000\n\nprior_df = tibble(\n    pop = rnorm(n),\n    `Kinda Flat` = rnorm(n, sd = 5), \n    informative = rnorm(n, mean = 0.5, sd = 1)\n) |&gt;\n    pivot_longer(everything(),\n        names_to = 'prior',\n        values_to = 'value'\n    )\n\n\nggplot(prior_df, aes(x = value, fill = prior)) +\n    geom_density(alpha = 0.5) +\n    scale_fill_manual(values = c('Kinda Flat' = '#a40000', 'informative' = '#00b7a7', 'pop' = '#ffcd12')) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis could be better but you kind of get the idea. A flat/uninformative prior is at best a bit like saying “the parameter of interest is somewhere between \\(-\\infty\\) and \\(\\infty\\).” My flat prior in the plot is more akin to a weakly informative prior where we are ruling out impossible values, but not really ruling out extreme values. A weakly informative prior is more akin to like we wouldn’t expect the treatment effect to be all that big but it is not outside of the realm of possibility that due to a weird draw of the experimental population people really take to the treatment. Priors can get pretty crazy because well the real world is messy and making simplyfying assumptions is hard. This is why (most) Bayesians will simulate the world first before even touching the data.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#simulating-worlds",
    "href": "Bayesian-Stats.html#simulating-worlds",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Simulations are superpowerful because we get to play god in a way that we don’t normally get to do as social scientists. We can really simply simulate that you have roughly a 50/50 chance of getting heads or tails.\n\ncoin_flips = replicate(10000, sample(c('heads', 'tails'), 1))\n\ncoin_flips |&gt;\n    as_tibble() |&gt;\n    group_by(value) |&gt;\n    summarise(counts = n()) |&gt;\n    ungroup() |&gt;\n    mutate(probs = counts/sum(counts))\n\nWhile this is intuitive we can see how this varies by number of flips.\n\nnum_flips = 100\n\nflips = sample(c('heads', 'tails'), size = num_flips, replace = TRUE)\n\ncoin_flips = tibble(\n    heads_frequency = cumsum(flips == 'heads')/1:num_flips,\n    flip_number = 1:num_flips\n\n)\n\n\nggplot(coin_flips, aes(x = flip_number, y = heads_frequency)) +\n    geom_line() +\n    geom_hline(yintercept = 0.5) +\n    scale_y_continuous(limits = c(0,1)) +\n    labs(x = 'Flip Number', y = 'Proportion of Heads') +\n    theme_minimal()\n\n\n\n\n\n\n\n\nIn expectation we start getting closer and closer to 50% heads. Neat, but why should we care? Well we can test how well our prior does on various situations with a known truth.\nLets take a randomized control trial with unobserved confounding using the really excellent DeclareDesign package.\n\noptions(scipen = 999)\nlibrary(DeclareDesign)\n\nrct &lt;-\n  declare_model(N = 100,\n                U = rnorm(N),\n                potential_outcomes(Y ~ 0.2 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\nfake_data = draw_data(rct)\n\nSo now we have some fake data where we can display the ‘truth’ or in this case 0.2\n\ndiagnose_design(rct)\n\nWhen we go and estimate it on some fake data we can see how keeping or omitting the unobserved confounding.\n\ncontrol_for_confounding = lm(Y ~ Z + U, data = fake_data)\n\nno_controls = lm(Y ~ Z, data = fake_data)\n\n\nmodelsummary::modelsummary(list('Controls Added' = control_for_confounding,\n                                 'No Controls' = no_controls),\n                          gof_map = 'nobs',\n                          stars = TRUE)\n\nThis is pretty cool to see how this can go if we don’t account for things that should be accounted for. The same general principle applies in Bayesian analysis. The reason we simulate out a RCT is that they are super expensive! We want to diagnose what could go wrong before we tell our partners what to do. The same idea applies for Bayes because we have two separate problems that can make it hard to tell what is going on. We have computational problems that can arise due to how Bayesian models are fit and we have modeling problems which are really just scientific problems. Conceptually these are somewhat distinct but practically these two run into each other all the time. We can isolate some of the computational mechanics of fitting a bad model on data we know is ‘good’. This makes fitting lots of models easier.\nHow should we simulat the data? Typically we will define a parameter that seems reasonable! So if we are trying measure. So if we were trying to model the impact of a treatment on conversion rate aka how often do we move from a free user to a subscriber setting simulating a uniform distribution across treatment and control.\n\nlibrary(patchwork)\nlibrary(brms)\nlibrary(modelsummary)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\nconversions_tibble = tibble(\n    Control = runif(100, min = 0, max = 100),\n    Treatment = runif(100, min = 0, max = 100),\n) |&gt;\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none') \n\n\n\n\n\n\n\n\nWe wouldn’t expect that by doing nothing that conversion rate is uniformly distributed between 0 and 100 percent. Conversion rate for Netflix or established streaming services is probably closer to something that looks like this.\n\n\nCode\nconversions_tibble_reasonable = tibble(\n    # most in the control group don't convert \n    Control = rbeta(n = 100, shape1 = 3, shape2 = 7) * 10,\n    # people in the control group are slightly more likely to convert\n    Treatment = rbeta(n = 100, shape1 = 6, shape2 = 4) * 10,\n) |&gt;\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble_reasonable, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none')",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#checking-our-models",
    "href": "Bayesian-Stats.html#checking-our-models",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "There are lots of ways to check our model! One way that is fairly common because Gelman reccomends it is prior predictive checks. The first is a prior predictive check which is just a way for us to check on what influence the prior is going to have on the posterior distribution. As our data grows the influence of our prior is going to decrease and the likelihood function is going to start to have more influence on the posterior distribution. Or in other words how probable the observed data is given different value of the model. However, they are not totally irrelevant the prior can impose a form of regularization shrinking the posterior predictions back towards the ‘true value.’ This is to say that even in large-n settings a well calibrated prior is still important in simple models. In much more complex models the prior is going to do a lot more.\nIn a sense we did something similar but what we are doing is modeling the problem only using the priors.\n\ntitanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\ntitanic &lt;- subset(titanic, PClass != \"*\")\n\nf &lt;- Survived ~ SexCode + Age + PClass\n\n\n\nmod_prior &lt;- brm(PClass ~ SexCode + Age,\n    data = titanic,\n     prior = c(\n        prior(normal(0, 3), class = b, dpar = \"mu2nd\"),\n        prior(normal(0, 3), class = b, dpar = \"mu3rd\")),\n    family = categorical(link = logit),\n    sample_prior = \"only\")\n\n\npp_check(mod_prior)   \n\n\n\n\n\n\n\n\nIn this case we the prior does okay matching the observed data. It could definitely be better\n\n\n\n\n\n\nLets break down some brms syntax first.\n\nprior - somewhat self explantory\nnormal - somewhat self explantory\nclass = b - Set the prior at the population level.\ndpar = 'class' - Here we are telling it that we are making an assumption about the probability of being in second or 3rd class relative to 1st class\n\n\nmod_prior |&gt;\n    modelsummary()\n\nOne important thing to note is that brms is based off of lme4 which is one of the premier multilevel modeling packages for frequentist. As a political scientist we aren’t really multilevel model people we are more throw OLS at everything people so some of the terminology around is different.\nIn OLS political science land we would say we include ‘class fixed effects’ to refer to a model that does something like this.\n\n# this is a bad model don't judge \nlogit_fe = glm(Survived ~ Age + SexCode + factor(PClass), data = titanic, family = binomial(link = 'logit'))\n\nmodelsummary(logit_fe)\n\nWhen we say class fixed effects we really just mean including an intercept per classs. In multilevel land what we mean by a fixed effect is really just saying that these effects are constant across groups. So think of the effect of school on classroom performance or the effect of product popularity on ROI on adverstising channels. The way we would replicate the this is something to the effect of\n\nbrm(Survived ~ Age + (1|PClass), data = titanic) |&gt;\n   modelsummary()\n\nAdmittedly also a really bad model but you get the idea.\n\n\n\nWe can then check our priors. We can also set priors on the intercept in our Titanic example if we stripped away age, class, and sex what would our baseline expectation of survival be? In an RCT this is really just what is a reasonable expectation for our control group. For other more complex models we could put priors on the effect of time, space, interactions between groupings and so much more.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#using-brms",
    "href": "Bayesian-Stats.html#using-brms",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "As an R Bayesian we use brm mostly. We are going to walk through a variety of models that you have fit before using frequentist methods using brms. As the excercise progressed it reallly just became redoing the Statistical Rethinking\n\n\nSo as a good Bayesian we are going to set some priors. First we are going to set a prior for the height in cm and then we are going to set a scale parameter for the weight in kg’s and stretches the distribution. The sigma parameter must be positive. The alpha parameter we are going to think about well what if all the covariates were zero? What would a reasonable set of values be? In this case what would be the weight of an individual be if we never took into account height? Well a somewhat reasonable prior is just taking the average of the weight and spreading it out.\nStan really likes when you scale things because well it makes everything easier to compute. Rescaling in this case gives us the benefit of making the intercept interpretable. Inside the regression the intercept parameter now just becomes the expected weight of an individual is when they are of average height.\n\ndata('Howell1', package = 'rethinking')\n\njust_adults = Howell1 |&gt;\n    filter(age &gt;= 18) |&gt;\n    mutate(height_z = scale(height))\n\n\n\npriors = c(# this is just weight in kilos so approx 132 lbs \n           prior(normal(60, 10), class = Intercept),\n           # this is just centering on the new variable. Now we are also \n           # this is kilos/cm with a spread of 10^2 \n           prior(normal(0, 10), class = b, coef = 'height_z'),\n           prior(uniform(0,50), class = sigma, ub = 50))\n\nfirst_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors,\n    data = just_adults\n)\n\n\nplot(first_cut)\n\n\n\n\n\n\n\n\nSo far the chains look really nice there is great mixing. Everything you would want! However, if we simulate our prior we see an interesting phenomena\n\nsample_prior = first_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors,\n    data = just_adults,\n    sample_prior = 'only'\n)\n\nheight_scale &lt;- attributes(just_adults$height_z) %&gt;%\n  set_names(janitor::make_clean_names(names(.)))\n\ndraws_prior &lt;- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 1000)) |&gt;\n                add_epred_draws(sample_prior, ndraws = 1000) |&gt;\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot(draws_prior, aes(x = height_unscaled, y = .epred, group = .draw)) +\n    geom_line(alpha = 0.2) +\n    coord_cartesian(xlim = c(130, 170), ylim = c(10, 100)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis plot looks super wonky we wouldn’t expect that as height increases weight would decrease which seems to be the case for a fair number of cases. What we are seeing is that not out maliciousness we have set up a weird world for our model to calibrate to.\n\npp_check(sample_prior)\n\n\n\n\n\n\n\n\nIf we look a the posterior predictions they look a little weird. We have some draws that go negative implying there are negative weights. To recalibrate our model a bit we need to simply choose a prior that constrains our world a bit more.\n\npriors2 = c(prior(normal(60, 10), class = Intercept),\n            # here we are actually just constraining \n            # beta to be positive \n           prior(lognormal(0, 1), class = b, lb = 0 ),\n            prior(uniform(0,10), class = sigma, ub = 10))\n\nsample_prior2 =  brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors2,\n    data = just_adults,\n    sample_prior = 'only'\n)\n\n\n\ndraws_prior2 &lt;- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 1000)) |&gt;\n                add_epred_draws(sample_prior2, ndraws = 1000) |&gt;\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot(draws_prior2 ,aes(x = height_unscaled,  y = .epred, group = .draw)) +\n    geom_line(alpha = 0.2) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis is definitely a lot better! Out of 1000 draws we get only two weird samples, but that happens. For the most part we can live with this.\n\npp_check(sample_prior2)\n\n\n\n\n\n\n\n\nThis is definitely at least encouraging! We are not getting really really weird predictions. There are definitely some extreme ones in there, but I wouldn’t be like to shocked.\n\nfinal_model = first_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors2,\n    data = just_adults\n)\n\n\nfinal_draws = tibble(height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 500)) |&gt; \n  add_predicted_draws(final_model, ndraws = 100) |&gt;\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\n\nggplot(just_adults, aes(x = height, y  = weight)) +\n    geom_point(alpha = 0.5) +\n    stat_lineribbon(data = final_draws,\n    aes(x = height_unscaled, y = .prediction), .width = 0.95, alpha = 0.2, inherit.aes = FALSE) +\n    theme_minimal() +\n    theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nShe looks great! What about the posterior predictions?\n\npp_check(final_model)\n\n\n\n\n\n\n\n\nShe is beautiful. However if we look at the data\n\nggplot(just_adults, aes(x = height, y = weight, color = as.factor(male))) +\n    geom_point() +\n    theme_minimal()\n\n\n\n\n\n\n\n\nAs you would expect we see that there group differences. So if our dag looks like this.\n\nlibrary(ggdag)\nlibrary(dagitty)\n\ncoords = list(x = c(Height = 0, Sex = 1, Weight = 2),\n              y = c(Height = 0, Sex  = 1, Weight = 0))\n\nlabs = c(Height = 'Height', Sex = 'Sex', Weight = 'Weight')\n\ndagify(Weight ~ Height + Sex,\n        Height ~ Sex,\n        outcome = 'Weight',\n        exposure = 'Height',\n        labels = labs,\n        coords = coords) |&gt;\n    ggdag_status(use_labels = 'label', text = FALSE) +\n    guides(fill = 'none', color = 'none') +\n    theme_dag()\n\n\n\n\n\n\n\n\nRich advises using index variables for things we would normally make a factor or an indicator variable. His argument is that this makes it easier to set a prior on each the different levels of the category. What this does is split apply our prior to both levels of sex. If we wanted to get crazy we could actually estimate a multilevel model. Will it outperform a linear model? ¯_(ツ)_/¯. In this case we could create a fairly neutral prior\n\njust_adults = just_adults |&gt;\n    mutate(sex = factor(male),\n           height_z = as.numeric(height_z))\n\nsex_priors = c(\n    # we do be because we are going to specify a model sans intercept\n    prior(normal(60, 10), class = b),\n    prior(uniform(0,10), class = sigma, lb = 0, ub = 10)\n)\n\nsex_model = brm(weight ~ 0 + sex,\n                prior = sex_priors,\n                data = just_adults)\n\nsex_model\n\nNow we have our indicator variables.\n\nsw_post_means = sex_model |&gt;\n    gather_draws(b_sex0, b_sex1)\n\n\nggplot(sw_post_means, aes(x = .value, fill = .variable)) +\n    stat_halfeye() +\n    labs(fill = NULL, x = 'Posterior mean weight') +\n    theme_minimal() +\n    theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\nSo we could just subtract the the variables like this.\n\ndiffs_manual = sw_post_means |&gt;\n    pivot_wider(names_from = .variable, values_from = .value) |&gt;\n    mutate(diff = b_sex1 - b_sex0)\n\ndiffs_brms = sex_model |&gt;\n    spread_draws(b_sex0, b_sex1) |&gt;\n    mutate(diff = b_sex1 - b_sex0)\n\ntinytest::expect_equal(diffs_brms, diffs_manual)\n\nWe could also use marginaleffects.\n\nlibrary(marginaleffects)\n\nme_contrasts = get_draws(avg_comparisons(sex_model))\n\ntinytest::expect_equal(me_contrasts$draw, diffs_brms$diff)\n\nggplot(me_contrasts, aes(x = draw)) +\n    stat_halfeye() +\n    labs(x = 'posterior contrasts') +\n    theme_minimal()\n\n\n\n\n\n\n\n\nWe can also get posterior prediced draws with marginal effects via\n\npreds =  tibble(sex = c(\"0\", \"1\")) |&gt; \n  add_predicted_draws(sex_model, ndraws = 1000)\n\nggplot(preds, aes(x = .prediction, fill = sex)) +\n    stat_halfeye() +\n    theme_minimal() +\n    theme(legend.position = 'bottom') \n\n\n\n\n\n\n\n\nJust to be sure lets make sure the model makes sense\n\ncheck_sex_priors = brm(\n    weight ~ 0 + sex,\n    prior = sex_priors,\n    data = just_adults,\n    sample_prior = 'only'\n)\n\n\npp_check(check_sex_priors)\n\n\n\n\n\n\n\n\nEhh it somewhat hard to tell but in all honesty it seems like we are a little bit streching the issue a bit. Now we can use a little parlor trick to avoid some of the the wonky syntax of the nl argument of brms. It will kick a stink but she still works.\n\npriors &lt;- c(prior(normal(60, 10), class = b, coef = 'sex1'),\n            prior(lognormal(0, 1), class = b, coef = 'sex1:height_z'),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex &lt;- brm(\n  bf(weight ~ 0 + sex + sex:height_z),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors\n)\n\npriors2 &lt;- c(prior(normal(60, 10), class = b, nlpar = a),\n            prior(lognormal(0, 1), class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex_wonky =brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors2,\n  chains = 4, cores = 4\n)\n\n# model_height_sex \n# Regression Coefficients:\n#               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sex0             45.15      0.45    44.27    46.00 1.00     2560     2372\n# sex1             45.15      0.46    44.22    46.04 1.00     2734     2716\n# sex0:height_z     5.08      0.48     4.14     6.02 1.00     2846     2773\n# sex1:height_z     4.65      0.42     3.82     5.47 1.00     2716     2618\n# Further Distributional Parameters:\n#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sigma     4.27      0.16     3.96     4.60 1.00     4054     2961\n\n# model_height_sex_wonky\n# Regression Coefficients:\n#               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sex0             45.15      0.45    44.27    46.00 1.00     2560     2372\n# sex1             45.15      0.46    44.22    46.04 1.00     2734     2716\n# sex0:height_z     5.08      0.48     4.14     6.02 1.00     2846     2773\n# sex1:height_z     4.65      0.42     3.82     5.47 1.00     2716     2618\n# \n# Further Distributional Parameters:\n#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sigma     4.27      0.16     3.96     4.60 1.00     4054     2961\n\nSome of the logic of brms with the posterior is a little weird for me but there is not neccessarily a good comparisions in marginaleffects that I am aware of, but is probably going to be obvious if I just ask Vincent.\n\nsex_height_weight_post_pred &lt;- expand_grid(\n  height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 50),\n  sex = 0:1\n) |&gt; \n  add_predicted_draws(model_height_sex, ndraw = 4000) |&gt;\n  compare_levels(variable = .prediction, by = sex, comparison = list(c(\"0\", \"1\"))) |&gt; \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\n\nggplot(sex_height_weight_post_pred, aes(x = .prediction)) +\n  stat_halfeye(fill = 'red') +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHow do we interpret this kind of wonky thing? Well we are really justing talking about the distribution of the predicted differences between men and women’s weight. If we want to talk about the differences meaningfully between two different distributions then we have to take the differences. T\n\n\n\nReally what we want is one model to represent the entire causal system to get the effect of sex on weight without a ton of additional work. What we are going to do is basically estimate the effect of sex on weight through height. It is a little bit easier to think of the effect of like this.\n\ndagify(Weight ~ Height,\n       Height ~ Sex) |&gt;\n    ggdag() +\n    theme_dag()\n\n\n\n\n\n\n\n\nTo do this we have to set our priors accordingly to account for the path through height.\n\npriors &lt;- c(prior(normal(60, 10), resp = weight, class = b, nlpar = a),\n            prior(lognormal(0, 1), resp = weight, class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), resp = weight, class = sigma, lb = 0, ub = 10),\n            # prior(normal(160, 10), resp = height, class = b),\n            prior(normal(0, 1), resp = heightz, class = b),\n            prior(uniform(0, 10), resp = heightz, class = sigma, lb = 0, ub = 10))\n\nmodel_luxury &lt;- brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE) + \n    bf(height_z ~ 0 + sex) + \n    set_rescor(TRUE),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors\n)\n\n\nluxury_post_mean_diff &lt;- expand_grid(\n  height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 50),\n  sex = 0:1\n) |&gt; \n  add_epred_draws(model_luxury) |&gt;\n  compare_levels(variable = .epred, by = sex, comparison = list(c(\"1\", \"0\")))\n\nluxury_post_mean_diff |&gt; \n  filter(.category == \"weight\") |&gt; \n  ggplot(aes(x = .epred)) +\n  stat_halfeye(fill = 'pink') +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen − Men\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBrms has lots of ways to draw predictions from the posterior distribution! This is great, but they are a little bit different in subtle ways that can make it a hard to grasp at first. As with lots of things we are just going to follow an excellent Andrew Heiss blog post to understand the differences.\n\n\nCode\nlibrary(palmerpenguins)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(ggtext)\n\ntheme_pred &lt;- function() {\n  theme_minimal(base_family = \"Roboto Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 0),\n          legend.title = element_text(face = \"bold\"))\n}\n\ntheme_pred_dist &lt;- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          axis.text.y = element_blank(),\n          panel.grid.major.y = element_blank(),\n          panel.grid.minor.y = element_blank())\n}\n\ntheme_pred_range &lt;- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          panel.grid.minor.y = element_blank())\n}\n\nclrs &lt;- MetBrewer::met.brewer(\"Java\")\n\npenguins =   penguins |&gt;\ndrop_na(sex) |&gt; \n  mutate(is_gentoo = species == \"Gentoo\") |&gt; \n  mutate(bill_ratio = bill_depth_mm / bill_length_mm)\n\n\nmodel_normal &lt;- brm(\n  bf(body_mass_g ~ flipper_length_mm),\n  family = gaussian(),\n  data = penguins\n)\n\n\n\npenguins_avg_flipper &lt;- penguins |&gt; \n  summarize(flipper_length_mm = mean(flipper_length_mm))\n\n# Extract different types of posteriors\nnormal_linpred &lt;- model_normal |&gt; \n  linpred_draws(newdata = penguins_avg_flipper)\n\nnormal_epred &lt;- model_normal |&gt; \n  epred_draws(newdata = penguins_avg_flipper)\n\nnormal_predicted &lt;- model_normal |&gt; \n  predicted_draws(newdata = penguins_avg_flipper,\n                  seed = 12345)  \n\n\np1 &lt;- ggplot(normal_linpred, aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[3]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Linear predictor** &lt;span style='font-size: 14px;'&gt;*µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist() +\n  theme(plot.title = element_markdown())\n\np2 &lt;- ggplot(normal_epred, aes(x = .epred)) +\n  stat_halfeye(fill = clrs[2]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] and *µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_epred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\np3 &lt;- ggplot(normal_predicted, aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[1]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(2900, 5500)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Normal(*µ*, *σ*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\n(p1 / plot_spacer() / p2 / plot_spacer() / p3) +\n  plot_layout(heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\n\n\nSo both posterior_linpred and posterior_epred are lookig at the \\(\\mu\\) part of the model or effectively\n\\[\n\\mu = \\alpha + \\beta\\text{Flipper Length}\n\\]\nFor posterior_linpred what we are doing are we going to take the individual predictions from our model, the intercept and multiply them by the average flipper length.\n\nlinear_predictions_manual = model_normal |&gt;\nspread_draws(b_Intercept, b_flipper_length_mm) |&gt;\n mutate(mu = b_Intercept + \n           (b_flipper_length_mm * penguins_avg_flipper$flipper_length_mm))\n\ntinytest::expect_equal(linear_predictions_manual$mu, normal_linpred$.linpred)\n\nReally importanly what you will notice is that we don’t incorporate any inforamtion from \\(\\sigma\\) or the posterior or the part of the model that remains uncertain after modeling the outcome. As you might imagine that may have a some important implications on what the predictions from the posterior look like!\nposterior_predict uses this sigma term to effectively take samples from the normal around the mu with the sigma as a standard deviation. So the only thing we change in our code, is this\n\npostpred_manual = model_normal |&gt;\nspread_draws(b_Intercept, b_flipper_length_mm, sigma) |&gt;\n mutate(mu = b_Intercept + \n        (b_flipper_length_mm * penguins_avg_flipper$flipper_length_mm),\n       predictions = rnorm(n(), mu, sigma))\n\ntinytest::expect_equal(postpred_manual$predictions, normal_predicted$.prediction)\n\nThat makes sense since we are taking some samples. However, if we plot them they are going to look the same.\n\nmanual = ggplot(postpred_manual, aes(x = predictions)) + \n    stat_halfeye(fill = 'hotpink') +\n    labs(title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Normal(*µ*, *σ*)&lt;/span&gt;\", subtitle = \"rnorm(b_Intercept + (b_flipper_length_mm * 201), sigma)\") +\n    theme_pred_dist() +\n    theme(plot.title = element_markdown())\n\nmanual /p3\n\n\n\n\n\n\n\n\nSince we are incorporating the remaining uncertainty from the model we are going to have a little bit of a larger spread.\nepreds is a little weird. In many cases they can be the same as the linear predcitons. In fact if we did epredswe would get just get the linear predicitions returned back to us. epreds is short for Expected predicitions. This will combine the E[y] and \\(\\mu\\). Thankfully we have a really easy case to look at\n\nlogit_mod = brm(\n    is_gentoo ~ flipper_length_mm,\n    family = bernoulli(link = 'logit'),\n    data = penguins\n)\n\n\nepreds_logit = logit_mod |&gt;\n    epred_draws(newdata = penguins_avg_flipper)\n\nlinear_pres_logit = logit_mod |&gt;\n    linpred_draws(newdata = penguins_avg_flipper)\n\nepreds_logit_plot = ggplot(epreds_logit, aes(x = .epred)) +\n    stat_halfeye() +\n    labs(title = 'Expected Predictions') +\n    theme_minimal()\n\nlogit_linpreds = ggplot(linear_pres_logit, aes(x = .linpred)) +\n    stat_halfeye() +\n    labs(title = 'Linear Predicitions')+ \n    theme_minimal()\n\n\nepreds_logit_plot/logit_linpreds\n\n\n\n\n\n\n\n\nThese are super different! In this case epreds is going to take the predictions on the probability scale while linpreds is going to use the logit scales or the log odds scale(ewww). One of the things about epreds is it will shift a lot. For a beta regression the expectation will effectively be the average of the posterior’s average.\n\n\n\nWhen we adjust for a continous variable we are going adjust for the actual values of the continous variables and the slope because every value produces a different relationshiop between the treatment and the ouctcome. Scaling helps because we can think of big effects in terms of the mean and the sd.\n\ndata('WaffleDivorce', package = 'rethinking')\n\n\n\n\nwaffle_clean = WaffleDivorce |&gt;\n    janitor::clean_names() |&gt;\n    mutate(across(c(marriage, divorce, median_age_marriage),  \\(x) scale(x), .names = \"{col}_scaled\")) |&gt;\n    mutate(across(c(marriage, divorce, median_age_marriage), \\(x)  as.numeric(scale(x)), .names = \"{col}_z\"))\n\nwaffle_priors = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z'),\n    prior(normal(0,0.5), class = b, coef = 'marriage_z'),\n    prior(exponential(1), class = sigma))\n\n\nscaled_waffles = brm(\n    divorce_z ~ marriage_z + median_age_marriage_z,\n    data = waffle_clean,\n    prior = waffle_priors,\n    sample_prior = 'only' \n)\n\n# range(waffle_clean$median_age_marriage_z)\n\ncheck_priors = tibble(median_age_marriage_z = seq(-3, 3),\n                     marriage_z = 0) |&gt;\n                add_epred_draws(scaled_waffles, ndraws = 100)\n\nggplot(check_priors, aes(x = median_age_marriage_z, y = .epred, group = .draw)) +\n    geom_line(alpha = 0.5)\n\n\n\n\n\n\n\n\nThese are some of the implied regression lines from the model. For the most part we some pretty plausible regression lines. We have some weird ones where as the age of marriage increases the divorce rate decreases. Other regression lines we have mostly flat which I guess would make sense since we are not stratifying by state. We can then go ahead and estimate the model. I will spare myself the tedium of specifying two models. Instead we will estimate the effect of age of marriage through marriage rate. I suppose I didn’t explain it when I did it earlier because I was getting to angry at getting the : syntax to work correctly.\nSo the idea is that we really have two models. The effect of Age on Divorce and the effect of Age on marriage. So we have to tell brms how do to this because simply doing\n\nbrm(divorce_z ~ marriage_z + age_z, data = d)\n\nWould just adjust for the age of marriage in our model. This is great if we are only interested in the effect of marriage rate on divorce but is not our stated goal. There are lots of ways to invoke this in brms. I will probably just separate these into two different objects just for ease of reading.\n\nwaffle_priors_two = c(\n    prior(normal(0, 0.2), class = Intercept, resp = divorcez),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z', resp = divorcez),\n    prior(normal(0,0.5), class = b, coef = 'marriage_z', resp = divorcez),\n    prior(exponential(1), class = sigma, resp = divorcez),\n    prior(normal(0, 0.2), class = Intercept, resp = marriagez),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z', resp = marriagez),\n    prior(exponential(1), class = sigma, resp = marriagez))\n\nSince we technically have two regression models smooshed together we have to add priors for this model as well. To make sure brms doesn’t get mad at us we feed just use resp to tell it what equation the prior is for. We could obviously change the prior around age of marriage. This could get super complicated if we needed it, but for now we are gonna k.i.s.s.\nIn the gender example that we worked on earlier we did set_rescor(TRUE) which is going to allow correlation betwee the two different equations. This makes sense because we are making the assumption that there are probably other factors that influence this relationship that we don’t observe. In our waffle divorce model we would also want to include the effect of region/state. But in our DAG we don’t include it for pedagogical purposes\n\ndivorce_model = bf(divorce_z ~ median_age_marriage_z + marriage_z)\n\nmarriage_rate_model = bf(marriage_z ~ median_age_marriage_z)\n\nconflicted::conflict_prefer('stanfit', 'rstan')\n\nfull_luxury_model = brm(\n    divorce_model + marriage_rate_model + set_rescor(FALSE),\n    prior = waffle_priors_two,\n    data = waffle_clean,\n    cores = 4\n)\n\nWhats cool about this approach is we have modeled a few effects. We can get the effect of age on divorce and age on marriage rate. All we need to do is extract the predictions from the posterior and set the correct response variable.\n\ndivorce_predictions = tibble(median_age_marriage_z = seq(-2, 2, length.out = 40 ),\n                             # make sure we don't use a grid of marriage rates \n                             marriage_z = 0) |&gt;\n                       add_predicted_draws(full_luxury_model, resp = 'divorcez')\n\nmarriage_predictions = tibble(median_age_marriage_z = seq(-2, 2, length.out = 40 )) |&gt;\n                       add_predicted_draws(full_luxury_model, resp = 'marriagez')",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#regularization-and-cross-validation-in-bayesian-models",
    "href": "Bayesian-Stats.html#regularization-and-cross-validation-in-bayesian-models",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Regularization in Bayes works a bit differently then in frequentism/machine learning. Instead of simply adding a penalty term to the sum of the squared residuals we can impose some what ‘stronger weakly informative priors’ or there are priors that are kid of just used for regularization. To pick a model that performs well we should probably have an idea about how to pick a good model.\nA good model is one that balances good fit with flexibility to incorporate new data and describes the world realistically. This is probably the most straightforward way to describe the bias variance tradeoff. We will use the example from the book of brain size vs body mass\n\n# getting tired of  + theme_minmal\n\ntheme_set(theme_minimal())\n\n d &lt;- \n  tibble(species = c(\"afarensis\", \"africanus\", \"habilis\", \"boisei\", \"rudolfensis\", \"ergaster\", \"sapiens\"), \n         brain   = c(438, 452, 612, 521, 752, 871, 1350), \n         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))\nggplot(d, aes(x = mass, y = brain)) +\n    geom_point()\n\n\n\n\n\n\n\n\nThis will also be a useful way to learn how to use the update function. We are standardizing the variables becaus\n\n\n\n\n\n\nwe want to standardize body mass–give it mean zero and standard deviation one–and rescale the outcome, brain volume, so that the largest observed value is 1. Why not standardize brain volume as well? Because we want to preserve zero as a reference point: No brain at all. You can’t have negative brain. I don’t think.\n\n\n\n\nd = d |&gt;\n    mutate(mass_std  = (mass - mean(mass)) / sd(mass),\n         brain_std = brain / max(brain))\n\nbrain_model_priors = c(\n    prior(normal(0.5, 1), class = Intercept),\n    prior(normal(0,10), class = b),\n    prior(lognormal(0,1), class = sigma))\n\n\nbrain_model = brm(\n    brain_std  ~  mass_std,\n    prior = brain_model_priors,\n    data = d,\n    file = 'fits/brain_model',\n    chains = 4, cores = 4\n)\n\nbrain_sqr = update(\n    brain_model,\n    newdata = d,\n    brain_std ~ mass_std + I(mass_std^2),\n    chains =4, cores =4,\n    file = 'fits/brain_model_sqr'\n)\n\nbrain_cubed = update(\n    brain_model,\n    newdata = d, \n         formula = brain_std ~ mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4),\n         iter = 2000, warmup = 1000, chains = 4, cores = 4,\n         seed = 7,\n         control = list(adapt_delta = .995)\n)\n\nI am less dedicated (to this excerise than Soloman Kurz)[https://bookdown.org/content/4857/ulysses-compass.html#the-problem-with-parameters] so I am not going to go through the trouble of messing with stan directly. But we can see the logic of what is going on\n\nbrain_loo_lins =\\(mod, ylim = range(d$brain_std)){\n\n nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200))\n  \n  # simulate and wrangle\n  fitted(mod, newdata = nd, probs = c(.055, .945)) |&gt; \n    data.frame() |&gt; \n    bind_cols(nd) |&gt; \n    \n    # plot!  \n    ggplot(aes(x = mass_std)) +\n    geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5)) +\n    geom_point(data = d,\n               aes(y = brain_std)) +\n    labs(\n         x = \"body mass (std)\",\n         y = \"brain volume (std)\") +\n             coord_cartesian(xlim = c(-1.2, 1.5),\n                    ylim = c(0.3, 1.0))\n  \n\n}\n\nmod_list = list(brain_model, brain_sqr, brain_cubed)\n\nplots = purrr::map(mod_list, \\(x) brain_loo_lins(x)) \n\nwrap_plots(plots, ncol =1)\n\n\n\n\n\n\n\n\nObviously fitting a bunch of terms to like 4 data points is not an ideal way to model data. But this kind of mimics how we do a lot of stats in data science. One way to can shrink our estimates is to use regularization. To do this we can just set a more restrictive prior.\n\ndat = tibble(x = seq(from = - 3.5, to = 3.5, by = 0.01)) |&gt;\n  mutate(a = dnorm(x, mean = 0, sd = 0.2),\n         b = dnorm(x, mean = 0, sd = 0.5),\n         c = dnorm(x, mean = 0, sd = 1.0)) |&gt; \n  pivot_longer(-x) \n\n\nggplot(dat, aes(x = x, y = value, color = name, fill = name)) +\n    geom_area(alpha = 0.4, linewidth = 0.5, position = 'identity') +\n    theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nWhat is really strange if you are coming from a pure machine learning background is that we don’t neccesasrily need to implement a pure version of Ridge regression in Bayes. We can simply place more restrictive priors on the model using the good old fashion \\(\\beta \\approx N(0,1)\\) or placing a Laplace prior on the coefficients to achieve the same effect.\n\nn_sim   &lt;- 1e3\nkseq    &lt;- 1:5\n\nmake_sim &lt;- function(n, b_sigma) {\n  sapply(kseq, function(k) {\n    print(k);\n    r &lt;- replicate(n_sim, rethinking::sim_train_test(N = n, k = k, b_sigma = b_sigma));c(mean(r[1, ]), mean(r[2, ]), stats::sd(r[1, ]), stats::sd(r[2, ])) \n    }\n    ) |&gt; \n    \n    # this is a new line of code\n    data.frame()\n}\n\ns &lt;-\n  crossing(n  = c(20, 100),\n           b_sigma = c(1, 0.5, 0.2)) |&gt; \n  mutate(sim = map2(n, b_sigma, make_sim)) |&gt; \n  unnest(sim)\n\n\nn_sim   &lt;- 1e3\nn_cores &lt;- 8\nkseq    &lt;- 1:5\n\n# define the simulation function\nmy_sim &lt;- function(k) {\n  \n  print(k);\n  r &lt;- replicate(n_sim, rethinking::sim_train_test(N = n, k = k));\n  c(mean(r[1, ]), mean(r[2, ]), stats::sd(r[1, ]), stats::sd(r[2, ]))\n  \n}\n\n\n# here's our dev object based on `N &lt;- 20`\nn      &lt;- 20\ndev_20 &lt;-\n  sapply(kseq, my_sim)\n\n# here's our dev object based on N &lt;- 100\nn       &lt;- 100\ndev_100 &lt;- \n  sapply(kseq, my_sim)\n\n\n\n\ndev_tibble &lt;-\n  rbind(dev_20, dev_100) |&gt; \n  data.frame() |&gt; \n  mutate(statistic = rep(c(\"mean\", \"sd\"), each = 2) |&gt; rep(x = _, times = 2),\n         sample    = rep(c(\"in\", \"out\"), times = 2) |&gt; rep( x = _, times = 2),\n         n         = rep(c(\"n = 20\", \"n = 100\"), each = 4)) |&gt; \n  pivot_longer(-(statistic:n)) |&gt; \n  pivot_wider(names_from = statistic, values_from = value) |&gt;\n  mutate(n     = factor(n, levels = c(\"n = 20\", \"n = 100\")),\n         npar = str_extract(name, \"\\\\d+\") |&gt; as.double()) |&gt; \n  mutate(npar = ifelse(sample == \"in\", npar - .075, npar + .075))\n\n\n\n\n\n\n\n\n\n\nThe argument that Rich makes is that regularization is often a bit skeptical in Bayesian statistics. The priors are bit harder to tune. In frequentist statistics we search a large number of very small values of the hyperparameter. This can take a fair amount of time. Now think about searching through the posterior distribution for a ton of different models for the ideal prior. This can take a long time. However, there are a lot of work going into regularization priors since we use them for BART and other estimation methods.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#taming-markov-chains",
    "href": "Bayesian-Stats.html#taming-markov-chains",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "All the core algorithms have different ways to sample from the posterior. Stan uses a Hamiltonian Monte Carlo (HMC)\n\nlibrary(animation)\n\nmyU2 &lt;- function( q , a=0 , b=1 , k=0 , d=0.5 ) {\n    s &lt;- exp(q[2]) # sigma on log latent scale\n    mu &lt;- q[1]\n    U &lt;- sum( dnorm(y,mu,s,log=TRUE) ) + dnorm(mu,a,b,log=TRUE) + dnorm(q[2],k,d,log=TRUE)\n    return( -U )\n}\n\n# gradient function\n# need vector of partial derivatives of U with respect to vector q\nmyU_grad2 &lt;- function( q , a=0 , b=1 , k=0 , d=0.5 ) {\n    mu &lt;- q[1]\n    s &lt;- exp(q[2])\n    G1 &lt;- sum( y - mu ) * exp(-2*q[2]) + (a - mu)/b^2 #dU/dmu\n    G2 &lt;- sum( (y - mu)^2 ) * exp(-2*q[2]) - length(y) + (k-q[2])/d^2 #dU/ds\n    return( c( -G1 , -G2 ) ) # negative bc energy is neg-log-prob\n}\n\n# test data\nset.seed(7)\ny &lt;- abs(rnorm(50))\ny &lt;- c( y , -y ) # ensure mean is zero\n\n###########\n# example paths\nlibrary(shape) # for good arrow heads\n# blank(bty=\"n\")\n\n# priors\npriors &lt;- list()\npriors$a &lt;- 0\npriors$b &lt;- 1\npriors$k &lt;- 0\npriors$d &lt;- 0.3\n\n#ss &lt;- ss + 1\nset.seed(42) # seed 9 for examples\n\n# init\nn_samples &lt;- 4\nQ &lt;- list()\nQ$q &lt;- c(-0.4,0.2)\nxr &lt;- c(-0.6,0.6)\nyr &lt;- c(-0.25,0.4)\n\nstep &lt;- 0.02\nL &lt;- 12 # 0.02/12 okay sampling --- 0.02/20 is good for showing u-turns\nxpos &lt;- c(4,2,1,2) # for L=20\n#xpos &lt;- c(2,3,2,1) # for L=55\npath_col &lt;- col.alpha(\"black\",0.5)\n\ndraw_bg &lt;- function() {\n    plot( NULL , ylab=\"log_sigma\" , xlab=\"mu\" , xlim=xr , ylim=yr )\n    # draw contour of log-prob\n    cb &lt;- 0.2\n    mu_seq &lt;- seq(from=xr[1]-cb,to=xr[2]+cb,length.out=50) \n    logsigma_seq &lt;- seq(from=yr[1]-cb,to=yr[2]+cb,length.out=50)\n    z &lt;- matrix(NA,length(mu_seq),length(logsigma_seq))\n    for ( i in 1:length(mu_seq) )\n        for ( j in 1:length(logsigma_seq) )\n            z[i,j] &lt;- myU2( c( mu_seq[i] , logsigma_seq[j] ) , a=priors$a , b=priors$b , k=priors$k , d=priors$d )\n    cl &lt;- contourLines( mu_seq , logsigma_seq , z , nlevels=30 )\n    for ( i in 1:length(cl) ) lines( cl[[i]]$x , cl[[i]]$y , col=col.alpha(\"black\",0.5) , lwd=1 )\n}\n\n\nQ &lt;- list()\nQ$q &lt;- c(-0.4,0.2) # start point\nxr &lt;- c(-0.4,0.4) # x range in plot\nyr &lt;- c(-0.25,0.3) # y range in plot\n\ndraw_bg()\n\nn_samples &lt;- 10\n# points( Q$q[1] , Q$q[2] , pch=4 , col=\"black\" )\npts &lt;- matrix(NA,nrow=n_samples,ncol=3)\n\nfor ( i in 1:n_samples ) {\n\n    Q &lt;- HMC2( myU2 , myU_grad2 , step , L , Q$q , a=priors$a , b=priors$b , k=priors$k , d=priors$d )\n\n    draw_bg()\n\n    # draw previous points\n    if ( i &gt; 1 ) {\n        for ( j in 1:(i-1) ) {\n            V &lt;- 0.9\n            points( pts[j,1] , pts[j,2] , pch=ifelse( pts[j,3]==1 , 1 , 16 ) , col=grau(V) , lwd=2 )\n        }\n    }\n\n    # draw trajectory\n    for ( l in 1:L ) {\n        lines( Q$traj[l:(l+1),1] , Q$traj[l:(l+1),2] , col=\"white\" , lwd=8 )\n        lines( Q$traj[l:(l+1),1] , Q$traj[l:(l+1),2] , col=4 , lwd=5 )\n        ani.record()\n    }\n    #points( Q$traj[2:L+1,] , pch=16 , col=\"white\" , cex=0.3 )\n\n    # draw new point\n    pts[i,1:2] &lt;- Q$traj[L+1,]\n    pts[i,3] &lt;- Q$accept\n\n    #Arrows( Q$traj[L,1] , Q$traj[L,2] , Q$traj[L+1,1] , Q$traj[L+1,2] , arr.length=0.3 , arr.adj = 0.7 , col=4 )\n    #text( Q$traj[L+1,1] , Q$traj[L+1,2] , i , cex=1.2 , pos=1 , offset=0.4 )\n    \n    points( Q$traj[L+1,1] , Q$traj[L+1,2] , pch=ifelse( Q$accept==1 , 1 , 16 ) , col=ifelse( Q$accept==1 , 4 , 2 ) , lwd=2 )\n\n    invisible( replicate( 3 , ani.record() ) )\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a way we are effectively just using gradient descent to get our posterior. What we are doing is just throwing a marble into a bowl with n-dimensions and then just recording where it lands. Since it is a bowl the marble is going to end towards the center of the bowl. We are going to use step size to tell our arm how hard to throw the marble. In gradient descent we really just use these throws to give the marble a hint what path would be faster to minimize a loss function.\nIn two or three dimensions this is hard when we start adding in the complexities of sampling from a huge dimensional space lots of problems can arise. As stated early on problems with computing or divergent chains are some indicators that our model may not be working all that well. When taming our markov chains it can help to set more informative priors. Even Stan doesn’t get mad at you its important to inspect the health of the markov chains. To make this easier on ourselves lets just simulate a model.\n\ndata('Wines2012', package = 'rethinking')\n\n\nwine_clean = Wines2012 |&gt;\n    mutate(score_z = scale(score),\n           wine = as.numeric(wine))\n\npriors = c(\n    prior(normal(0,1), class = b, coef = 'wine'),\n    prior(exponential(1), class = sigma)\n)\n\nwine_score_model = brm(score_z ~ 0 + wine,\nprior = priors,\n data = wine_clean)\n\n\n\nplot(wine_score_model)\n\n\n\n\n\n\n\n\n\n\nMost of the libraries have some built in method of trace plots. Visual diagnostics of MCMC chains tend towards the idea of ‘this looks like a jumbly mess’ so when it looks like a random mess that is good.\n\nlibrary(MetBrewer)\nmanual_trace_data = as_draws_df(wine_score_model)\n\nggplot(manual_trace_data, aes(x = .iteration, y  = b_wine, color = as.factor(.chain))) +\n    geom_line(alpha = 0.5) +\n    scale_color_met_d(name = 'Lakota')\n\n\n\n\n\n\n\n\nThis is kind of what you want it should look like lots of random lines. Lets make some bad priors to see what happens\n\npriors_bad = c(\n    prior(lognormal(0,10), class = b, coef = 'wine'),\n    prior(exponential(4), class = sigma)\n)\n\nbad_wine_score_model = brm(score_z ~ 0 + wine,\nprior = priors_bad,\n data = wine_clean)\n\nbad_chains = as_draws_df(bad_wine_score_model)\n\nggplot(bad_chains, aes(x = .iteration, y = b_wine,\n                       color = as.factor(.chain),\n                       group = as.factor(.chain))) +\n    geom_line() +\n scale_color_met_d(name = 'Lakota')\n\n\n\n\n\n\n\n\nI played up the priors a big but as you can see these chains are really bad. We have what looks to be clear trends in the first and fourth chains. We cahn’t really even see the second and third chain. Overall this is a mess.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#trace-rank-plots-trank-plots",
    "href": "Bayesian-Stats.html#trace-rank-plots-trank-plots",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "We can also use rank orders to get a sense of the health of the chains. Basically no chain should consistently look the best. They should look like some jumbled up mess.\n\nas_draws_df(bad_wine_score_model) |&gt;\n    bayesplot::mcmc_rank_overlay()\n\n\n\n\n\n\n\n\nAs you can see it looks like a few chains are actually broken while others just seem to be doing bettter than others. Good trank plots look more like this:\n\nas_draws_df(wine_score_model) |&gt;\n    bayesplot::mcmc_rank_overlay() +\n    scale_y_continuous(limits = c(30,65)) +\n    scale_color_met_d(name = 'Lakota')\n\n\n\n\n\n\n\n\nwhere no one chain consistently outranks any other chain consistently.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#numeric-diagnositcs",
    "href": "Bayesian-Stats.html#numeric-diagnositcs",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "When chains converge the end of the chain and the beggining of a chain are exploring the same region. Independent chains explore the same or similar region. \\(\\hat{R}\\) is effectively a ratio of variances so we can think of it as\n\\[\n\\hat{R} = \\frac{\\text{Within Chain Variance}}{\\text{Total Variance of All Chains}}\n\\]\nIn practice this means that values closer to give us some indication of how well are chains are doing. Values like 1.01 could tell us that we just need to run the chains for longer or that we need to go back and rethink our priors.\n\n\n\nThis is an estimate of the number of effective samples what this is kind of telling us is that\n\n\n\n\n\n\nImportant\n\n\n\nHow long would the chain be if each sample was independent of the one before it\n\n\nMcElreath argues that we can think of rnorm as the ideal behavior of n_eff where each draw from a normal distribution is independent of the one before it. So our number of n_eff would be equal to what ever you feed the n argument. The same idea applies to n_eff where we are going to get some number that is generally less than the number of samples we are taking, but sometimes Stan will find places where we can grow the length of the chain. This is not a bug because, well, when we are traversing multidimensional spaces we are not going to be as clever about that as some may think. We would see the number off effective samples shrink if we had autocorrelation because past values of the variables are giving us more information about the variable we are modeling so the length of the chain will be much shorter. The toy bad model\n\n\n\n\nsummary(bad_wine_score_model)\n\nAs we can see in the basd wine model we have really really short chains and Rhat values that tell us that the chains aren’t mixing all that well! When we made the trace plot we saw that some chains were actually broken so the short length of chains make a ton of sense. When we compare this to a good model the difference is pretty stark\n\nsummary(wine_score_model)\n\n\n\n\nOften times when we have computational problems with our model we have issues with our scientific problems with our model. Calculating the posterior is hard thats why the racist Sir Ronald Fisher was a coward and took the log of the likelihood. When we are first building a model we will often have problems getting the chains to work well because our assumptions about the world get passsed onto our model. The trouble that we are having sampling from the posterior distribution maybe because it doesn’t make any sense.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#building-an-interaction-in-brms",
    "href": "Bayesian-Stats.html#building-an-interaction-in-brms",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "We often have conditional hypothesis especially in the social sciences. Political science in particular uses a lot of interaction terms. Some of our most cited statistics articles are interaction terms. Importanly for multilevel models we often times have interactions with lots of factors. Inidividuals are going to interact with where they are from or what party they identify with.\n\ndata('rugged', package = 'rethinking')\n\nrugged = rugged |&gt;\n    drop_na(rgdppc_2000) |&gt;\n    mutate(log_gdp = log(rgdppc_2000),\n           log_gdp_z = log_gdp/mean(log_gdp),\n           rugged_std = rugged/max(rugged),\n           rugged_std_c = rugged_std - mean(rugged_std))\n\nLets take a crack at it with some bad priors\n\npriors_first_cut = c(\n    prior(normal(1, 1), class = Intercept),\n    prior(normal(0,1), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_first_cut,\n    sample_prior = 'only'\n)\n\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |&gt;\n    add_linpred_draws(check_priors, ndraws = 100) |&gt;\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n\n\n\n\n\n\n\n\nHere we are getting some pretty crazy looking regression lines when we move back to the unscaled version. By scaling things we can make lots of priors look pretty reasonable but when we put them back on the og scale we are going to run into some issues.\nLets constrain the priors a bit more to the defaultish values Rich suggests\n\npriors_rich = c(\n    prior(normal(1, 0.1), class = Intercept),\n    prior(normal(0,0.5), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_rich,\n    sample_prior = 'only'\n)\n\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |&gt;\n    add_linpred_draws(check_priors, ndraws = 100) |&gt;\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n\n\n\n\n\n\n\n\nHere we are getting slightly more plausible lines but we still have more than our fair share of weird worlds. We could make it a little bit better by tuning the prior a bit more.\n\npriors_rich = c(\n    prior(normal(1, 0.1), class = Intercept),\n    prior(normal(0,0.3), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_rich,\n    sample_prior = 'only'\n)\n\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |&gt;\n    add_linpred_draws(check_priors, ndraws = 100) |&gt;\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n\n\n\n\n\n\n\n\nCool now we just need to add an interaction with Rich’s index approach. Now we can add the priors to each of the ‘intercept terms’. The annoying part is that the nlpar are really just place holders where we are going to set priors for each level of the interaction. This would be fine but for now this would be incredibly annoying.\n\nrugged = rugged |&gt;\n    mutate(cid = ifelse(cont_africa == 1, '1', '2'))\n\nint_priors = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n                prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n                prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n                prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n                prior(exponential(1), class = sigma))\n\n\nint_model_binary = brm(\n    bf(log_gdp_z ~ 0 + a  + b * rugged_std_c,\n    a ~ 0 + cid,\n    b ~ 0 + cid,\n    nl = TRUE),\n    data = rugged,\n    prior = int_priors)\n\nI have never directly interpreted an interaction term in my life and have been told never too. So I am just going to run it through marginaleffects.\n\nprds  = predictions(int_model_binary,\n                    newdata = datagrid(cid = c('1', '2'),\n                                       rugged_std_c = mean))\n\npreds_plot = get_draws(prds) |&gt;\n    transform(type = 'response') |&gt;\n    mutate(cid = ifelse(cid == '1', 'Africa', 'Not Africa'))\n\n\nggplot(preds_plot, aes(x = draw, fill = cid)) +\n    stat_halfeye() +\n    scale_fill_met_d('Lakota') +\n    labs(y = 'Density', x = 'Predicted Values of Log GDP')\n\n\n\n\n\n\n\n\nHere are computing predictions across each level of our binary indicator while holding ruggedness at its mean value. Here we are seeing that the predictionns for continents outside of Africa cluster around above one indicating we would have above average GDP per capita when ruggedness is held at its mean values. This is a little bit weird to think about since we don’t really have a good intuition about the values of the data.\n\nunscaled = preds_plot |&gt;\n    mutate(draw = exp(draw * mean(rugged$log_gdp)))\n\n\nggplot(unscaled, aes(x = draw, fill = cid)) +\n    stat_halfeye() +\n    scale_fill_met_d('Lakota') +\n    labs(y = 'Density', x = 'Predicted Values GDP')\n\n\n\n\n\n\n\n\nTo replicate the work done in Rethinking we can simply do:\n\nnd &lt;- \n  crossing(cid= 1:2,\n           rugged_std = seq(from = -0.2, to = 1.4, length.out = 30)) |&gt; \n  mutate(rugged_std_c = rugged_std - mean(rugged$rugged_std))\n\ncountries &lt;- c(\"Equatorial Guinea\", \"South Africa\", \"Seychelles\", \"Swaziland\", \"Lesotho\", \"Rwanda\", \"Burundi\", \"Luxembourg\", \"Greece\", \"Switzerland\", \"Lebanon\", \"Yemen\", \"Tajikistan\", \"Nepal\")\n\nfit = fitted(\n    int_model_binary,\n    newdata = nd, \n    probs = c(0.015, 0.985)\n) |&gt;\n    data.frame() |&gt;\n    bind_cols(nd) |&gt;\n    mutate(cont_africa = ifelse(cid == 1, 'African Nations', 'Non-African Nations'))\n\ncleaned = rugged |&gt;\n    mutate(cont_africa = ifelse(cid == 1, 'African Nations', 'Non-African Nations'))\n\n\nggplot(cleaned, aes(x = rugged_std, y = log_gdp_z, fill = cont_africa, color = cont_africa)) +\n    geom_smooth(data = fit,\n                aes(y = Estimate, ymin = Q1.5, ymax = Q98.5)) +\n    coord_cartesian(xlim = c(0, 1)) +\n    theme(legend.position = \"none\") +\n    facet_wrap(vars(cont_africa))",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#continous-interactions",
    "href": "Bayesian-Stats.html#continous-interactions",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "data(tulips, package = 'rethinking')\n\nd = tulips |&gt;\n    mutate(blooms_std  = blooms/max(blooms),\n           water_cent = water - mean(water),\n           shade_cent = shade - mean(shade))\n\nFortunately in the case of continous by continous interactions we really only need to put a prior on one term so we can just do.\n\ncont_priors = c(\n    prior(normal(0.5, 0.25), class = Intercept),\n    prior(normal(0, 0.25), class = b, coef = water_cent),\n    prior(normal(0, 0.25), class = b, coef = shade_cent),\n    prior(normal(0, 0.25), class =b, coef = 'water_cent:shade_cent'),\n    prior(exponential(1), class = sigma)\n)\n\ncont_inter = brm(\n    blooms_std ~ water_cent:shade_cent + water_cent + shade_cent,\n    data = d,\n    prior = cont_priors\n)\n\nCool now lets try to recreate the plot in Rethinking using marginaleffects\n\npreds = get_draws(\n    predictions(cont_inter, newdata = datagrid(\n        shade_cent = -1:1,\n        water_cent = -1:1\n    ))\n) |&gt;\n    mutate(nice_labs = glue::glue('Shade (centered) = {shade_cent}'))\n\nggplot(preds, aes(x = water_cent, y = draw, group = drawid)) +\n    geom_smooth(alpha = 1/5) +\n    geom_point(alpha = 0.5) +\n    facet_wrap(vars(nice_labs)) +\n    labs(y = 'Blooms (Standardized)', x = 'Water (centered)')",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#modeling-dgps-of-all-different-flavors.",
    "href": "Bayesian-Stats.html#modeling-dgps-of-all-different-flavors.",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "It would be nice if everything was linear. It is nice that OLS generally performs pretty well in a lot of cases where it has no reason behaving that way. However, modeling the DGP correctly often is helpful if anything to give us some sanity checks.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#logits",
    "href": "Bayesian-Stats.html#logits",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "data(UCBadmit,package = 'rethinking')\n\nadmit_data = UCBadmit\n\nCounts and events are inherently bounded either by force of de facto bounded. We have been using the exponential distribution in a lot of our priors. Effectively the exponetial distribution is just a distribution that tells us about a time to an event that has a constant rate. We have really just been using it to constrain the unexplained variance to be postive. The difficulty with setting priors for a logit is that we are doing it on the log odds scale so they don’t always have an intuitive mapping as they did in OLS land.\nWe can kind of think of it as anything +4 = almost always and anything -4=almost never so when we stretch out the prior or compress it on the logit scale it will create probability distributions we may not have a good intuition of what the prior should be. Fortunately we can get a handle on this using the qlogis and plogis function from R to get a sense of how our priors are going to translate on the log-odds scale and on the probability scale. The function link function can shrink turn somewhat reasonable priors into really extreme values. This kind of makes since in logit world we our outcome is constrained to 1 and 0 while OLS can take on really any value as long it can be computed.\n\n\nCode\ndata(weather_perth, package = 'bayesrules')\n\nweather = weather_perth |&gt;\n    select(day_of_year, raintomorrow, humidity9am, humidity3pm, raintoday) |&gt;\n   mutate(across(c(humidity9am, humidity3pm), \n                ~scale(., scale = FALSE), .names = \"{col}_centered\")) |&gt; \n  mutate(across(c(humidity9am, humidity3pm), \n                ~as.numeric(scale(., scale = FALSE)), .names = \"{col}_c\")) |&gt; \n  mutate(raintomorrow_num = as.numeric(raintomorrow) - 1)\n\nextract_attributes &lt;- function(x) {\n  attributes(x) %&gt;%\n    set_names(janitor::make_clean_names(names(.))) %&gt;%\n    as_tibble() %&gt;%\n    slice(1)\n}\n\nunscaled &lt;- weather %&gt;%\n  select(ends_with(\"_centered\")) |&gt; \n  summarize(across(everything(), ~extract_attributes(.))) |&gt; \n  pivot_longer(everything()) |&gt; \n  unnest(value) |&gt; \n  split(~name)\n\n\nSo if we think that the baseline rate of rain is about 20% we can get see what a reasonable prior on the log-odds scale would look like this:\n\nqlogis(.2)\n\nSo if we think that there is a range between 15% to 25% of rain is normal than we can translate it to roughly around .5 in either direction. It is a little hard to think of what that would translate to so we can use plogis to convert it back to the probablity scale. So we have a range between 8% and 40% chance of rain. This honestly seems fine to me since I don’t have any good sense of how likely or unlikely this is for Peth.\n\nplogis(qlogis(0.2) - (2 * 0.5))\n\nplogis(qlogis(0.2) + (2 * 0.5))\n\nThe priors for how humidity will shape the weather are a bit hard to get my head around because I don’t have a good idea about what makes sense. The Bayes rules book makes the argument that 0.07 makes a lot of sense? If we plug this into the exp ratio\n\nexp(0.07 - 0.07)\n## [1] 1\nexp(0.07 + 0.07)\n\nSo the odds of rain the next will increase by 0-15% for each percentage point increase in humidity\n\npriors_first = c(prior(normal(-1.39, 0.5), class = Intercept),\n                 prior(normal(0.07, 0.035), class = b, coef = 'humidity9am_c'))\n\nclrs &lt;- MetBrewer::met.brewer(\"Lakota\", 6)\n\ncheck_weather_priors = brm(\n    raintomorrow ~ humidity9am_c,\n    data = weather, \n    family = bernoulli(link = 'logit'), \n    prior = priors_first,\n    sample_prior = 'only'\n)\n\np1 &lt;- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |&gt; \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |&gt; \n  add_epred_draws(check_weather_priors, ndraws = 100) |&gt; \n  ggplot(aes(x = humidity9am, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.5, size = 0.5, color = clrs[6]) +\n  labs(x = \"9 AM humidity\", y = \"Probability of rain tomorrow\")\n\np2 &lt;- tibble(\n  humidity9am = seq(0, 100, by = 0.1)\n) |&gt; \n  mutate(humidity9am_c = humidity9am - unscaled$humidity9am_centered$scaled_center) |&gt; \n  add_predicted_draws(check_weather_priors, ndraws = 100) |&gt; \n  group_by(.draw) |&gt; \n  summarize(proportion_rain = mean(.prediction == 1)) |&gt; \n  ggplot(aes(x = proportion_rain)) +\n  geom_histogram(binwidth = 0.02, color = \"white\", fill = clrs[1]) +\n  labs(x = \"Proportion of rainy days in each draw\", y = \"Count\")\n\np1 | p2\n\n\n\n\n\n\n\n\nI think we can feel pretty good about the worlds we have drawn. Lets go ahead and check the posteriors\n\nreal_model =  brm(\n    raintomorrow ~ humidity9am_c,\n    data = weather, \n    family = bernoulli(link = 'logit'), \n    prior = priors_first\n)\n\npp_check(real_model, ndraws = 50)\n\n\n\n\n\n\n\n\nThat is a pretty good looking posterior predictive check. We desire posterior predicitive checks this pretty.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#multilevel-models",
    "href": "Bayesian-Stats.html#multilevel-models",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Confession these guys scare me a bit. Knowing what to call them is fraught they have weird syntax and also just have a ton of machinery behind them. The general idea is that categories tell us something interesting and important about observations in the category and across categories. We are basically using partial pooling where we are not fully treating each category as independent like we would with a fixed effect model.\n\ndata('reedfrogs', package = 'rethinking')\n\nadd_tank = reedfrogs |&gt;\n    mutate(tank = row_number(),\n           preds = ifelse(pred == 'no' ,1L, 2L),\n           group = ifelse(size == 'small', 1L, 2L),\n           log_density_sd = scale(log(density)))\n\nWhen thinking about models with multiple levels like we have in the reed frog example is we need to ask ourselves is how much will each of the clusters vary? In this notation we are trying to learn \\(\\bar{a}\\) which is effectively the average tank and then try to learn how variable the tanks are.\n\\[\n\\begin{aligned}\n\\text{Survived} \\sim Binomial(D_{i}, p_{i}) \\\\\nlogit(p_i) = \\alpha_{T[i]} \\\\\n\\alpha_j \\sim Normal(\\bar{\\alpha, ?}) \\\\\n\\bar{a} \\sim Normal(0, 1.5)\n\\end{aligned}\n\\]\nIt is kind of hard to get an idea we can go through and use cross validation to get an idead of what sigma should be. We are going to have to use some wonky(to me) syntax. In this case if we just didn’t include the | trials(density) it would make the assumption that surv was binary which is mostly fine if that is the case. We want to model the proportion that survived from each tank. So we have to tell brms how many survivd.\n\n\nCode\nwith_trials &lt;- \n  brm(data = add_tank, \n      family = binomial,\n      surv | trials(density) ~  (1 | tank),\n      prior = c(prior(normal(0, 1.5), class = Intercept),  # alpha bar\n                prior(exponential(1), class = sd)),        # sigma\n      iter = 5000, warmup = 1000, chains = 4, cores = 4)\n\ncheck_mod_priors = \\(sigma_values = 1.5){\n   \n   new_prior = c(set_prior(paste0(\"normal(0, \", sigma_values, \")\"), class = 'Intercept'),  # alpha bar\n                 set_prior('exponential(1)', class = 'sd'))\n   \n   m = brm(data = add_tank, \n      family = binomial,\n      surv | trials(density) ~ 1 +  (1 | tank),\n      prior = new_prior,        # sigma\n      iter = 5000, warmup = 1000, chains = 4, cores = 10)\n\n  crit = add_criterion(m, criterion = 'loo') \n\n  return(crit)\n}\n\nsig_values = seq(0.1, 5, by = 0.1)\n\nnames_vec = paste('Sigma = ', sig_values)\n\nnames(sig_values) = names_vec\n\n\ncheck_models = map(sig_values, check_mod_priors)\n\nget_psis = map(check_models, \\(x){\n    psis(x)\n})\n\n\nThis is kind of an expensive task computationally and not all that fun to wrangle. We can actually learn the optimal posterior distribution of sigma from modeling. If we look at the summary of the model we can see that the model learns the optimal value of sigma or the global variation of tanks.\n\ntank_priors = c(prior(cauchy(0, 1.5), class = Intercept),  \n                prior(exponential(1), class = sd))\nwith_trials &lt;- \n  brm(data = add_tank, \n      family = binomial,\n      surv | trials(density) ~  (1 | tank),\n      prior = tank_priors,        \n      iter = 5000, warmup = 1000, chains = 4, cores = 4)\n\n\nwith_trials |&gt;\n    tidy()\n\n\n\n\nCool well Rich doesn’t actually go into how to use this immediately which is always fun. The issue we are going to run into is that when we add features is tha varying effects or effects per intercept can introduce confounding. To side step some of the issues of following along week by week I am going to use the Bayes Rules book.\n\ndata('cherry_blossom_sample', package = 'bayesrules')\n\nrunning = cherry_blossom_sample |&gt;\n    select(runner, age, net) |&gt;\n    drop_na() |&gt;\n    mutate(runner_nice = glue::glue('Runner {runner}'),\n           runner_nice = fct_inorder(runner_nice),\n           across(c(net, age), \\(x) scale(x, scale = FALSE), .names = '{col}_c'))\n\nunscaled = running |&gt;\n    select(ends_with('c')) |&gt;\n    summarise(across(everything(), \\(x) extract_attributes(x))) |&gt;\n    pivot_longer(everything()) |&gt;\n    unnest(value) \n\nWhats nice is that all the runners are in one age group. We would expect that there would be interactions between the runner specific intercepts and the age specific intercepts. Subsequently we would need priors for those age specific intercepts.\n\nggplot(running, aes(x = net)) +\n    geom_histogram(color = 'white')\n\n\n\n\n\n\n\n\nAs we would kind of expect we see spikes around specific times eg some people really want to finish aroun the 90 minute mark or under 2 hours. Since every runner is different in their own way there are some stuff that is going to be different. Some people are naturally faster or have been training for longer. However, we can use information from each of the runners to learn something about the other runners. So tune the varying intercepts part of the prior.\n\nrunning_priors = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(exponential(1), class = sd))\n\n\nfirst_cut = brm(\n    net ~ (1| runner),\n    data = running,\n    prior = running_priors,\n    sample_prior = 'only'\n)\n\np1 = running |&gt;\nadd_predicted_draws(first_cut)  |&gt;\nggplot(aes(x = .prediction, group = .draw)) +\ngeom_density() +\nlabs(title = 'SD = 1')\n\nThis doesn’t look like totally awful but it seems like we have a.\n\nrunning_priors2 = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(exponential(0.5), class = sd))\n\n\nfirst_cut = brm(\n    net ~ (1| runner),\n    data = running,\n    prior = running_priors2,\n    sample_prior = 'only'\n)\n\n\nplot_dat = running |&gt;\nadd_predicted_draws(first_cut) \n\n\np2 = ggplot(plot_dat,aes(x = .prediction, group = .draw)) +\ngeom_density() +\nlabs(title = 'SD = 0.5')\n\nThat is nice we are getting slightly less impossible draws with more and more looking like our real sample. We can definitely going to tune this a bit more since we seem to have lots of deviation in the spikes. We would imagine that members of the same group are far less variable than this. Lets take a look at the differences between the real and predicitions from our prior.\n\nplot_dat |&gt;\n    ungroup() |&gt;\n    reframe(actual_avg = mean(net),\n              predicted_avg = mean(.prediction), \n              actual_range = range(net),\n              predicted_range = range(net))\n\nThe ranges look pretty good! There is a pretty big difference in the actual and predicted average so maybe a little bit more tuning of the standard deviation will do us some good.\n\nrunning_priors3 = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(exponential(0.1), class = sd))\n\n\nfirst_cut = brm(\n    net ~ (1| runner),\n    data = running,\n    prior = running_priors3,\n    sample_prior = 'only'\n)\n\nplot_dat = running |&gt;\nadd_predicted_draws(first_cut) \n\n\np3 = ggplot(plot_dat,aes(x = .prediction, group = .draw)) +\ngeom_density() +\nlabs(title = 'SD = 0.1')\n\nNow lets look at the changes summary statistics.\n\nplot_dat |&gt;\n    ungroup() |&gt;\n    reframe(actual_avg = mean(net),\n              predicted_avg = mean(.prediction), \n              actual_range = range(net),\n              predicted_range = range(net))\n\nLets look at the plots next to each other.\n\np1 / p2/ p3\n\n\n\n\n\n\n\n\nIt is somewhat hard to tell what is the best prior would be. The argument that they make in the book is that that we wouldn’t expect a ton of variation between and within runners in the same age group. Lets simultate out the prior\n\nsd_1 = ggplot() +\n    stat_function(fun = ~dexp(.,1/10), geom = 'area') +\n    xlim(c(0, 60)) \nsd_5 = ggplot() +\n    stat_function(fun = ~dexp(., 5/10), geom = 'area') +\n    xlim(c(0, 60)) \n\nsd_un = ggplot() +\n    stat_function(fun = ~dexp(., 1), geom = 'area') +\n    xlim(c(0, 60)) \n\nsd_1 | sd_5 | sd_un\n\n\n\n\n\n\n\n\nThe exponetial of 1 and 0.5 seems to think that there is a lot smaller variation I think the more informative prior seems to letting the sigmas vary a bit more. Which makes sense if we have a just signup kind of race. An exponential of 1 or 0.5 might make more sense if we were looking at the times for professional racers where the margins are a lot slimmer. So lets go with the 0.1. Next we are going to think about the relationship between age and time. Or slightly rephrased, as you get older how many minutes do you lose off your mile? That is a somewhat hard question to answer. Lets start with a slightly agressive prior. The reason I say agressive is that we are making the argument that as age increases we lose about two and half minutes on average give or take a minute.\n\npriors_runners = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(normal(2.5, 1), class = b),\n    prior(exponential(0.1), class = sigma),\n    prior(exponential(0.1), class = sd)\n)\n\nrunner_model = brm(\n    net ~ age_c + (1|runner), \n    data = running, \n    prior = priors_runners,\n    sample_prior = 'only'\n)\n\nrunning |&gt;\n    add_linpred_draws(runner_model, ndraws = 8)  |&gt;\n    ggplot(aes(x = age, y = net)) +\n    geom_line(aes(y = .linpred, group = paste(runner, .draw))) +\n    facet_wrap(vars(.draw))\n\n\n\n\n\n\n\n\nThese look pretty good! Now its time to actually incorporate the data\n\nrunner_model_final = brm(\n    net ~ age_c + (1|runner), \n    data = running, \n    prior = priors_runners,\n    iter = 4000, cores = 10, threads = 5\n)\n\nrunner_predictions = \n     predictions(runner_model_final, re_formula = NA, ndraws = 200, type = 'prediction') |&gt;\n     get_draws() |&gt;\n     mutate(age = (age_c + mean(running$age)))\n\n\n me = ggplot(runner_predictions, aes(x = age, y = draw)) +\n     stat_lineribbon(alpha = 0.5) +\n     labs(x = 'Age', y = 'Race Time(Minutes)', title = 'Made with marginaleffects') +\n     theme(legend.position = 'none') + \n     scale_y_continuous(breaks = seq(80, 100, by = 5))\n\nlpd = runner_model_final |&gt;\n    linpred_draws(running, ndraws = 200, re_formula = NA) |&gt;\n    ggplot(aes(x = age, y = net)) +\n    stat_lineribbon(aes(y = .linpred)) +\n    labs(x = 'Age', y = 'Race time', title = 'made with linepredraws') +\n    theme(legend.position = 'none')\n\nThese are pretty close to the same!\n\nme + lpd \n\n\n\n\n\n\n\n\nBasically all this is telling us as that effectively as age increases so does race time for the typical runner. The next step is to look at the runner specific analysis or the ‘fixed effect’ in MLM.\n\nrunner_effects = runner_model_final |&gt;\n    spread_draws(b_Intercept, r_runner[runner,]) |&gt;\n    mutate(runner_intercepts = b_Intercept + r_runner) |&gt;\n    ungroup() |&gt;\n    mutate(runner = fct_reorder(factor(runner), runner_intercepts, .fun = mean))\n\nglobal_effect = runner_model_final |&gt;\n    tidy(effects = c('fixed'), conf.level = 0.89) |&gt;\n    filter(term == '(Intercept)')\n\nggplot(runner_effects, aes(x = runner_intercepts, y = runner)) +\n    stat_pointinterval() +\n    annotate(geom = 'rect', ymin = -Inf, ymax = Inf,\n              xmin = global_effect$conf.low, xmax = global_effect$conf.high, alpha = 0.5) +\n    geom_vline(xintercept = global_effect$estimate)\n\n\n\n\n\n\n\n\n\n\n\nThis is all well and dandy but this is what it looks like when we just fit an OLS model per runner\n\nrunning |&gt; \n  ggplot(aes(x = age, y = net, group = runner)) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5, color = clrs[1]) +\n  coord_cartesian(ylim = c(60, 125)) +\n  labs(x = \"Age\", y = \"Race time\",\n       title = \"Observed data\",\n       subtitle = \"Basic per-runner OLS models\")\n\n\n\n\n\n\n\n\nThis makes sense because time effects us all differently if we think about the effect of age on football positions we get a different relationship to production. A QB can be uber productive in their 30 and above seasons versus a wide receiver or corner. To account for these relationships we need to model a specific intercept and a particular slope for each runner.\nWe need a more robust language for these kinds of models because they are uber complicated because now both the intercept and the slope per each runner are going to move together. We could theoretically write them as distributions but the tinkering is going to be a bear. Instead we can use multi-dimensional priors to describe the correlations between the features. There are effectively three things we need to think about\n\nthe feature means: the mean of the intercepts, the global intercept etc\nthe correlation matrix: what is the expected correlation\nHow much to they vary?\n\nThe tricky part is how do we set a prior for a correlation matrix? We can use an LKJ prior. This prior is a effectively a shape. Bigger values are more skeptical of large correlation coefficients.\n\nwithr::with_seed(123, {\n  rho_plots &lt;- tibble(rho = c(-0.99, 0, 0.99)) |&gt; \n    mutate(title = glue::glue(\"ρ = {rho}\"),\n           subtitle = c(\"Strong negative correlation\\nbetween slope and intercept\",\n                        \"No correlation\\nbetween slope and intercept\",\n                        \"Strong positive correlation\\nbetween slope and intercept\")) |&gt; \n    mutate(Sigma = purrr::map(rho, ~matrix(c(1, .x, .x, 1), 2, 2))) |&gt; \n    mutate(data = purrr::map(Sigma, ~{\n      MASS::mvrnorm(n = 100, mu = c(2, 3), Sigma = .x) |&gt; \n        as_tibble() |&gt; \n        rename(b0 = V1, b1 = V2)\n    })) |&gt; \n    mutate(plot = pmap(list(data, title, subtitle), ~{\n      ggplot(..1) +\n        geom_abline(aes(intercept = b0, slope = b1, color = b0),\n                    size = 0.3) +\n        scale_color_viridis_c(option = \"rocket\", begin = 0.1, end = 0.85,\n                              limits = c(-1, 5)) +\n        labs(title = ..2, subtitle = ..3, color = \"β&lt;sub&gt;0&lt;/sub&gt;\") +\n        lims(x = c(0, 3), y = c(0, 10)) +\n        theme(axis.line = element_line(),\n              legend.title = element_markdown())\n    }))\n})\n\nwrap_plots(rho_plots$plot) + \n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nIn this case when the correclation is large and negative big intercepts have smaller slopes this would mean that individual runners who have longer baseline race times would see less of an age effect. When p is bigger and positive bigger intercepts have steeper slopes meaning runners with longer baseline race times would have more of an age effect. Whats nice about the LKJ prior in brms is that if we don’t want to touch the means and variance parts we don’t have to we can just set the regularization parts. The breakdown looks like this\n\n\\(\\eta\\) &lt; 1 extreme correlations are more likely\n\\(\\eta\\) = 1, all correlations are equally likely\n\\(\\eta\\) &gt; 1 central correlations are more likely, and the larger the \\(\\eta\\) the narrower the distribution is around 0\n\nHere we will start with a fairly bland prior.\n\nvarying_slope_priors_1 = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(2.5, 1), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(exponential(0.1), class = sd),\n            prior(lkj(1), class = cor))\n\n\nvar_mod1 = brm(\n    net ~ age_c + (1 + age_c | runner),\n    data = running,\n    prior = varying_slope_priors_1,\n    iter = 4000, threads = threading(5), cores = 10, \n    control = list(adapt_delta = 0.9),\n    sample_prior = 'only' \n)\n\nLets look at the sims\n\nsim1 = running |&gt;\n    add_predicted_draws(var_mod1) \n\n\nggplot(sim1, aes(x = .prediction, group = .draw)) +\n    geom_density() +\n    coord_cartesian(xlim = c(0, 300))\n\n\n\n\n\n\n\n\nThis seems broadely reasonable! What if we bump up the regularization a bit?\n\nvarying_slope_priors_2 = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(2.5, 1), class = b),\n            prior(exponential(0.1), class = sigma),\n            prior(exponential(0.1), class = sd),\n            prior(lkj(1.5), class = cor))\n\n\nvar_mod2 = brm(\n    net ~ age_c + (1 + age_c | runner),\n    data = running,\n    prior = varying_slope_priors_2,\n    iter = 4000, threads = threading(5), cores = 10, \n    control = list(adapt_delta = 0.9),\n    sample_prior = 'only' \n)\n\nNow lets look at the prediction from the model\n\nrunning |&gt;\n    add_predicted_draws(var_mod2) |&gt;\n    ggplot(aes(x = .prediction, group = .draw)) +\n    geom_density() +\n    coord_cartesian(xlim = c(0, 300))\n\n\n\n\n\n\n\n\nTo my eye these look pretty similar so lets fit both and then compare the three models. Now lets check the corss validation statistics\n\nvar_mod1 = update(\n    var_mod1,\n    sample_prior = 'no'\n)\n\nvar_mo2 = update(var_mod2,\n    sample_prior = 'no')\n\nNow we should go and look at how far off each model is\n\np1 = pp_check(runner_model_final, ndraws = 25) +\nlabs(title = 'Random Intercepts only')\n\np2 = pp_check(var_mod1, ndraws = 25) +\n    labs(title = 'Random Intercepts & Slopes', subtitle =  'ETA = 1')\n\np3 = pp_check(var_mo2, ndraws = 25) +\n    labs(title = 'Random Intercepts & Slopes', subtitle = 'ETA = 1.5')\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\nThese all look vaguely the same so now we should check how well they predict out of sample\n\nloo_stats = tribble(~model_name, ~model,\n                    'Random Intercepts', runner_model_final,\n                    'Random Intercepts & Slopes \\n ETA = 1', var_mod1,\n                    'Random Intercepts & Slopes \\n ETA = 1.5', var_mo2) |&gt;\n            mutate(loo = purrr::map(model, \\(x) loo(x))) |&gt;\n            mutate(loo_stuff = purrr::map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |&gt;\n            select(model_name, loo_stuff) |&gt; \n  unnest(loo_stuff) |&gt; \n  filter(statistic == \"elpd_loo\") |&gt; \n  arrange(desc(Estimate))\n\nloo_stats\n\nMy eyes do not deceive effectively we aren’t extracting more predicitive accuracy out of the model. Which is great we can use the random intercepts safely. Now lets play around with the cauchy prior on the random intercepts model and then see how this effects things.\n\npriors_runners = c(\n    prior(normal(100, 10), class = Intercept),\n    prior(normal(2.5, 1), class = b),\n    prior(cauchy(0,10), class = sigma),\n    prior(cauchy(0,10), class = sd)\n)\n\nrunner_model = brm(\n    net ~ age_c + (1|runner), \n    data = running, \n    prior = priors_runners,\n    sample_prior = 'only'\n)\n\ncheck_models_cauchy = running |&gt;\n    add_predicted_draws(runner_model) \n\nggplot(check_models_cauchy, aes(x = .prediction, group = .draw)) +\n    geom_density() +\n    coord_cartesian(xlim = c(0, 300))\n\n\n\n\n\n\n\n\nWow umm the predictions look uber wonky. Lets see what happens when we fit the model\nWhen we plot the posterior predictions we see that we have slightly heavier tails which is kind of the point of using the cauchy distribution.\n\npp_check(runner_model_cauchy, ndraws = 25)\n\n\n\n\n\n\n\n\nNow lets go and grab the posterior predictions.\n\ncauchy_preds = running |&gt;\nadd_linpred_draws(runner_model_cauchy, ndraws = 200, re_formula = NA) |&gt;\nggplot(aes(x = age, y = net)) +\n     stat_lineribbon(aes(y = .linpred)) +\n     labs(title = 'Runner Model with Cauchy Priors') \n\n\nlpd + cauchy_preds\n\n\n\n\n\n\n\n\nThese are vaugely the same in terms of posterior prdictions. The credible intervals for the cuachy priors seem to be a little bit different.\n\nloo_stats = tribble(~model_name, ~model,\n                    'Random Intercepts exponential prior', runner_model_final,\n                    'Random Intercepts Cauchy prior', runner_model_cauchy,\n                    'Random Intercepts & Slopes \\n ETA = 1', var_mod1,\n                    'Random Intercepts & Slopes \\n ETA = 1.5', var_mo2) |&gt;\n            mutate(loo = purrr::map(model, loo)) |&gt;\n            mutate(loo_stuff = purrr::map(loo, ~as_tibble(.$estimates, rownames = \"statistic\"))) |&gt;\n            select(model_name, loo_stuff) |&gt; \n  unnest(loo_stuff) |&gt; \n  filter(statistic == \"elpd_loo\") |&gt; \n  arrange(desc(Estimate))\n\nloo_stats |&gt;\n    tinytable::tt()\n\nThe cauchy prior does slightly bettter than the random intercepts model with a exponential prior but not so dramatic that I would be sweating about it.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#nested-structures",
    "href": "Bayesian-Stats.html#nested-structures",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "As you might imagine there are lots of structures that we can model. A particularly relevant setup is the mountain climbers example. We often have nested structure so in this case we have climber in expedition teams. For MMM’s we have people within channels and within that we have additional structures that we may know based off of demographic information or clusters that we can model. In this case we will start small and then we can build on these fundamentals because we also have expeditions within season and then age.\n\ndata('climbers_sub', package = 'bayesrules')\n\nclimbers = climbers_sub |&gt;\n    select(expedition_id, member_id, success, year, season,\n         age, expedition_role, oxygen_used) |&gt; \n  mutate(age_c = scale(age, scale = FALSE))\n\nextract_attributes &lt;- function(x) {\n  attributes(x) %&gt;%\n    set_names(janitor::make_clean_names(names(.))) %&gt;%\n    as_tibble() %&gt;%\n    slice(1)\n}\n\nunscaled_climbers &lt;- climbers %&gt;%\n  select(ends_with(\"_c\")) |&gt; \n  summarize(across(everything(), ~extract_attributes(.))) |&gt; \n  pivot_longer(everything()) |&gt; \n  unnest(value) |&gt; \n  split(~name)\n\nLets get some basic summary statistics\n\nclimber_expedition = climbers |&gt;\n    group_by(expedition_id) |&gt;\n    summarise(total = n(),\n              prop_success = mean(success))\n\nsuccess_by_size = ggplot(climber_expedition, aes(x = total, y = prop_success)) +\n    geom_count() +\n    labs(x = 'Expedition Team Size', y = 'Proportion of Expedition Team That Finished')\n\ndist_of_team_finished = ggplot(climber_expedition, aes(x = prop_success)) +\n    geom_histogram() +\n    labs(x = 'Proportion of Expedition Team That Finished')\n\nsuccess_by_size + dist_of_team_finished\n\n\n\n\n\n\n\n\nNow lets look at the success by size by season\n\nclimbers |&gt;\n    mutate(total = n(),\n              prop_success = mean(success), .by = expedition_id) |&gt;\n          ggplot( aes(x = total, y = prop_success)) +\n    geom_count() +\n    labs(x = 'Expedition Team Size', y = 'Proportion of Expedition Team That Finished') +\n    facet_wrap(vars(season))      \n\n\n\n\n\n\n\n\nThis is an interesting partial pooling thing that we will look at bit later. Most of the expeditions happen during Spring and Autumn with only a few happening in winter with close to no success and one during summer with close to 100 percent success. So seasons are definitely something that we would want to model.\nIf we look at the model in the book we expect that whether or not you use oxygen is importanat\n\nclimbers |&gt;\n    group_by(age, oxygen_used) |&gt;\n    summarise(success_rate = mean(success)) |&gt;\n    ggplot(aes(x = age, y = success_rate, color = oxygen_used)) +\n    geom_point()\n\n\n\n\n\n\n\n\nAs we can see there age is a pretty poor predictor of a success rate compared to oxygen. So now lets set some priors.\n\nclimbing_priors = c(\n    prior(normal(0, 2.5), class = Intercept),\n    prior(normal(0, 2.5), class = b),\n    prior(exponential(1), class = sd)\n)\n\nclimbing_prior_mod = brm(\n    success ~ age_c + oxygen_used + (1 | expedition_id),\n    data = climbers, \n    family = bernoulli(link = 'logit'),\n    prior = climbing_priors,\n    sample_prior = 'only'\n)\n\ncheck_prds = climbers |&gt;\n    add_predicted_draws(climbing_prior_mod, ndraws = 1000)\n\nggplot(check_prds, aes(x = age, y = .prediction, color = oxygen_used)) +\n    geom_dots(aes(side = ifelse(.prediction == '1', 'bottom', 'top')), pch = 19, scale = 0.2)\n\n\n\n\n\n\n\npp_check(climbing_prior_mod)\n\n\n\n\n\n\n\n\nHmmm it seems that we could probably do a bit better. If we check the coefficients the implication is that if we use oxygen then it will decrease our likelihood of survival which intuitively we know is crazy! Once again working with",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#ordered-monotonic-predictors",
    "href": "Bayesian-Stats.html#ordered-monotonic-predictors",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Lets say we have some ordered predictor like education. We would expect that the difference between elementary and college is not linear. The effect would look something like this\n$$\n\\[\\begin{aligned}\n1(Elementary) \\phi_i = 0 \\\\\n\n2(\\text{Middle School}) \\phi_i = \\delta_1 \\\\\n\n3(\\text{High School})  \\phi_i = \\delta_1 + \\delta_2 \\\\\n\n4 (\\text{Some College}) \\phi_i = \\delta_1 + \\delta_3\n\\end{aligned}\\]\n$$\nAnd so on and so forth. Effectively we are adding the effect current level of education completed plus the effect of the previous level. In our example Elementary school does not really count because it is the first level so the effect of completing elementary school plus the effect of completing middle school is just the effect of middle school. For ordered categories we are sampling from a Dirichlet distribution. Funnily enough these can be thought of as weird beta distributions.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "Bayesian-Stats.html#gaussian-processes",
    "href": "Bayesian-Stats.html#gaussian-processes",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "These seem to be uber popular with Bayesians who do anything related to time and space. The idea kind of boils down to Tobler’s first law of geography “things that are close to each other are more similar than things that are far away from each other.”\nA Gaussian process is “an inifinte dimensional generalization of multivariate distributions.” In English this says that we use a kernel function that generalizes to infinite dimensions/observations/predictions instead of a conventional variance-covariance matrix. If you think of the varying slopes and intercepts models that we have worked with before there isn’t any ordering to the categories. I have a hierarcical ranking of seasons but that is not a constraint that I would ever impose on a model. If we wanted to include something like distance or time the variance-covariance matrix would be huge because well it is a continous variable. Instead of the standard matrix we just sub in a kernel to take the place of this big unruly matrix. A kernel function willl generalize to infinite dimensions. The kernel gives the covariance between any pair of points as function of their distance. As always we set a prior which is effectively the rate of decay between the tips. Like 0 distance all the way to the max of the distance and the maximum covariance. These are kind of just regularization priors in some respect.",
    "crumbs": [
      "Bayesian Statistics"
    ]
  },
  {
    "objectID": "unsupervised-learning.html",
    "href": "unsupervised-learning.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In the prior “chapters” we covered a variety of supervised learning techniques. By supervised we have some direct control over hyperparameter tuning and how we enter things into the model to predict a response. Unsupervised learning doesn’t neccessarily have a dependent variable that we are trying to predict. Instead the unsupervised techniques covered in ISLR and ESLR look at unsupervised learning as an approach to help us uncover hidden groups in our data. They focus on PCA and various clustering methods. There are no hard and fast rules with this and we often think of it as an EDA excercise or a way to reduce the dimensionality of our data. In effect supervised learning is learning the relationship of data without labels. So say we need to build a classifier. We label the data as penguin not a penguin without then we train a model on these things. For an unsupervised model we may not care so much but may want to throw everything in a model and then see if the model can uncover groupings of penguin not penguin.\n\n\nThe curse of dimensionality frequently comes up in machine learning because we often have a ton of predictors. The curse of dimensionality can be a little weird if you don’t deal with math a whole lot. The general idea of the curse of dimensionality is that as the number of dimensions increase our models are going to start to break down because as the number of dimensions increase our model is going to find data points standing all alone. We may not know that because our model starts to do well finding things that are alone in a neighborhood. A good model will be able to find these points hiding in a crowd a bad model wouldn’t! Both good and bad models will find points by themselves which is not neccessarily helpful as we start to introduce new data points the model’s performance will start to detoriate.\nThe thing with the curse of dimensionality is that we never escape it because we are going to run into when modeling variables. For OLS the curse of dimensionality enters as we start to add more and more variables because we are stratiying our data by more and more information. Even when we are doing something like inverse propensity score weighting and then using these weights in a bivariate OLS you are running into the curse in the propensity score equation.\n\n\n\nWe can use methods like PCA to reduce the number of dimensions. The general idea is that we are trying to map high dimensional relationships into lower dimesional proxies.\n\nimport polars as pl\nimport polars.selectors as cs \nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\npengs = pl.from_pandas(load_penguins())\n\npengs.select(pl.exclude('year', 'species', 'sex', 'island')).drop_nans().corr()\n\n\nshape: (4, 4)\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\nf64\nf64\nf64\nf64\n\n\n\n\n1.0\n-0.235053\n0.656181\n0.59511\n\n\n-0.235053\n1.0\n-0.583851\n-0.471916\n\n\n0.656181\n-0.583851\n1.0\n0.871202\n\n\n0.59511\n-0.471916\n0.871202\n1.0\n\n\n\n\n\n\nEach of these variables map on to some measure of penguin bigness some of them are more correlated with each other but if we had flipper width or bill width or some other measure of penguins bigness there is going to significant overlap. Once we enter these into a regression these models are going to pretty good job at descrbing penguin weirdness. A way to help us is to summarize these variables into what we call principal components.\nPrincipal components are a way to summarize the variation in these overlapping variables in a minimal set of variables.\n\nfrom sklearn.decomposition import PCA\n\nnormalized = pengs.select(cs.numeric(), pl.col('species')).select(pl.exclude('year')).with_columns(\n        (cs.numeric() - cs.numeric().mean() / cs.numeric().std())\n    ).drop_nulls()\n\npca = PCA()\nnumeric_data = normalized.select(cs.numeric())\npca.fit(numeric_data)\n\n# Transform the data and include species labels\ntr_df = pca.transform(numeric_data)\npca_components = pl.DataFrame(\n    tr_df,\n    schema=[f'PC{i+1}' for i in range(tr_df.shape[1])]\n).with_columns(normalized['species']) \n\npca_df = pca_components.to_pandas()\n\n# Define a color map for the species\ncolor_map = {\n    \"Adelie\": \"red\",\n    \"Gentoo\": \"blue\",\n    \"Chinstrap\": \"green\"\n}\n\n# Create the scatter plot\nplt.figure(figsize=(8, 6))\nfor species, color in color_map.items():\n    subset = pca_df[pca_df['species'] == species]\n    plt.scatter(\n        subset['PC1'], \n        subset['PC2'], \n        label=species, \n        color=color, \n        alpha=0.7\n    )\n\n# Add plot details\nplt.title('PCA: First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title='Species')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that these two variables do a reasonable enough job of distinguishing the species from each other. What is going on underneath the hood is that we are creating some linear combinations of these variables. We can relax this constraint using gams if we want but in practice we do this linearly alot.\n\n\n\nIn a sense we are finding subgroups with this PCA example but we can also use clustering algorithms. Basically clustering algorithms will try to ifer subgroups from the data either through minimizing a distance measure if we are using k-means. We could also use mixture clustering where we make some assumptions of the distributions but can let more moment conditions enter into the clustering algorithm.",
    "crumbs": [
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "unsupervised-learning.html#the-curse",
    "href": "unsupervised-learning.html#the-curse",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The curse of dimensionality frequently comes up in machine learning because we often have a ton of predictors. The curse of dimensionality can be a little weird if you don’t deal with math a whole lot. The general idea of the curse of dimensionality is that as the number of dimensions increase our models are going to start to break down because as the number of dimensions increase our model is going to find data points standing all alone. We may not know that because our model starts to do well finding things that are alone in a neighborhood. A good model will be able to find these points hiding in a crowd a bad model wouldn’t! Both good and bad models will find points by themselves which is not neccessarily helpful as we start to introduce new data points the model’s performance will start to detoriate.\nThe thing with the curse of dimensionality is that we never escape it because we are going to run into when modeling variables. For OLS the curse of dimensionality enters as we start to add more and more variables because we are stratiying our data by more and more information. Even when we are doing something like inverse propensity score weighting and then using these weights in a bivariate OLS you are running into the curse in the propensity score equation.",
    "crumbs": [
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "unsupervised-learning.html#what-can-we-do-about-the-curse",
    "href": "unsupervised-learning.html#what-can-we-do-about-the-curse",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "We can use methods like PCA to reduce the number of dimensions. The general idea is that we are trying to map high dimensional relationships into lower dimesional proxies.\n\nimport polars as pl\nimport polars.selectors as cs \nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\npengs = pl.from_pandas(load_penguins())\n\npengs.select(pl.exclude('year', 'species', 'sex', 'island')).drop_nans().corr()\n\n\nshape: (4, 4)\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\nf64\nf64\nf64\nf64\n\n\n\n\n1.0\n-0.235053\n0.656181\n0.59511\n\n\n-0.235053\n1.0\n-0.583851\n-0.471916\n\n\n0.656181\n-0.583851\n1.0\n0.871202\n\n\n0.59511\n-0.471916\n0.871202\n1.0\n\n\n\n\n\n\nEach of these variables map on to some measure of penguin bigness some of them are more correlated with each other but if we had flipper width or bill width or some other measure of penguins bigness there is going to significant overlap. Once we enter these into a regression these models are going to pretty good job at descrbing penguin weirdness. A way to help us is to summarize these variables into what we call principal components.\nPrincipal components are a way to summarize the variation in these overlapping variables in a minimal set of variables.\n\nfrom sklearn.decomposition import PCA\n\nnormalized = pengs.select(cs.numeric(), pl.col('species')).select(pl.exclude('year')).with_columns(\n        (cs.numeric() - cs.numeric().mean() / cs.numeric().std())\n    ).drop_nulls()\n\npca = PCA()\nnumeric_data = normalized.select(cs.numeric())\npca.fit(numeric_data)\n\n# Transform the data and include species labels\ntr_df = pca.transform(numeric_data)\npca_components = pl.DataFrame(\n    tr_df,\n    schema=[f'PC{i+1}' for i in range(tr_df.shape[1])]\n).with_columns(normalized['species']) \n\npca_df = pca_components.to_pandas()\n\n# Define a color map for the species\ncolor_map = {\n    \"Adelie\": \"red\",\n    \"Gentoo\": \"blue\",\n    \"Chinstrap\": \"green\"\n}\n\n# Create the scatter plot\nplt.figure(figsize=(8, 6))\nfor species, color in color_map.items():\n    subset = pca_df[pca_df['species'] == species]\n    plt.scatter(\n        subset['PC1'], \n        subset['PC2'], \n        label=species, \n        color=color, \n        alpha=0.7\n    )\n\n# Add plot details\nplt.title('PCA: First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title='Species')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that these two variables do a reasonable enough job of distinguishing the species from each other. What is going on underneath the hood is that we are creating some linear combinations of these variables. We can relax this constraint using gams if we want but in practice we do this linearly alot.",
    "crumbs": [
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "unsupervised-learning.html#clustering",
    "href": "unsupervised-learning.html#clustering",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In a sense we are finding subgroups with this PCA example but we can also use clustering algorithms. Basically clustering algorithms will try to ifer subgroups from the data either through minimizing a distance measure if we are using k-means. We could also use mixture clustering where we make some assumptions of the distributions but can let more moment conditions enter into the clustering algorithm.",
    "crumbs": [
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "Since the bulk of your work will be building machnine learning models it is probably going to be important that you get way more comfortable with machine learning in python. You are a bit of a unicorn in the sense that you will do anything but learn pandas. So you are going to have to make sure that this isn’t new information.\n\n\nCode\nimport polars as pl \nimport numpy as np\nimport pandas as pd \nimport statsmodels.formula.api as smf \nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV, train_test_split, ShuffleSplit, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import mean_squared_error, make_scorer\nimport polars.selectors as cs\nfrom marginaleffects import *\nfrom plotnine import *\nfrom great_tables import GT \n\n\nboston = pl.read_csv('data/Boston.csv').to_pandas()\n\n\nSince R is kind of your native language the way Python does things is weird to you so a simple linear model like this\n\nboston = read.csv('data/Boston.csv')\n\nmod = lm(medv ~ lstat, data = boston)\n\nbecomes this (monster) in python where you now also have to tell it that you need the constant. Which is frankly crazy.\n\nform_model = smf.ols('medv ~ lstat', data = boston).fit()\n\nx = boston['lstat']\n\nx = sm.add_constant(x)\n\nsm_model = sm.OLS(boston['medv'], x).fit()\n\nform_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n16:23:15\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n34.5538\n0.563\n61.415\n0.000\n33.448\n35.659\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.000\n-1.026\n-0.874\n\n\n\n\n\n\n\n\nOmnibus:\n137.043\nDurbin-Watson:\n0.892\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n291.373\n\n\nSkew:\n1.453\nProb(JB):\n5.36e-64\n\n\nKurtosis:\n5.319\nCond. No.\n29.7\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n# sm_model.summary()\n\nLike an intersting cultural difference between these two is how we do things after estimation. For R apply functions to it since R is a more functionally oriented language. However if we access the object we created we have a whole host of class methods for this task. So if we wanted to predict what would happen at specified values we would do something to the effect of\n\n\nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})\n\nx_new = sm.add_constant(new_df)\n\npreds = sm_model.get_prediction(x_new)\n\n## this prints a huge array\npreds_mean = preds.predicted_mean\n\nMultiple regression works somewhat similarly. Unfortunately it takes this hideous form\n\ny = boston['medv']\n\nx = boston[['crim', 'age']]\n\nx = sm.add_constant(x)\n\nsm.OLS(y, x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.217\n\n\nModel:\nOLS\nAdj. R-squared:\n0.213\n\n\nMethod:\nLeast Squares\nF-statistic:\n69.52\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n2.20e-27\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-1778.5\n\n\nNo. Observations:\n506\nAIC:\n3563.\n\n\nDf Residuals:\n503\nBIC:\n3576.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n29.8007\n0.971\n30.698\n0.000\n27.893\n31.708\n\n\ncrim\n-0.3118\n0.045\n-6.914\n0.000\n-0.400\n-0.223\n\n\nage\n-0.0896\n0.014\n-6.499\n0.000\n-0.117\n-0.062\n\n\n\n\n\n\n\n\nOmnibus:\n189.020\nDurbin-Watson:\n0.710\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n553.472\n\n\nSkew:\n1.831\nProb(JB):\n6.53e-121\n\n\nKurtosis:\n6.583\nCond. No.\n199.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n# smf.ols('medv ~ crim + age', data = boston).fit().summary()\n\n\nsummary(lm(medv ~ crim + age, data = boston))\n\n\nCall:\nlm(formula = medv ~ crim + age, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.940  -4.991  -2.420   2.110  32.033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.80067    0.97078  30.698  &lt; 2e-16 ***\ncrim        -0.31182    0.04510  -6.914 1.43e-11 ***\nage         -0.08955    0.01378  -6.499 1.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.157 on 503 degrees of freedom\nMultiple R-squared:  0.2166,    Adjusted R-squared:  0.2134 \nF-statistic: 69.52 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nWhat starts to get interesting is that what if we need we want to fit everything in one go? In R this is pretty simple\n\nsummary(lm(medv ~ ., data = boston))\n\n\nCall:\nlm(formula = medv ~ ., data = boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4167  -2.8190  -0.5834   2.0250  26.1489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.642977   4.934432   8.439 3.59e-16 ***\nX            -0.002426   0.002103  -1.154 0.249130    \ncrim         -0.122172   0.032996  -3.703 0.000238 ***\nzn            0.048513   0.013939   3.480 0.000545 ***\nindus         0.012833   0.062126   0.207 0.836433    \nchas          2.858484   0.869863   3.286 0.001088 ** \nnox         -18.546508   3.854423  -4.812 1.99e-06 ***\nrm            3.685614   0.420780   8.759  &lt; 2e-16 ***\nage           0.001098   0.013502   0.081 0.935242    \ndis          -1.507860   0.202099  -7.461 3.92e-13 ***\nrad           0.307457   0.068691   4.476 9.46e-06 ***\ntax          -0.011976   0.003849  -3.112 0.001969 ** \nptratio      -0.932889   0.132223  -7.055 5.88e-12 ***\nlstat        -0.553515   0.050658 -10.926  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.796 on 492 degrees of freedom\nMultiple R-squared:  0.735, Adjusted R-squared:  0.728 \nF-statistic:   105 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nWhereas in python you need to do something like this\n\nsmf.ols('medv ~' + '+'.join(boston.columns.difference(['medv'])), data = boston).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.734\n\n\nModel:\nOLS\nAdj. R-squared:\n0.728\n\n\nMethod:\nLeast Squares\nF-statistic:\n113.5\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n2.23e-133\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-1504.9\n\n\nNo. Observations:\n506\nAIC:\n3036.\n\n\nDf Residuals:\n493\nBIC:\n3091.\n\n\nDf Model:\n12\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n41.6173\n4.936\n8.431\n0.000\n31.919\n51.316\n\n\nage\n0.0036\n0.013\n0.271\n0.787\n-0.023\n0.030\n\n\nchas\n2.8400\n0.870\n3.264\n0.001\n1.131\n4.549\n\n\ncrim\n-0.1214\n0.033\n-3.678\n0.000\n-0.186\n-0.057\n\n\ndis\n-1.4908\n0.202\n-7.394\n0.000\n-1.887\n-1.095\n\n\nindus\n0.0135\n0.062\n0.217\n0.829\n-0.109\n0.136\n\n\nlstat\n-0.5520\n0.051\n-10.897\n0.000\n-0.652\n-0.452\n\n\nnox\n-18.7580\n3.851\n-4.870\n0.000\n-26.325\n-11.191\n\n\nptratio\n-0.9375\n0.132\n-7.091\n0.000\n-1.197\n-0.678\n\n\nrad\n0.2894\n0.067\n4.325\n0.000\n0.158\n0.421\n\n\nrm\n3.6581\n0.420\n8.705\n0.000\n2.832\n4.484\n\n\ntax\n-0.0127\n0.004\n-3.337\n0.001\n-0.020\n-0.005\n\n\nzn\n0.0470\n0.014\n3.384\n0.001\n0.020\n0.074\n\n\n\n\n\n\n\n\nOmnibus:\n171.096\nDurbin-Watson:\n1.077\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n709.937\n\n\nSkew:\n1.477\nProb(JB):\n6.90e-155\n\n\nKurtosis:\n7.995\nCond. No.\n1.17e+04\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.17e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nor\n\nx = boston.drop(columns=['medv'])\n\nx = sm.add_constant(x)\n\ny = boston['medv']\n\n\nsm.OLS(y,x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.735\n\n\nModel:\nOLS\nAdj. R-squared:\n0.728\n\n\nMethod:\nLeast Squares\nF-statistic:\n105.0\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n1.26e-132\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-1504.2\n\n\nNo. Observations:\n506\nAIC:\n3036.\n\n\nDf Residuals:\n492\nBIC:\n3096.\n\n\nDf Model:\n13\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n41.6430\n4.934\n8.439\n0.000\n31.948\n51.338\n\n\n\n-0.0024\n0.002\n-1.154\n0.249\n-0.007\n0.002\n\n\ncrim\n-0.1222\n0.033\n-3.703\n0.000\n-0.187\n-0.057\n\n\nzn\n0.0485\n0.014\n3.480\n0.001\n0.021\n0.076\n\n\nindus\n0.0128\n0.062\n0.207\n0.836\n-0.109\n0.135\n\n\nchas\n2.8585\n0.870\n3.286\n0.001\n1.149\n4.568\n\n\nnox\n-18.5465\n3.854\n-4.812\n0.000\n-26.120\n-10.973\n\n\nrm\n3.6856\n0.421\n8.759\n0.000\n2.859\n4.512\n\n\nage\n0.0011\n0.014\n0.081\n0.935\n-0.025\n0.028\n\n\ndis\n-1.5079\n0.202\n-7.461\n0.000\n-1.905\n-1.111\n\n\nrad\n0.3075\n0.069\n4.476\n0.000\n0.172\n0.442\n\n\ntax\n-0.0120\n0.004\n-3.112\n0.002\n-0.020\n-0.004\n\n\nptratio\n-0.9329\n0.132\n-7.055\n0.000\n-1.193\n-0.673\n\n\nlstat\n-0.5535\n0.051\n-10.926\n0.000\n-0.653\n-0.454\n\n\n\n\n\n\n\n\nOmnibus:\n168.602\nDurbin-Watson:\n1.082\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n688.210\n\n\nSkew:\n1.459\nProb(JB):\n3.61e-150\n\n\nKurtosis:\n7.912\nCond. No.\n1.38e+04\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.38e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\nWe can start to do things like add transformations like this\n\nx = boston.drop(columns = 'medv')\n\ny = boston['medv']\n\nx['sqr_lstat'] = x['lstat'] **2\n\ny['medv'] = np.log(y['medv'])\n\nx = sm.add_constant(x) \n\nsm.OLS(y,x).fit().summary()\n\nFor interactions we do something like this\n\nx = boston[['age', 'lstat']]\n\nx['lstat:age'] = x['age'] * x['lstat']\n\nx = sm.add_constant(x) \n\ny = boston['medv']\n\n\nsm.OLS(y, x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.556\n\n\nModel:\nOLS\nAdj. R-squared:\n0.553\n\n\nMethod:\nLeast Squares\nF-statistic:\n209.3\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n4.86e-88\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-1635.0\n\n\nNo. Observations:\n506\nAIC:\n3278.\n\n\nDf Residuals:\n502\nBIC:\n3295.\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n36.0885\n1.470\n24.553\n0.000\n33.201\n38.976\n\n\nage\n-0.0007\n0.020\n-0.036\n0.971\n-0.040\n0.038\n\n\nlstat\n-1.3921\n0.167\n-8.313\n0.000\n-1.721\n-1.063\n\n\nlstat:age\n0.0042\n0.002\n2.244\n0.025\n0.001\n0.008\n\n\n\n\n\n\n\n\nOmnibus:\n135.601\nDurbin-Watson:\n0.965\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n296.955\n\n\nSkew:\n1.417\nProb(JB):\n3.29e-65\n\n\nKurtosis:\n5.461\nCond. No.\n6.88e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.88e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nFor qualitative variables we need to switch to a new dataset. There is a lot of interesting information in multicategory variables. One thing that we have to keep in mind when using qualitative variables normally is that we have a reference category. This may not always be straightforward to infer and we are throwing information away that is interesting. One hot encoding or breaking out the qualitative variable to indicatior variables is a nice way to do this,\n\ncarseats = pl.read_csv('data/Carseats.csv')\n\ncarseats.select(pl.col('ShelveLoc').unique()).head()\n\n\nshape: (3, 1)\n\n\n\nShelveLoc\n\n\nstr\n\n\n\n\n\"Bad\"\n\n\n\"Medium\"\n\n\n\"Good\"\n\n\n\n\n\ncars_small = carseats.select(pl.col('Sales', 'CompPrice', 'Income', 'ShelveLoc')).to_dummies(cs.string())\n\nx = cars_small.select(pl.exclude('Sales')).to_pandas()\n\ny = cars_small.to_pandas()['Sales']\n\nx = sm.add_constant(x)\n\nsm.OLS(y, x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nSales\nR-squared:\n0.352\n\n\nModel:\nOLS\nAdj. R-squared:\n0.345\n\n\nMethod:\nLeast Squares\nF-statistic:\n53.63\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n4.38e-36\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-895.60\n\n\nNo. Observations:\n400\nAIC:\n1801.\n\n\nDf Residuals:\n395\nBIC:\n1821.\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n3.8101\n0.755\n5.046\n0.000\n2.326\n5.295\n\n\nCompPrice\n0.0106\n0.007\n1.420\n0.156\n-0.004\n0.025\n\n\nIncome\n0.0184\n0.004\n4.473\n0.000\n0.010\n0.026\n\n\nShelveLoc_Bad\n-0.9343\n0.312\n-2.994\n0.003\n-1.548\n-0.321\n\n\nShelveLoc_Good\n3.8167\n0.322\n11.843\n0.000\n3.183\n4.450\n\n\nShelveLoc_Medium\n0.9277\n0.287\n3.235\n0.001\n0.364\n1.492\n\n\n\n\n\n\n\n\nOmnibus:\n0.001\nDurbin-Watson:\n1.941\n\n\nProb(Omnibus):\n1.000\nJarque-Bera (JB):\n0.030\n\n\nSkew:\n-0.003\nProb(JB):\n0.985\n\n\nKurtosis:\n2.958\nCond. No.\n7.74e+17\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 1.38e-29. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nFor quick and dirty things this is nice and fairly straightforward. We are not really going to delve to deep on fitting a ton of models but really this would just involve some F-string. Instead we are going to focus on the machine learning workflow. Going through step by step and doing these are not terribily time consuming but as things get more complicated we are going to need a more robust framework to deal with this.\n\n\n\nOur basic assumptions of linear regression are that\n\nThere are linear relationship between our outcome and our predictors\n\n\nThis is something we violate all the time. For the most part we can transform the dependent or independent variable to dependent variable\n\n\nNo Perfect multicolinearity. This is effectively a mathematical constraint. If any of the predictors are an exact linear combination of each other then we can’t actually calculate the model. Software solves this for us and kicks out various terms. It will do this arbitrarily so.\n\n\nIn practice this assumption is never violated but we still need to worry about some collinearity. The general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can’t systematically account for whether this will\n\nThe general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can’t systematically account for whether this will inflate the standard errors or deflate them. Meaning that if we are interested in variable importance or making statements about what moving one variable up or down will do.\n\nSpepherical Error term aka IID error assumption\n\nFor simplicity we can group two related assumptions\nThe first part is that we expect homogeneity\n\\[\n\\begin{equation}\n  var(\\mu_i) = E[\\mu_i] - E[\\mu_i | X_i)^2]\n\\end{equation}\n\\]\nThe second part is that there is no autocorrelation.\n\\[\n\\begin{equation}\n  cov(\\mu_i, \\mu_j | X_i, X_j) = 0\n\\end{equation}\n\\]\nIn practice when we violate this asumption our coefficients are unaffected but our standard errors can be drastically wrong. We can correct the standard errors to account for violations by loosening up how we compute the variance-covariance matrix. In some ways this is acceptable under very specific settings however. This can generally point to deeper issues with modeling the data. Like one way to trigger heterosckedasicity is simply to model binary data with OLS. A more appropriate solution would just to be model the DGP correctly.\n\n\n\nOne solution that this flavor of statistics has proposed to reduce the variance of OLS estimates when the number of predictors large or the predictors are really collinear than we can use shirkage aka regularization to penalize regression coefficients towards zero.\nRidge regression and LASSO regression are not entirely different than OLS regression. Each of these estimators reduces the residual sum of squares however we tack on a penalty term to the lasso and ridge estimators.\n\\[\\begin{align}\n\n\\text{OLS} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 \\\\\n\n\\text{Ridge} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum \\beta^{2}_{j} \\\\\n\n\\text{LASSO} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum |{\\beta_{j}}|\n\n\\end{align}\n\n\n\\]\nWhat is going on underneath the hood?\n\nRidge penalty we are summing the squared coefficients and multiplying it by the hyperparameter.\n\nEffectively imposing a penalty equivelent to the square of the magnitude of the coefficients\n\nLASSO penalty we are just summing the absolute values of the coefficients.\n\nEffectively imposing a penalty equivelent to the absolute value of the magnitude of coefficients\n\n\nThe reason why we want to penalize a model is that as we start training and start adding in variables we think help predict our outcome our models are going to start to do a better job fitting the noise. Each approach has pros and cons. A ridge penalty will let all predictors enter into the predictions even if they don’t contribute much. A lasso penalty will implictly start to kick out variables that don’t contribute anything to the model. Theoretically this is important if we have a high amount of multicollinearity/we don’t wanted automated feature selection. Effectively this question is a little theoretical in practice we are going to compare the predicitve accuracy of the two to find this out. The ridge and lasso penalties will appear in other cases so I will probably go over this again.\n\n\n\nTo set up this question lets think of a hypothetical business problem. We are trying to assign somebody a premium based on the data we have on other people. We have access to their age, their location, past driving history, make and model of their car, and gender. Theoretically we would expect these variables to be related to one another in some way. For example make and model of their car, age, and gender is probably going to tell us something about their past driving history. We would expect that a 20 year old male driving a sports car is probably going to have a higher likelihood of having one or more speeding tickets then a female driving the same car. When we go to model this relationship each of these variables are going to be related to each other.\nLets setup an OLS with this example where age enters into the model in years, car type is a factor with a Toyota Corolla as it reference level, past incidents is the log of past driving behavior, location is a simple indicator variable where rural is the reference level, and gender is a indicator variable where the reference level is female.\n\\[\nPremium = \\alpha + \\beta_{1} Age + \\beta_{2} \\text{car_type} + \\beta_{3} location + \\beta_{4} \\log{history} + \\beta_{5} Gender + \\varepsilon\n\\]\nFor simplicity we will focus on gender as the primary explainer for insurance premiums. Lets say that gender and location are highly correlated this may occur for a variety of reasons e.g. presence of a military base, presence of a college campus etc. When we go to interpret the effect of being male on an insurance premium we hold all variables in the equation constant. However, if the moving from a rural area to the city is highly deterministic of gender we can neatly unpack the effect of gender on insurance premium. While an interpretational problem this makes it difficult to unpack the effect of a single variable on insurance premiums.\nAnother important factor is multicollinearity impacts our uncertainty estimates either biasing them down leading us to fail to reject the null more times than we should or can bias our standard errors downwards over rejecting the null hypothesis (Gujarati, Porter, and Gunasekar 2012; Lenz and Sahn 2021). Lets take a simulated example to make this point a bit more explicit\n\nlibrary(performance)\nlibrary(magrittr)\nlibrary(tidyverse)\n\n\n\n\ngenerate_dummy_data &lt;- function(data, seed = NULL) {\n  set.seed(seed = seed)\n  data &lt;- dplyr::tibble(gender = rbinom(100, 1, 0.5),\n                        location = ifelse(runif(100) &lt; 0.9, gender, 1 - gender),\n                        y = 5 + 3 * gender + rnorm(100, 0, 3))\n  return(data)\n}\n\ngenerate_dummy_model &lt;- function(data) {\n  model &lt;- lm(y ~ gender + location, data = data)\n  return(model)\n}\n\nextract_model_coefficients &lt;- function(model) {\n  coefs &lt;- broom::tidy(model) |&gt; janitor::clean_names()\n  # replace dots and white spaces with underscores\n  return(coefs)\n}\n\nrun_lm_simulation &lt;- function(data_generator, model_generator, n = 1000) {\n  simulation &lt;- \n    dplyr::tibble(i = 1:n) %&gt;% \n    dplyr::mutate(\n      data = purrr::map(.x = i, .f = ~ data_generator(seed = .x)),\n      model = purrr::map(.x = data, .f = model_generator),\n      coefs = purrr::map(.x = model, .f = extract_model_coefficients)\n      ) %&gt;%\n  return(simulation)}\n\ndummy_simulation &lt;- run_lm_simulation(data_generator = generate_dummy_data,\n                                      model_generator = generate_dummy_model, \n                                      n = 1000)\n\n\n\nsimulated_p = dummy_simulation |&gt;\n    unnest(coefs) |&gt;\n    filter(term != '(Intercept)')\n\n\n\nggplot(simulated_p, aes(x = p_value, fill = term)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(term), scales = \"free_y\", ncol = 1) +\n  labs(title = \"Distribution of P-Values for x and x_collinear\",\n       x = \"P-Value\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this case we see that we have some encouraging results that we are not going to over reject gender but if we look at location we are much more prone to failing to reject the null hypothesis. This is fine in a simulated world because we induced this relationship ourselves. However, in the real world that is not available to us.\nTo get around multicollinearity we use methods that penalize our coeficients towards zero. The two most common forms of penalization are known as ridge and LASSO regression. The canonical regression equation minimizes the sum of squared residuals. Regularization adds a penalty term to this equation that shrinks the coefficient estimates toward zero. Ridge regression (L2 penalty) adds the sum of the squared coefficients to the loss function, which discourages large coefficient values but does not set them to zero. LASSO regression (L1 penalty) adds the sum of the absolute values of the coefficients, which can shrink some coefficients to exactly zero, effectively performing feature selection.\n\nlibrary(tidymodels)\n\nhitters = read_csv('data/Hitters.csv') |&gt;\n    janitor::clean_names() |&gt;\n    filter(!is.na(salary))\n\n\nridge_spec = linear_reg(mixture = 0, penalty = 0) |&gt;\n    set_mode('regression') |&gt;\n    set_engine('glmnet') |&gt;\n    fit(salary ~ ., data = hitters)\n\nridge_spec |&gt;\n    autoplot() + \n    theme_minimal() +\n    labs(title = 'Ridge Regularization')\n\n\n\n\n\n\n\n\nAs we can see with the ridge penalty as we increase the size of the penalty the closer to zero these coefficients get. For some coefficients they get pushed towards zero immediately. One of the things that happens with multicollinear data is that jumps in one variable can cause massive swings in another in ways that aren’t entirely transparent. In prediction context we may not neccessarily care about the individual impact of one of 100 variables. We are more concerned that our model is healthy and making good predictions on data that it hasn’t seen. By regularizing our model we penalize the model for being extremely flexible this may increase our bias a bit but this comes at the gain of reducing the variance. What this means is that we are going to get a healthier prediction machine. For a LASSO model some of these coefficients will be zero just by the nature of the penalty. Effectively performing feature selection for us.\n\n\n\nOne important thing to note is that neither when we change the scale of our predictor variable the LASSO and Ridge esstimates will not adjust accordingly because the larger coefficients that result from say salary and age are going to be on different scales which our coefficients are going to respond to accrodingly. However, the penalty term is not going react well at all since it will penalize larger coefficients by default. To build on our prior knowledge lets build a scikit learn pipeline that validates\n\nK = 5\nkfold = KFold(K,\n                  random_state=0,\n                  shuffle=True)\n\nhitters = pl.read_csv('data/Hitters.csv').with_columns(\n     pl.col('Salary').str.to_integer(strict = False).alias('Salary')\n).drop_nulls()\n\nscaler = StandardScaler()\n\nK = 5\n\nx = hitters.select(pl.exclude('Salary')).to_dummies(cs.string())\n\ny = hitters.select(pl.col('Salary')).to_numpy()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2 )\n\nlambdas = 10**np.linspace(8, -2, 100) / y.std()\n\nridge = ElasticNet(l1_ratio = 0)\n\nparam_grid = {'ridge__alpha': lambdas}\n\npipe = Pipeline(steps = [('scaler', scaler), ('ridge', ridge)])\n\npipe.fit(x_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', ElasticNet(l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', ElasticNet(l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(l1_ratio=0) \n\nvalidation = ShuffleSplit(n_splits=1,\n                              test_size=0.5,\n                              random_state=0)\n\ngrid = GridSearchCV(pipe, param_grid, cv = validation, scoring='neg_mean_squared_error')\n\ngrid.fit(x, y)\n\nGridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', ElasticNet(l1_ratio=0))]),\n             param_grid={'ridge__alpha': array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', ElasticNet(l1_ratio=0))]),\n             param_grid={'ridge__alpha': array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0) \n\ngrid.best_params_['ridge__alpha']\n\nnp.float64(0.0009804286565854396)\n\ngrid.best_estimator_\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0) \n\nridge_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            -grid.cv_results_['mean_test_score'],\n            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\nax.set_ylim([50000,250000])\n\n(50000.0, 250000.0)\n\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)\n\n\n\n\n\n\n\n\nSo we have it looking at the MSE but we can also look at it with R2\n\nridge_cv = ElasticNetCV(alphas = lambdas,\n                        l1_ratio = 0,\n                        cv = kfold)\n\n\npipe_cv = Pipeline(steps =[('scaler', scaler), ('ridge', ridge_cv)])       \n\npipe_cv.fit(x, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNetCV?Documentation for ElasticNetCVElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03, 3.60638894e+03, 2.85800156e+03,\n       2.26491736e+03, 1.794908...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n             cv=KFold(n_splits=5, random_state=0, shuffle=True), l1_ratio=0) \n\ntuned_ridge = pipe_cv.named_steps['ridge']\nridgeCV_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            tuned_ridge.mse_path_.mean(1),\n            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_ylim([50000,250000])\n\n(50000.0, 250000.0)\n\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)                   \n\n\n\n\n\n\n\n\nWe could do the same thing with a lasso regression but this workflow is not great instead we can make a function and then loop over the grids.\n\n\nmodels = {\n    'ridge': ElasticNet(l1_ratio=0),\n    'lasso': ElasticNet(l1_ratio=1)  # Lasso is a special case of ElasticNet\n}\n\n# Prepare for GridSearchCV\n\nparam_grids = {\n    'ridge': {'elasticnet__alpha': lambdas},  # Use 'elasticnet' as the step name\n    'lasso': {'elasticnet__alpha': lambdas}\n}\n\nresults = []\n# Set up ShuffleSplit cross-validation for GridSearchCV\nvalidation = ShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n\n# Function to perform grid search and output results\ndef tune_model(model_name, model, param_grid):\n    # Build pipeline\n    pipe = Pipeline(steps=[('scaler', scaler), ('elasticnet', model)])  # Step name matches model_name\n    \n    # Perform grid search\n    grid = GridSearchCV(pipe, param_grid, cv=validation, scoring='neg_mean_squared_error')\n    grid.fit(x_train, y_train)\n    \n    # Extract best parameters and model\n    best_alpha = grid.best_params_['elasticnet__alpha']\n    best_model = grid.best_estimator_\n    results.append({\n        'model': model_name.capitalize(),  # Store as \"Ridge\" or \"Lasso\"\n        'best_alpha': best_alpha,\n        'best_model': str(best_model),\n        'best_score': grid.best_score_\n    })\n    return best_model\n    \nbest_models = {}\n# Tune each model\nfor model_name, model in models.items():\n    best_models[model_name] = tune_model(model_name, model, param_grids[model_name])\n\n\nresults_df = pl.DataFrame(results)\n\nSo now we have the best model but we would like to grab the most important features. We may need something to present to stakeholders or to better understand what is going on in our models.\n\ndef get_variable_importance(best_model, feature_names):\n    # Access the 'elasticnet' step in the pipeline\n    elastic_net_step = best_model.named_steps['elasticnet']\n    coef = elastic_net_step.coef_\n    feature_importance = sorted(zip(feature_names, coef), key=lambda x: abs(x[1]), reverse=True)\n    features, coefficients = zip(*feature_importance)\n    return features, coefficients\n# Prepare side-by-side plots\nfig, axes = plt.subplots(1, len(best_models), figsize=(15, 6), sharey=True)\n\n# Get feature names\nfeature_names = x.columns\n\n# Plot VIP for each model\nfor i, (model_name, best_model) in enumerate(best_models.items()):\n    # Extract variable importance\n    features, coefficients = get_variable_importance(best_model, feature_names)\n    \n    # Determine bar colors based on coefficient sign\n    colors = ['green' if coef &gt; 0 else 'red' for coef in coefficients]\n    \n    # Create subplot\n    axes[i].barh(features, np.abs(coefficients), color=colors)\n    axes[i].set_title(f'Variable Importance: {model_name.capitalize()}')\n    axes[i].set_xlabel('Absolute Coefficient Value')\n    if i == 0:  # Add y-axis label only for the first plot\n        axes[i].set_ylabel('Features')\n    axes[i].invert_yaxis()  # Invert y-axis for descending order\n    \n    # Add a legend\n    axes[i].legend(['Positive Impact', 'Negative Impact'], loc='lower right')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhich is nice we see what is postively impacting value. However, I don’t neccessarily like these plots since we are not really showing anything interesting. A better way would be to show marginal effects. In R we would do something to this effect.\n\n\nCode\nlibrary(tidymodels)\nlibrary(marginaleffects)\nlibrary(ISLR)\n\nHitters &lt;- as_tibble(Hitters) |&gt;\n  filter(!is.na(Salary))\n\nHitters_split &lt;- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train &lt;- training(Hitters_split)\nHitters_test &lt;- testing(Hitters_split)\n\nHitters_fold &lt;- vfold_cv(Hitters_train, v = 10)\n\n\nridge_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) |&gt; \n  step_novel(all_nominal_predictors()) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nridge_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 0) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"glmnet\")\n\nridge_workflow &lt;- workflow() |&gt; \n  add_recipe(ridge_recipe) |&gt; \n  add_model(ridge_spec)\n\npenalty_grid &lt;- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\ntune_res &lt;- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nbest_penalty = select_best(tune_res, metric = 'rsq')\n\nridge_final &lt;- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit &lt;- fit(ridge_final, data = Hitters_train)\n\navg_predictions(ridge_final_fit, newdata = Hitters_test, by = 'somefactor', newdata = some_grid_of_values)\n\n\nIn machine learning parlance we could make a partial dependency plot. Which is really just plotting the predictions when we move values of our variables that we may think are interesting.\n\n\n\nWe have more than a few evaluation metrics for linear regression or regression based tasks to be specific so it is worth going over the most popular ones. In at Table 1 I outline the basics of each measure but will discuss each of these measures in greater detail.\n\n\nCode\nols_mets = pl.DataFrame({\n    'Metric': [\n        'R-Squared',\n        'Mean Square Error (MSE)',\n        'Root Mean Square Error (RMSE)',\n        'Mean Absolute Error (MAE)',\n        'Mean Absolute Percentage Error (MAPE)',\n        'Symmetric Mean Absolute Percentage Error (SMAPE)'\n    ],\n    'What it Does': [\n        'The proportion of variation explained by the model',\n        'The average error of the model',\n        'The square root of the average error of the model',\n        'Similar to the MSE but we calculate with |y - yhat|',\n        'Similar to the MSE but calculated by |y - yhat| / yi',\n        'Similar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)'\n    ]\n})\n\nGT(ols_mets)\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nMetric\nWhat it Does\n\n\n\n\nR-Squared\nThe proportion of variation explained by the model\n\n\nMean Square Error (MSE)\nThe average error of the model\n\n\nRoot Mean Square Error (RMSE)\nThe square root of the average error of the model\n\n\nMean Absolute Error (MAE)\nSimilar to the MSE but we calculate with |y - yhat|\n\n\nMean Absolute Percentage Error (MAPE)\nSimilar to the MSE but calculated by |y - yhat| / yi\n\n\nSymmetric Mean Absolute Percentage Error (SMAPE)\nSimilar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)\n\n\n\n\n\n\n\n\n\n\n\nBefore we dive into each of these metrics it is worth going over the OLS optimization problem. For an OLS the optimization problem is the sum of the squared residuals. The objective function is given by\n\\[\n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - \\hat{y}_{i})^{2}\n\\]\nFor notational purposes \\(\\beta\\) is a vector of regression coefficients, \\(y_{i}\\) is the observed value of our dependent variable and \\(\\hat{y}\\) is the predicted value from our model. It is a little bit clearer if we rewrite the optimization problem with\n\\[\n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - (\\alpha + \\beta \\times x_i))^{2}\n\\]\nOf the optimatization problems in the world this one is kind of beautifully simple. So linear regression is going to draw a line (or hyperplane) through our data that tries to minimize the distance between each of the points. We are going to see how far away the predicted value is from the observed value and then we are going to square them to ensure they aren’t negative then we are going to sum them to get the total distance for each point. In effect what is going on is something to this\n\n\nfitted = form_model.fittedvalues\n\nresids = form_model.resid\n\nboston['fitted'] = fitted\n\nm,b = np.polyfit(boston['lstat'], boston['medv'], 1)\n\nfig,ax = plt.subplots(figsize=(15,6))\nplt.plot(boston['lstat'], boston['lstat'] * m + b, color = 'pink')\nplt.scatter(boston['lstat'], fitted, color = 'red',alpha = 0.1)\nplt.scatter(boston['lstat'], boston['medv'], color = 'blue', alpha = 0.5)\nfor i in range(len(boston)):\n    plt.plot([boston['lstat'].iloc[i], boston['lstat'].iloc[i]], \n             [boston['medv'].iloc[i], boston['fitted'].iloc[i]], color='blue', alpha=0.5)\nplt.xlabel('Percent Low Status')\nplt.ylabel('Home Value Median ($1000)')\n\n\n\n\n\n\n\n\nEffectively we are minimizing the distance between the points and our line via this objective function. These metrics are all broadly related to this same idea.\n\n\n\\(R^2\\) aka the ‘coefficient of determination’ is some what straight forward it is \\(1-\\frac{RSS}{TSS}\\). Lets break this down a little further\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum^N_{i = 1} (y_{i} - \\hat{y}_i)^{2}}{\\sum^N_{i = 1} (y_i - \\bar{y})^{2}}\n\\]\nSo if we look closely at the fraction is the numerator is the sum of the squared residuals or the distance metric we talked about earlier. In the denominator we have the something broadly similar. The total sum of squares is something akin to a more naive predicition error. Instead of \\(\\hat{y}\\) we have \\(\\bar{y}\\) which is just the mean of y. So we are effectively taking the summed distance of the observed value of y and the mean of y. What this gives us is the fraction of the variance explained by our model. In political science and the social sciences more generally this is something that most people don’t even report anymore. In many cases we are creating regression tables where we have different dependent variables. One of the primary weaknesses of using \\(R^{2}\\) is that the easiest way to improve your \\(R^2\\) is to simply add more variables. As the model gets more information it is is going to a better job of getting \\(\\hat{y}\\) close to the observed value of Y.\nAs a political scientist by training I am always skeptical of the idea of using \\(R^2\\) for anything other than chuckling at people who see it as some sort of measure of distance from the true model. I think King (1986) sums some of the criticisms quite well. Decisions about how we process our data are also going to influence this metric because we are going to either increase or decrease the distance. \\(R^2\\) in a machine learning context may fit the description of a goodness of fit test where we are figuring out the amount of variation in the data explained. We are often swapping out variables on the right hand side. \\(R^2\\) we can assess if some combination of these variables reduce the amount of unexplained variation. Generally we would probably want to use the penalized version of \\(R^2\\) so we don’t reward our model for just adding more variables. However, I think one of the most interesting points about \\(R^2\\) are things that are already present in our OLS model. This is going to hold less and less as our model gets less interpretable.\n\n\n\nHeuristically it seems a lot of machine learning evaluation for regression models uses mean squared error or some variation of it to check model performance. Conceptually these metrics are linked because we can get the \\(R^2\\) by doing:\n\\[\nR^2 = 1 - \\frac{x \\times MSE}{\\sum^n_{i =1} (y_i - \\hat{y})^2}\n\\]\nHowever, the thing with \\(R^2\\) is that it is not neccessarily that sensistive to extreme values. While in general it is difficult to have wildly divergent measure of \\(R^2\\) and MSE we may actually care about how our models perform in response to outliers. If big errors are costly to our model performance than we are going to want a measure that more directly captures their influence.\n\\[\nMSE = \\frac{1}{n} \\sum^n_{i =1} (y_i - \\hat{y})^2\n\\]\nWhere we are effectively getting the average error instead of the total error. One thing that flips with respect to \\(R^2\\) is that we are getting a slightly more interpretable difference metric. Meaning that we are getting the average distance between our predicted values and the observed values. So we are looking for this number to be kind of low! However, whats important to note is that there is no scaling factor. Meaning that outliers are going to really effect this metric because we are taking the difference and then going to square it. Lets imagine a situation where I am trying to increase the distance between a friend and the ball. If I stand right next to them and throw a ball it will land closer to them then if I started from farther away and threw the ball. Since we go through this squaring process what ends up happening is that we don’t have a great mental model of how to interpret this distance. What is the squared deviation of the median home value? Kind of not that intuitive because that distance is harder to put into words. To get it back on the original scale we can use the Root mean squared error which is just\n\\[\nRMSE = \\sqrt{MSE}\n\\]\nThis puts the MSE back on its original scale. If our model is predicting median home prices and the RMSE is 1000, this means that a typical prediction error is around 1000 dollars, with larger errors being weighted more heavily than smaller ones.\n\n\n\n\nIn the prior, our model could be good at predicting the data we have. However, when we introduce new data, it might struggle to generalize and predict new observations accurately. This challenge arises from overfitting, where the model is really good at describing the dataset it has already seen. To mitigate this we split our data into training sets, test sets, and validation sets effectively hiding parts of our data from our model. It never has access to every single part of the data. By reducing the amount of data the model sees it can’t learn every single strange data point in the model.\nThe reason we do this is because we want our model to predict new data but also do a good job of approximating the data generating process. These two goals are inherently conflictual. Bias represents how far away we are from the target while variance represents how far away our guesses are from each other. If we build a model that is good at predicting every single quirk of the dataset, aka reducing variance, then when we introduce new data to the model it is going to be very very brittle. If we reduce the complexity of the model making it more flexible, aka reducing bias, then we risk not being able to catch the patterns in our data.\nIn the bias-variance tradeoff, we aim to find a balance: a model that is simple enough to generalize well to new data but complex enough to capture the important patterns. Techniques like cross-validation, regularization, and hyperparameter tuning help us navigate this tradeoff and improve the model’s predictive performance.\nMathematically we can think of the bias-variance tradeoff along the lines of mean squared error where\n\\[\nError = Bias^2 + Variance + Noise\n\\]\nWhere we have the how far away the predictions are from the real values, how far away the predictions are from each other, and the randomness from the data generating process. We can’t really doing anything about the randomness from the DGP. A useful heuristic is that we can reduce how far away our predictions are from our real values by introducing more complexity to the model. This may mean more complex functional forms of variables, introduce interactions, or use more flexible models. We can increase variance by using simple models, use bagging or boosting to leverage “bad learners”, or use regularization to penalize our models.\n\n\n\n\nCode\n# code taken from https://www.r-bloggers.com/2021/07/double-descent-part-i-sample-wise-non-monotonicity/\n\nf &lt;- function(x){\n  (-0.4 + 1/(x+0.5)) + (0.5*exp(x))\n}\n#The point where the prediction error is minimized\noptimum &lt;- optimize(f, interval=c(0, 1), maximum=FALSE, tol = 1e-8)\ntemp_data &lt;- data.frame(x = optimum$minimum, y=optimum$objective)\n\nf1 &lt;- function(x){\n  ifelse(x&lt;=2,   (-0.4 + 1/(x+0.5)) + (0.5*exp(x)), NA)\n}\nf2 &lt;- function(x){\n  ifelse(x&gt;=2, (0 + 1/(1/(0.5*exp(4/x)))), NA)\n}\n#Prediction variance function (it is piecewise so creating two of them).\nvar_f1 &lt;- function(x){\n  ifelse(x&lt;=2,   (0.5*exp(x)), NA)\n}\nvar_f2 &lt;- function(x){\n  ifelse(x&gt;=2, 1/(1/(0.5*exp(4/x))), NA)\n}\n#Prediction bias function (it is piecewise so creating two of them).\nbias_f1 &lt;- function(x){\n  ifelse(x&lt;=2,-0.4 + 1/(x+0.5),NA )\n}\nbias_f2 &lt;- function(x){\n  ifelse(x&gt;=2,0,NA )\n}\nggplot(data = temp_data, aes(x=x, y=y)) +\n  xlim(0,4) +\n  geom_function(fun = var_f1, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = var_f2, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = bias_f1, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = bias_f2, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = f1, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_function(fun = f2, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  geom_point() + \n  theme_minimal() + ylab(\"Error\") + xlab(\"Number of Predictors/Number of observations\") +\n  theme(axis.text=element_blank(),\n        axis.ticks=element_blank()) +\n  annotate(\"text\", x=0.32, y=-0.2+1/(0.2+0.5), label= expression(paste(\"B\", ias^2)), color = \"blue\") +\n  annotate(\"text\", x=0.2, y=-0.2+0.5*exp(0.2), label= \"Variance\", color = \"red\") +\n  annotate(\"text\", x=0.26, y=0.21+(0.5*exp(0.2) + 1/(0.2+0.5)), label= expression(paste(\"Variance + B\", ias^2)), color = \"forestgreen\") +\n  annotate(\"text\", x=2.4, y=-0.2+1/(0.2+0.5), label= \"Interpolation limit\", color = \"black\") \n\n\n\n\n\n\n\n\n\nDouble descent is effectively just the idea that we don’t always see degrading performance in the test set when our model is really complex. Once we get past a certain threshold our model will start to perform well again.\nEffectively at some point the model will pass through every point in our dataset in multidimensional space. Effectively what is happening is that the model will memorize the training set because we let it be super complex and all we are doing is slightly modifying how we draw the line. Thus within some region we call the interpolation threshold our model is going to do poorly within this region. Once we get past this region our model’s performance will start to get better again and even outperform the ‘simple’ model again.\nWe can think of double descent as recasting our bias variance tradeoff a bit. Once we reach the interpolation threshold our model gains a somewhat crude approximation of the entirety of the data. In the bias variance framework think of bivariate linear regression. Its predictions are going to be pretty far off in some cases. A similar thing kind of happens in the interpolation region. The interpolations are crude but as we add more and more parameters the interpolations start to get better and better until the test error improves.\nThe reason this happens is that when we hit this interpolation region there is effectively only one model that can perfectly interpolate the data and when we add new data this will get thrown off. With to few parameters in the interpolation region we are not giving our model enough shots to be flexible enough to get this specification correct. By adding more and more parameters what we are doing is giving the model more chances to draw the line effectively.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "linear-regression.html#transformations",
    "href": "linear-regression.html#transformations",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "We can start to do things like add transformations like this\n\nx = boston.drop(columns = 'medv')\n\ny = boston['medv']\n\nx['sqr_lstat'] = x['lstat'] **2\n\ny['medv'] = np.log(y['medv'])\n\nx = sm.add_constant(x) \n\nsm.OLS(y,x).fit().summary()\n\nFor interactions we do something like this\n\nx = boston[['age', 'lstat']]\n\nx['lstat:age'] = x['age'] * x['lstat']\n\nx = sm.add_constant(x) \n\ny = boston['medv']\n\n\nsm.OLS(y, x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.556\n\n\nModel:\nOLS\nAdj. R-squared:\n0.553\n\n\nMethod:\nLeast Squares\nF-statistic:\n209.3\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n4.86e-88\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-1635.0\n\n\nNo. Observations:\n506\nAIC:\n3278.\n\n\nDf Residuals:\n502\nBIC:\n3295.\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n36.0885\n1.470\n24.553\n0.000\n33.201\n38.976\n\n\nage\n-0.0007\n0.020\n-0.036\n0.971\n-0.040\n0.038\n\n\nlstat\n-1.3921\n0.167\n-8.313\n0.000\n-1.721\n-1.063\n\n\nlstat:age\n0.0042\n0.002\n2.244\n0.025\n0.001\n0.008\n\n\n\n\n\n\n\n\nOmnibus:\n135.601\nDurbin-Watson:\n0.965\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n296.955\n\n\nSkew:\n1.417\nProb(JB):\n3.29e-65\n\n\nKurtosis:\n5.461\nCond. No.\n6.88e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.88e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nFor qualitative variables we need to switch to a new dataset. There is a lot of interesting information in multicategory variables. One thing that we have to keep in mind when using qualitative variables normally is that we have a reference category. This may not always be straightforward to infer and we are throwing information away that is interesting. One hot encoding or breaking out the qualitative variable to indicatior variables is a nice way to do this,\n\ncarseats = pl.read_csv('data/Carseats.csv')\n\ncarseats.select(pl.col('ShelveLoc').unique()).head()\n\n\nshape: (3, 1)\n\n\n\nShelveLoc\n\n\nstr\n\n\n\n\n\"Bad\"\n\n\n\"Medium\"\n\n\n\"Good\"\n\n\n\n\n\ncars_small = carseats.select(pl.col('Sales', 'CompPrice', 'Income', 'ShelveLoc')).to_dummies(cs.string())\n\nx = cars_small.select(pl.exclude('Sales')).to_pandas()\n\ny = cars_small.to_pandas()['Sales']\n\nx = sm.add_constant(x)\n\nsm.OLS(y, x).fit().summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nSales\nR-squared:\n0.352\n\n\nModel:\nOLS\nAdj. R-squared:\n0.345\n\n\nMethod:\nLeast Squares\nF-statistic:\n53.63\n\n\nDate:\nWed, 26 Feb 2025\nProb (F-statistic):\n4.38e-36\n\n\nTime:\n16:23:16\nLog-Likelihood:\n-895.60\n\n\nNo. Observations:\n400\nAIC:\n1801.\n\n\nDf Residuals:\n395\nBIC:\n1821.\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n3.8101\n0.755\n5.046\n0.000\n2.326\n5.295\n\n\nCompPrice\n0.0106\n0.007\n1.420\n0.156\n-0.004\n0.025\n\n\nIncome\n0.0184\n0.004\n4.473\n0.000\n0.010\n0.026\n\n\nShelveLoc_Bad\n-0.9343\n0.312\n-2.994\n0.003\n-1.548\n-0.321\n\n\nShelveLoc_Good\n3.8167\n0.322\n11.843\n0.000\n3.183\n4.450\n\n\nShelveLoc_Medium\n0.9277\n0.287\n3.235\n0.001\n0.364\n1.492\n\n\n\n\n\n\n\n\nOmnibus:\n0.001\nDurbin-Watson:\n1.941\n\n\nProb(Omnibus):\n1.000\nJarque-Bera (JB):\n0.030\n\n\nSkew:\n-0.003\nProb(JB):\n0.985\n\n\nKurtosis:\n2.958\nCond. No.\n7.74e+17\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 1.38e-29. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nFor quick and dirty things this is nice and fairly straightforward. We are not really going to delve to deep on fitting a ton of models but really this would just involve some F-string. Instead we are going to focus on the machine learning workflow. Going through step by step and doing these are not terribily time consuming but as things get more complicated we are going to need a more robust framework to deal with this.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "linear-regression.html#regression-assumptions",
    "href": "linear-regression.html#regression-assumptions",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "Our basic assumptions of linear regression are that\n\nThere are linear relationship between our outcome and our predictors\n\n\nThis is something we violate all the time. For the most part we can transform the dependent or independent variable to dependent variable\n\n\nNo Perfect multicolinearity. This is effectively a mathematical constraint. If any of the predictors are an exact linear combination of each other then we can’t actually calculate the model. Software solves this for us and kicks out various terms. It will do this arbitrarily so.\n\n\nIn practice this assumption is never violated but we still need to worry about some collinearity. The general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can’t systematically account for whether this will\n\nThe general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can’t systematically account for whether this will inflate the standard errors or deflate them. Meaning that if we are interested in variable importance or making statements about what moving one variable up or down will do.\n\nSpepherical Error term aka IID error assumption\n\nFor simplicity we can group two related assumptions\nThe first part is that we expect homogeneity\n\\[\n\\begin{equation}\n  var(\\mu_i) = E[\\mu_i] - E[\\mu_i | X_i)^2]\n\\end{equation}\n\\]\nThe second part is that there is no autocorrelation.\n\\[\n\\begin{equation}\n  cov(\\mu_i, \\mu_j | X_i, X_j) = 0\n\\end{equation}\n\\]\nIn practice when we violate this asumption our coefficients are unaffected but our standard errors can be drastically wrong. We can correct the standard errors to account for violations by loosening up how we compute the variance-covariance matrix. In some ways this is acceptable under very specific settings however. This can generally point to deeper issues with modeling the data. Like one way to trigger heterosckedasicity is simply to model binary data with OLS. A more appropriate solution would just to be model the DGP correctly.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "linear-regression.html#shrinkage-estimators",
    "href": "linear-regression.html#shrinkage-estimators",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "One solution that this flavor of statistics has proposed to reduce the variance of OLS estimates when the number of predictors large or the predictors are really collinear than we can use shirkage aka regularization to penalize regression coefficients towards zero.\nRidge regression and LASSO regression are not entirely different than OLS regression. Each of these estimators reduces the residual sum of squares however we tack on a penalty term to the lasso and ridge estimators.\n\\[\\begin{align}\n\n\\text{OLS} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 \\\\\n\n\\text{Ridge} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum \\beta^{2}_{j} \\\\\n\n\\text{LASSO} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum |{\\beta_{j}}|\n\n\\end{align}\n\n\n\\]\nWhat is going on underneath the hood?\n\nRidge penalty we are summing the squared coefficients and multiplying it by the hyperparameter.\n\nEffectively imposing a penalty equivelent to the square of the magnitude of the coefficients\n\nLASSO penalty we are just summing the absolute values of the coefficients.\n\nEffectively imposing a penalty equivelent to the absolute value of the magnitude of coefficients\n\n\nThe reason why we want to penalize a model is that as we start training and start adding in variables we think help predict our outcome our models are going to start to do a better job fitting the noise. Each approach has pros and cons. A ridge penalty will let all predictors enter into the predictions even if they don’t contribute much. A lasso penalty will implictly start to kick out variables that don’t contribute anything to the model. Theoretically this is important if we have a high amount of multicollinearity/we don’t wanted automated feature selection. Effectively this question is a little theoretical in practice we are going to compare the predicitve accuracy of the two to find this out. The ridge and lasso penalties will appear in other cases so I will probably go over this again.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "linear-regression.html#what-is-multicollinearity-and-how-do-we-fix-it",
    "href": "linear-regression.html#what-is-multicollinearity-and-how-do-we-fix-it",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "To set up this question lets think of a hypothetical business problem. We are trying to assign somebody a premium based on the data we have on other people. We have access to their age, their location, past driving history, make and model of their car, and gender. Theoretically we would expect these variables to be related to one another in some way. For example make and model of their car, age, and gender is probably going to tell us something about their past driving history. We would expect that a 20 year old male driving a sports car is probably going to have a higher likelihood of having one or more speeding tickets then a female driving the same car. When we go to model this relationship each of these variables are going to be related to each other.\nLets setup an OLS with this example where age enters into the model in years, car type is a factor with a Toyota Corolla as it reference level, past incidents is the log of past driving behavior, location is a simple indicator variable where rural is the reference level, and gender is a indicator variable where the reference level is female.\n\\[\nPremium = \\alpha + \\beta_{1} Age + \\beta_{2} \\text{car_type} + \\beta_{3} location + \\beta_{4} \\log{history} + \\beta_{5} Gender + \\varepsilon\n\\]\nFor simplicity we will focus on gender as the primary explainer for insurance premiums. Lets say that gender and location are highly correlated this may occur for a variety of reasons e.g. presence of a military base, presence of a college campus etc. When we go to interpret the effect of being male on an insurance premium we hold all variables in the equation constant. However, if the moving from a rural area to the city is highly deterministic of gender we can neatly unpack the effect of gender on insurance premium. While an interpretational problem this makes it difficult to unpack the effect of a single variable on insurance premiums.\nAnother important factor is multicollinearity impacts our uncertainty estimates either biasing them down leading us to fail to reject the null more times than we should or can bias our standard errors downwards over rejecting the null hypothesis (Gujarati, Porter, and Gunasekar 2012; Lenz and Sahn 2021). Lets take a simulated example to make this point a bit more explicit\n\nlibrary(performance)\nlibrary(magrittr)\nlibrary(tidyverse)\n\n\n\n\ngenerate_dummy_data &lt;- function(data, seed = NULL) {\n  set.seed(seed = seed)\n  data &lt;- dplyr::tibble(gender = rbinom(100, 1, 0.5),\n                        location = ifelse(runif(100) &lt; 0.9, gender, 1 - gender),\n                        y = 5 + 3 * gender + rnorm(100, 0, 3))\n  return(data)\n}\n\ngenerate_dummy_model &lt;- function(data) {\n  model &lt;- lm(y ~ gender + location, data = data)\n  return(model)\n}\n\nextract_model_coefficients &lt;- function(model) {\n  coefs &lt;- broom::tidy(model) |&gt; janitor::clean_names()\n  # replace dots and white spaces with underscores\n  return(coefs)\n}\n\nrun_lm_simulation &lt;- function(data_generator, model_generator, n = 1000) {\n  simulation &lt;- \n    dplyr::tibble(i = 1:n) %&gt;% \n    dplyr::mutate(\n      data = purrr::map(.x = i, .f = ~ data_generator(seed = .x)),\n      model = purrr::map(.x = data, .f = model_generator),\n      coefs = purrr::map(.x = model, .f = extract_model_coefficients)\n      ) %&gt;%\n  return(simulation)}\n\ndummy_simulation &lt;- run_lm_simulation(data_generator = generate_dummy_data,\n                                      model_generator = generate_dummy_model, \n                                      n = 1000)\n\n\n\nsimulated_p = dummy_simulation |&gt;\n    unnest(coefs) |&gt;\n    filter(term != '(Intercept)')\n\n\n\nggplot(simulated_p, aes(x = p_value, fill = term)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(term), scales = \"free_y\", ncol = 1) +\n  labs(title = \"Distribution of P-Values for x and x_collinear\",\n       x = \"P-Value\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this case we see that we have some encouraging results that we are not going to over reject gender but if we look at location we are much more prone to failing to reject the null hypothesis. This is fine in a simulated world because we induced this relationship ourselves. However, in the real world that is not available to us.\nTo get around multicollinearity we use methods that penalize our coeficients towards zero. The two most common forms of penalization are known as ridge and LASSO regression. The canonical regression equation minimizes the sum of squared residuals. Regularization adds a penalty term to this equation that shrinks the coefficient estimates toward zero. Ridge regression (L2 penalty) adds the sum of the squared coefficients to the loss function, which discourages large coefficient values but does not set them to zero. LASSO regression (L1 penalty) adds the sum of the absolute values of the coefficients, which can shrink some coefficients to exactly zero, effectively performing feature selection.\n\nlibrary(tidymodels)\n\nhitters = read_csv('data/Hitters.csv') |&gt;\n    janitor::clean_names() |&gt;\n    filter(!is.na(salary))\n\n\nridge_spec = linear_reg(mixture = 0, penalty = 0) |&gt;\n    set_mode('regression') |&gt;\n    set_engine('glmnet') |&gt;\n    fit(salary ~ ., data = hitters)\n\nridge_spec |&gt;\n    autoplot() + \n    theme_minimal() +\n    labs(title = 'Ridge Regularization')\n\n\n\n\n\n\n\n\nAs we can see with the ridge penalty as we increase the size of the penalty the closer to zero these coefficients get. For some coefficients they get pushed towards zero immediately. One of the things that happens with multicollinear data is that jumps in one variable can cause massive swings in another in ways that aren’t entirely transparent. In prediction context we may not neccessarily care about the individual impact of one of 100 variables. We are more concerned that our model is healthy and making good predictions on data that it hasn’t seen. By regularizing our model we penalize the model for being extremely flexible this may increase our bias a bit but this comes at the gain of reducing the variance. What this means is that we are going to get a healthier prediction machine. For a LASSO model some of these coefficients will be zero just by the nature of the penalty. Effectively performing feature selection for us.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "linear-regression.html#training-these-things",
    "href": "linear-regression.html#training-these-things",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "One important thing to note is that neither when we change the scale of our predictor variable the LASSO and Ridge esstimates will not adjust accordingly because the larger coefficients that result from say salary and age are going to be on different scales which our coefficients are going to respond to accrodingly. However, the penalty term is not going react well at all since it will penalize larger coefficients by default. To build on our prior knowledge lets build a scikit learn pipeline that validates\n\nK = 5\nkfold = KFold(K,\n                  random_state=0,\n                  shuffle=True)\n\nhitters = pl.read_csv('data/Hitters.csv').with_columns(\n     pl.col('Salary').str.to_integer(strict = False).alias('Salary')\n).drop_nulls()\n\nscaler = StandardScaler()\n\nK = 5\n\nx = hitters.select(pl.exclude('Salary')).to_dummies(cs.string())\n\ny = hitters.select(pl.col('Salary')).to_numpy()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2 )\n\nlambdas = 10**np.linspace(8, -2, 100) / y.std()\n\nridge = ElasticNet(l1_ratio = 0)\n\nparam_grid = {'ridge__alpha': lambdas}\n\npipe = Pipeline(steps = [('scaler', scaler), ('ridge', ridge)])\n\npipe.fit(x_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', ElasticNet(l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', ElasticNet(l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(l1_ratio=0) \n\nvalidation = ShuffleSplit(n_splits=1,\n                              test_size=0.5,\n                              random_state=0)\n\ngrid = GridSearchCV(pipe, param_grid, cv = validation, scoring='neg_mean_squared_error')\n\ngrid.fit(x, y)\n\nGridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', ElasticNet(l1_ratio=0))]),\n             param_grid={'ridge__alpha': array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', ElasticNet(l1_ratio=0))]),\n             param_grid={'ridge__alpha': array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0) \n\ngrid.best_params_['ridge__alpha']\n\nnp.float64(0.0009804286565854396)\n\ngrid.best_estimator_\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNet?Documentation for ElasticNetElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0) \n\nridge_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            -grid.cv_results_['mean_test_score'],\n            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\nax.set_ylim([50000,250000])\n\n(50000.0, 250000.0)\n\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)\n\n\n\n\n\n\n\n\nSo we have it looking at the MSE but we can also look at it with R2\n\nridge_cv = ElasticNetCV(alphas = lambdas,\n                        l1_ratio = 0,\n                        cv = kfold)\n\n\npipe_cv = Pipeline(steps =[('scaler', scaler), ('ridge', ridge_cv)])       \n\npipe_cv.fit(x, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNetCV?Documentation for ElasticNetCVElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03, 3.60638894e+03, 2.85800156e+03,\n       2.26491736e+03, 1.794908...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n             cv=KFold(n_splits=5, random_state=0, shuffle=True), l1_ratio=0) \n\ntuned_ridge = pipe_cv.named_steps['ridge']\nridgeCV_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            tuned_ridge.mse_path_.mean(1),\n            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_ylim([50000,250000])\n\n(50000.0, 250000.0)\n\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)                   \n\n\n\n\n\n\n\n\nWe could do the same thing with a lasso regression but this workflow is not great instead we can make a function and then loop over the grids.\n\n\nmodels = {\n    'ridge': ElasticNet(l1_ratio=0),\n    'lasso': ElasticNet(l1_ratio=1)  # Lasso is a special case of ElasticNet\n}\n\n# Prepare for GridSearchCV\n\nparam_grids = {\n    'ridge': {'elasticnet__alpha': lambdas},  # Use 'elasticnet' as the step name\n    'lasso': {'elasticnet__alpha': lambdas}\n}\n\nresults = []\n# Set up ShuffleSplit cross-validation for GridSearchCV\nvalidation = ShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n\n# Function to perform grid search and output results\ndef tune_model(model_name, model, param_grid):\n    # Build pipeline\n    pipe = Pipeline(steps=[('scaler', scaler), ('elasticnet', model)])  # Step name matches model_name\n    \n    # Perform grid search\n    grid = GridSearchCV(pipe, param_grid, cv=validation, scoring='neg_mean_squared_error')\n    grid.fit(x_train, y_train)\n    \n    # Extract best parameters and model\n    best_alpha = grid.best_params_['elasticnet__alpha']\n    best_model = grid.best_estimator_\n    results.append({\n        'model': model_name.capitalize(),  # Store as \"Ridge\" or \"Lasso\"\n        'best_alpha': best_alpha,\n        'best_model': str(best_model),\n        'best_score': grid.best_score_\n    })\n    return best_model\n    \nbest_models = {}\n# Tune each model\nfor model_name, model in models.items():\n    best_models[model_name] = tune_model(model_name, model, param_grids[model_name])\n\n\nresults_df = pl.DataFrame(results)\n\nSo now we have the best model but we would like to grab the most important features. We may need something to present to stakeholders or to better understand what is going on in our models.\n\ndef get_variable_importance(best_model, feature_names):\n    # Access the 'elasticnet' step in the pipeline\n    elastic_net_step = best_model.named_steps['elasticnet']\n    coef = elastic_net_step.coef_\n    feature_importance = sorted(zip(feature_names, coef), key=lambda x: abs(x[1]), reverse=True)\n    features, coefficients = zip(*feature_importance)\n    return features, coefficients\n# Prepare side-by-side plots\nfig, axes = plt.subplots(1, len(best_models), figsize=(15, 6), sharey=True)\n\n# Get feature names\nfeature_names = x.columns\n\n# Plot VIP for each model\nfor i, (model_name, best_model) in enumerate(best_models.items()):\n    # Extract variable importance\n    features, coefficients = get_variable_importance(best_model, feature_names)\n    \n    # Determine bar colors based on coefficient sign\n    colors = ['green' if coef &gt; 0 else 'red' for coef in coefficients]\n    \n    # Create subplot\n    axes[i].barh(features, np.abs(coefficients), color=colors)\n    axes[i].set_title(f'Variable Importance: {model_name.capitalize()}')\n    axes[i].set_xlabel('Absolute Coefficient Value')\n    if i == 0:  # Add y-axis label only for the first plot\n        axes[i].set_ylabel('Features')\n    axes[i].invert_yaxis()  # Invert y-axis for descending order\n    \n    # Add a legend\n    axes[i].legend(['Positive Impact', 'Negative Impact'], loc='lower right')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhich is nice we see what is postively impacting value. However, I don’t neccessarily like these plots since we are not really showing anything interesting. A better way would be to show marginal effects. In R we would do something to this effect.\n\n\nCode\nlibrary(tidymodels)\nlibrary(marginaleffects)\nlibrary(ISLR)\n\nHitters &lt;- as_tibble(Hitters) |&gt;\n  filter(!is.na(Salary))\n\nHitters_split &lt;- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train &lt;- training(Hitters_split)\nHitters_test &lt;- testing(Hitters_split)\n\nHitters_fold &lt;- vfold_cv(Hitters_train, v = 10)\n\n\nridge_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) |&gt; \n  step_novel(all_nominal_predictors()) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nridge_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 0) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"glmnet\")\n\nridge_workflow &lt;- workflow() |&gt; \n  add_recipe(ridge_recipe) |&gt; \n  add_model(ridge_spec)\n\npenalty_grid &lt;- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\ntune_res &lt;- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nbest_penalty = select_best(tune_res, metric = 'rsq')\n\nridge_final &lt;- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit &lt;- fit(ridge_final, data = Hitters_train)\n\navg_predictions(ridge_final_fit, newdata = Hitters_test, by = 'somefactor', newdata = some_grid_of_values)\n\n\nIn machine learning parlance we could make a partial dependency plot. Which is really just plotting the predictions when we move values of our variables that we may think are interesting.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "linear-regression.html#evaluation-metrics",
    "href": "linear-regression.html#evaluation-metrics",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "We have more than a few evaluation metrics for linear regression or regression based tasks to be specific so it is worth going over the most popular ones. In at Table 1 I outline the basics of each measure but will discuss each of these measures in greater detail.\n\n\nCode\nols_mets = pl.DataFrame({\n    'Metric': [\n        'R-Squared',\n        'Mean Square Error (MSE)',\n        'Root Mean Square Error (RMSE)',\n        'Mean Absolute Error (MAE)',\n        'Mean Absolute Percentage Error (MAPE)',\n        'Symmetric Mean Absolute Percentage Error (SMAPE)'\n    ],\n    'What it Does': [\n        'The proportion of variation explained by the model',\n        'The average error of the model',\n        'The square root of the average error of the model',\n        'Similar to the MSE but we calculate with |y - yhat|',\n        'Similar to the MSE but calculated by |y - yhat| / yi',\n        'Similar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)'\n    ]\n})\n\nGT(ols_mets)\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\nMetric\nWhat it Does\n\n\n\n\nR-Squared\nThe proportion of variation explained by the model\n\n\nMean Square Error (MSE)\nThe average error of the model\n\n\nRoot Mean Square Error (RMSE)\nThe square root of the average error of the model\n\n\nMean Absolute Error (MAE)\nSimilar to the MSE but we calculate with |y - yhat|\n\n\nMean Absolute Percentage Error (MAPE)\nSimilar to the MSE but calculated by |y - yhat| / yi\n\n\nSymmetric Mean Absolute Percentage Error (SMAPE)\nSimilar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)\n\n\n\n\n\n\n\n\n\n\n\nBefore we dive into each of these metrics it is worth going over the OLS optimization problem. For an OLS the optimization problem is the sum of the squared residuals. The objective function is given by\n\\[\n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - \\hat{y}_{i})^{2}\n\\]\nFor notational purposes \\(\\beta\\) is a vector of regression coefficients, \\(y_{i}\\) is the observed value of our dependent variable and \\(\\hat{y}\\) is the predicted value from our model. It is a little bit clearer if we rewrite the optimization problem with\n\\[\n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - (\\alpha + \\beta \\times x_i))^{2}\n\\]\nOf the optimatization problems in the world this one is kind of beautifully simple. So linear regression is going to draw a line (or hyperplane) through our data that tries to minimize the distance between each of the points. We are going to see how far away the predicted value is from the observed value and then we are going to square them to ensure they aren’t negative then we are going to sum them to get the total distance for each point. In effect what is going on is something to this\n\n\nfitted = form_model.fittedvalues\n\nresids = form_model.resid\n\nboston['fitted'] = fitted\n\nm,b = np.polyfit(boston['lstat'], boston['medv'], 1)\n\nfig,ax = plt.subplots(figsize=(15,6))\nplt.plot(boston['lstat'], boston['lstat'] * m + b, color = 'pink')\nplt.scatter(boston['lstat'], fitted, color = 'red',alpha = 0.1)\nplt.scatter(boston['lstat'], boston['medv'], color = 'blue', alpha = 0.5)\nfor i in range(len(boston)):\n    plt.plot([boston['lstat'].iloc[i], boston['lstat'].iloc[i]], \n             [boston['medv'].iloc[i], boston['fitted'].iloc[i]], color='blue', alpha=0.5)\nplt.xlabel('Percent Low Status')\nplt.ylabel('Home Value Median ($1000)')\n\n\n\n\n\n\n\n\nEffectively we are minimizing the distance between the points and our line via this objective function. These metrics are all broadly related to this same idea.\n\n\n\\(R^2\\) aka the ‘coefficient of determination’ is some what straight forward it is \\(1-\\frac{RSS}{TSS}\\). Lets break this down a little further\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum^N_{i = 1} (y_{i} - \\hat{y}_i)^{2}}{\\sum^N_{i = 1} (y_i - \\bar{y})^{2}}\n\\]\nSo if we look closely at the fraction is the numerator is the sum of the squared residuals or the distance metric we talked about earlier. In the denominator we have the something broadly similar. The total sum of squares is something akin to a more naive predicition error. Instead of \\(\\hat{y}\\) we have \\(\\bar{y}\\) which is just the mean of y. So we are effectively taking the summed distance of the observed value of y and the mean of y. What this gives us is the fraction of the variance explained by our model. In political science and the social sciences more generally this is something that most people don’t even report anymore. In many cases we are creating regression tables where we have different dependent variables. One of the primary weaknesses of using \\(R^{2}\\) is that the easiest way to improve your \\(R^2\\) is to simply add more variables. As the model gets more information it is is going to a better job of getting \\(\\hat{y}\\) close to the observed value of Y.\nAs a political scientist by training I am always skeptical of the idea of using \\(R^2\\) for anything other than chuckling at people who see it as some sort of measure of distance from the true model. I think King (1986) sums some of the criticisms quite well. Decisions about how we process our data are also going to influence this metric because we are going to either increase or decrease the distance. \\(R^2\\) in a machine learning context may fit the description of a goodness of fit test where we are figuring out the amount of variation in the data explained. We are often swapping out variables on the right hand side. \\(R^2\\) we can assess if some combination of these variables reduce the amount of unexplained variation. Generally we would probably want to use the penalized version of \\(R^2\\) so we don’t reward our model for just adding more variables. However, I think one of the most interesting points about \\(R^2\\) are things that are already present in our OLS model. This is going to hold less and less as our model gets less interpretable.\n\n\n\nHeuristically it seems a lot of machine learning evaluation for regression models uses mean squared error or some variation of it to check model performance. Conceptually these metrics are linked because we can get the \\(R^2\\) by doing:\n\\[\nR^2 = 1 - \\frac{x \\times MSE}{\\sum^n_{i =1} (y_i - \\hat{y})^2}\n\\]\nHowever, the thing with \\(R^2\\) is that it is not neccessarily that sensistive to extreme values. While in general it is difficult to have wildly divergent measure of \\(R^2\\) and MSE we may actually care about how our models perform in response to outliers. If big errors are costly to our model performance than we are going to want a measure that more directly captures their influence.\n\\[\nMSE = \\frac{1}{n} \\sum^n_{i =1} (y_i - \\hat{y})^2\n\\]\nWhere we are effectively getting the average error instead of the total error. One thing that flips with respect to \\(R^2\\) is that we are getting a slightly more interpretable difference metric. Meaning that we are getting the average distance between our predicted values and the observed values. So we are looking for this number to be kind of low! However, whats important to note is that there is no scaling factor. Meaning that outliers are going to really effect this metric because we are taking the difference and then going to square it. Lets imagine a situation where I am trying to increase the distance between a friend and the ball. If I stand right next to them and throw a ball it will land closer to them then if I started from farther away and threw the ball. Since we go through this squaring process what ends up happening is that we don’t have a great mental model of how to interpret this distance. What is the squared deviation of the median home value? Kind of not that intuitive because that distance is harder to put into words. To get it back on the original scale we can use the Root mean squared error which is just\n\\[\nRMSE = \\sqrt{MSE}\n\\]\nThis puts the MSE back on its original scale. If our model is predicting median home prices and the RMSE is 1000, this means that a typical prediction error is around 1000 dollars, with larger errors being weighted more heavily than smaller ones.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "linear-regression.html#bias-variance-tradeoff",
    "href": "linear-regression.html#bias-variance-tradeoff",
    "title": "Linear Regression and Shrinkage Estimators",
    "section": "",
    "text": "In the prior, our model could be good at predicting the data we have. However, when we introduce new data, it might struggle to generalize and predict new observations accurately. This challenge arises from overfitting, where the model is really good at describing the dataset it has already seen. To mitigate this we split our data into training sets, test sets, and validation sets effectively hiding parts of our data from our model. It never has access to every single part of the data. By reducing the amount of data the model sees it can’t learn every single strange data point in the model.\nThe reason we do this is because we want our model to predict new data but also do a good job of approximating the data generating process. These two goals are inherently conflictual. Bias represents how far away we are from the target while variance represents how far away our guesses are from each other. If we build a model that is good at predicting every single quirk of the dataset, aka reducing variance, then when we introduce new data to the model it is going to be very very brittle. If we reduce the complexity of the model making it more flexible, aka reducing bias, then we risk not being able to catch the patterns in our data.\nIn the bias-variance tradeoff, we aim to find a balance: a model that is simple enough to generalize well to new data but complex enough to capture the important patterns. Techniques like cross-validation, regularization, and hyperparameter tuning help us navigate this tradeoff and improve the model’s predictive performance.\nMathematically we can think of the bias-variance tradeoff along the lines of mean squared error where\n\\[\nError = Bias^2 + Variance + Noise\n\\]\nWhere we have the how far away the predictions are from the real values, how far away the predictions are from each other, and the randomness from the data generating process. We can’t really doing anything about the randomness from the DGP. A useful heuristic is that we can reduce how far away our predictions are from our real values by introducing more complexity to the model. This may mean more complex functional forms of variables, introduce interactions, or use more flexible models. We can increase variance by using simple models, use bagging or boosting to leverage “bad learners”, or use regularization to penalize our models.\n\n\n\n\nCode\n# code taken from https://www.r-bloggers.com/2021/07/double-descent-part-i-sample-wise-non-monotonicity/\n\nf &lt;- function(x){\n  (-0.4 + 1/(x+0.5)) + (0.5*exp(x))\n}\n#The point where the prediction error is minimized\noptimum &lt;- optimize(f, interval=c(0, 1), maximum=FALSE, tol = 1e-8)\ntemp_data &lt;- data.frame(x = optimum$minimum, y=optimum$objective)\n\nf1 &lt;- function(x){\n  ifelse(x&lt;=2,   (-0.4 + 1/(x+0.5)) + (0.5*exp(x)), NA)\n}\nf2 &lt;- function(x){\n  ifelse(x&gt;=2, (0 + 1/(1/(0.5*exp(4/x)))), NA)\n}\n#Prediction variance function (it is piecewise so creating two of them).\nvar_f1 &lt;- function(x){\n  ifelse(x&lt;=2,   (0.5*exp(x)), NA)\n}\nvar_f2 &lt;- function(x){\n  ifelse(x&gt;=2, 1/(1/(0.5*exp(4/x))), NA)\n}\n#Prediction bias function (it is piecewise so creating two of them).\nbias_f1 &lt;- function(x){\n  ifelse(x&lt;=2,-0.4 + 1/(x+0.5),NA )\n}\nbias_f2 &lt;- function(x){\n  ifelse(x&gt;=2,0,NA )\n}\nggplot(data = temp_data, aes(x=x, y=y)) +\n  xlim(0,4) +\n  geom_function(fun = var_f1, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = var_f2, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = bias_f1, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = bias_f2, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = f1, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_function(fun = f2, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  geom_point() + \n  theme_minimal() + ylab(\"Error\") + xlab(\"Number of Predictors/Number of observations\") +\n  theme(axis.text=element_blank(),\n        axis.ticks=element_blank()) +\n  annotate(\"text\", x=0.32, y=-0.2+1/(0.2+0.5), label= expression(paste(\"B\", ias^2)), color = \"blue\") +\n  annotate(\"text\", x=0.2, y=-0.2+0.5*exp(0.2), label= \"Variance\", color = \"red\") +\n  annotate(\"text\", x=0.26, y=0.21+(0.5*exp(0.2) + 1/(0.2+0.5)), label= expression(paste(\"Variance + B\", ias^2)), color = \"forestgreen\") +\n  annotate(\"text\", x=2.4, y=-0.2+1/(0.2+0.5), label= \"Interpolation limit\", color = \"black\") \n\n\n\n\n\n\n\n\n\nDouble descent is effectively just the idea that we don’t always see degrading performance in the test set when our model is really complex. Once we get past a certain threshold our model will start to perform well again.\nEffectively at some point the model will pass through every point in our dataset in multidimensional space. Effectively what is happening is that the model will memorize the training set because we let it be super complex and all we are doing is slightly modifying how we draw the line. Thus within some region we call the interpolation threshold our model is going to do poorly within this region. Once we get past this region our model’s performance will start to get better again and even outperform the ‘simple’ model again.\nWe can think of double descent as recasting our bias variance tradeoff a bit. Once we reach the interpolation threshold our model gains a somewhat crude approximation of the entirety of the data. In the bias variance framework think of bivariate linear regression. Its predictions are going to be pretty far off in some cases. A similar thing kind of happens in the interpolation region. The interpolations are crude but as we add more and more parameters the interpolations start to get better and better until the test error improves.\nThe reason this happens is that when we hit this interpolation region there is effectively only one model that can perfectly interpolate the data and when we add new data this will get thrown off. With to few parameters in the interpolation region we are not giving our model enough shots to be flexible enough to get this specification correct. By adding more and more parameters what we are doing is giving the model more chances to draw the line effectively.",
    "crumbs": [
      "Linear Regression and Shrinkage Estimators"
    ]
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Code\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport polars as pl \nimport polars.selectors as cs \nfrom sklearn.datasets import make_classification\n\nstocks = pl.read_csv('data/Smarket.csv')\n\n\nLets work through a somewhat contrived example. Lets say we wanted to predict the whether the stock market is going up or down. This is not neccessarily all that interesting but will be good practice. Lets visualize the data\n\ncorrs = stocks.select(~cs.string()).corr()\n\ncorrs \n\n\nshape: (8, 8)\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1.0\n0.0297\n0.030596\n0.033195\n0.035689\n0.029788\n0.539006\n0.030095\n\n\n0.0297\n1.0\n-0.026294\n-0.010803\n-0.002986\n-0.005675\n0.04091\n-0.026155\n\n\n0.030596\n-0.026294\n1.0\n-0.025897\n-0.010854\n-0.003558\n-0.043383\n-0.01025\n\n\n0.033195\n-0.010803\n-0.025897\n1.0\n-0.024051\n-0.018808\n-0.041824\n-0.002448\n\n\n0.035689\n-0.002986\n-0.010854\n-0.024051\n1.0\n-0.027084\n-0.048414\n-0.0069\n\n\n0.029788\n-0.005675\n-0.003558\n-0.018808\n-0.027084\n1.0\n-0.022002\n-0.03486\n\n\n0.539006\n0.04091\n-0.043383\n-0.041824\n-0.048414\n-0.022002\n1.0\n0.014592\n\n\n0.030095\n-0.026155\n-0.01025\n-0.002448\n-0.0069\n-0.03486\n0.014592\n1.0\n\n\n\n\n\n\nWe may also want to see some descriptives. A line plot would be nice but it would kind of hide alot so we are going to make a beeswarm plot\n\nyears_unique = stocks.unique(subset=\"Year\")[\"Year\"].to_list()\n\nyears_num = {year: i for i, year in enumerate(years_unique)}\n\nfor years in years_unique:\n    year_data = stocks.filter(pl.col(\"Year\") == years)\n    y = year_data[\"Volume\"].to_numpy()\n    x = np.random.normal(years_num[years], 0.1, len(y))\n\n    plt.scatter(x, y)\n\n\nplt.xticks(range(len(years_unique)), sorted(years_unique))\n\n([&lt;matplotlib.axis.XTick object at 0x149cf1b10&gt;, &lt;matplotlib.axis.XTick object at 0x14a3c3690&gt;, &lt;matplotlib.axis.XTick object at 0x15cb56a10&gt;, &lt;matplotlib.axis.XTick object at 0x15cb677d0&gt;, &lt;matplotlib.axis.XTick object at 0x15cb71250&gt;], [Text(0, 0, '2001'), Text(1, 0, '2002'), Text(2, 0, '2003'), Text(3, 0, '2004'), Text(4, 0, '2005')])\n\nplt.ylabel(\"Trading Volume\")\nplt.xlabel(\"Year\")\n\n\n\n\n\n\n\n\nWe are slowly but surely seeing an upwardsish trend in trading volume. There are definitely more graphs that we can make but for now we are going to move to the study portion.\n\n\nLogistic regression is probably your first classifier and is an incredibly important. They are a subfamily of Maximum Likelihood estimation. In a canned example lets say we have the a coin flip and we do it like a ton of times. We record each outcome where the probability is defined something like this.\n\\[\\begin{align}\nP(Heads) = \\frac{number of heads}{number of tosses} \\\\\nP(Tails) = 1 - P(Heads)\n\n\n\\end{align}\n\n\\]\nWe have some data and now we want to model it. We take the joint probablility as the function of some parameters aka the likelihood function. The value of theta that maxmimizes the likelihood function is called the maximum likelihood estimator.\nLogit is just a sub estimator of these estimators where we make the assumption that the DGP follows a binomial distribution and that the most appropriate link function is the logistic link which is just a technical sounding way of saying we are taking the log of \\(\\frac{p}{1-p}\\). This is really usefull because we bound the odds of an event happening between 0-1. The problem with MLE is that when underneath the hood we are logging the likelihood function because well prior to the invention of computer with this much power you would never be able to hand derive these things, but we still had data to model.\n\n\nLets take our stock market data and try to classify whether the direction is up or down using contemporaneous volumne and what appears to be lags of volume. Using the Stats model api we can do it like this\n\nstocks_pd = stocks.with_columns(\n    direction = pl.when(pl.col('Direction') == 'Up').then(1).otherwise(0)\n)\n\nform_version = smf.logit('direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume', data = stocks_pd.to_pandas()).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.691034\n         Iterations 4\n\nx = stocks_pd.drop(['Direction', 'direction', 'Year', 'Today']).to_numpy()\n\nx = sm.add_constant(x)\n\ny = stocks_pd['direction'].to_numpy()\n\nstats_version = sm.GLM(y, x, family = sm.families.Binomial()).fit()\n\nstats_version.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1250\n\n\nModel:\nGLM\nDf Residuals:\n1243\n\n\nModel Family:\nBinomial\nDf Model:\n6\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-863.79\n\n\nDate:\nWed, 26 Feb 2025\nDeviance:\n1727.6\n\n\nTime:\n16:23:38\nPearson chi2:\n1.25e+03\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.002868\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.1260\n0.241\n-0.523\n0.601\n-0.598\n0.346\n\n\nx1\n-0.0731\n0.050\n-1.457\n0.145\n-0.171\n0.025\n\n\nx2\n-0.0423\n0.050\n-0.845\n0.398\n-0.140\n0.056\n\n\nx3\n0.0111\n0.050\n0.222\n0.824\n-0.087\n0.109\n\n\nx4\n0.0094\n0.050\n0.187\n0.851\n-0.089\n0.107\n\n\nx5\n0.0103\n0.050\n0.208\n0.835\n-0.087\n0.107\n\n\nx6\n0.1354\n0.158\n0.855\n0.392\n-0.175\n0.446\n\n\n\n\nform_version.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\ndirection\nNo. Observations:\n1250\n\n\nModel:\nLogit\nDf Residuals:\n1243\n\n\nMethod:\nMLE\nDf Model:\n6\n\n\nDate:\nWed, 26 Feb 2025\nPseudo R-squ.:\n0.002074\n\n\nTime:\n16:23:38\nLog-Likelihood:\n-863.79\n\n\nconverged:\nTrue\nLL-Null:\n-865.59\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.7319\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.1260\n0.241\n-0.523\n0.601\n-0.598\n0.346\n\n\nLag1\n-0.0731\n0.050\n-1.457\n0.145\n-0.171\n0.025\n\n\nLag2\n-0.0423\n0.050\n-0.845\n0.398\n-0.140\n0.056\n\n\nLag3\n0.0111\n0.050\n0.222\n0.824\n-0.087\n0.109\n\n\nLag4\n0.0094\n0.050\n0.187\n0.851\n-0.089\n0.107\n\n\nLag5\n0.0103\n0.050\n0.208\n0.835\n-0.087\n0.107\n\n\nVolume\n0.1354\n0.158\n0.855\n0.392\n-0.175\n0.446\n\n\n\n\n\nTo assess our classifier we should go through several steps first we can look at the confusion matrix. Basically we can get as very general look at how well our predictions lineup with the actual data. First we need to get our predictions and then bin them into 1’s and zero\n\nform_preds = form_version.predict(stocks_pd.to_pandas())\n\ny_pred = (form_preds &gt;= 0.5).astype(int)\n\ny_actual = stocks_pd['direction'].to_pandas()\n\nconf_mat = confusion_matrix(y_actual, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n\ndisp.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x15cb4f090&gt;\n\n\n\n\n\n\n\n\n\nThis is a nice intuitive display. We are seeing what the model predicted versus what actually happens. When it comes to classification we have a variety of metrics.\n\\[\\begin{align}\n\\text{Accuracy} = \\frac{\\text{True Postive} + \\text{True Negative}}{TP + TN + FN + FP} \\\\\n\\text{Precision} = \\frac{TP}{TP + FP} \\\\\n\\text{Recall} = \\frac{TP}{TP + FN} \\\\\n\\text{F1 Score} = 2 \\times \\frac{precision \\times recall}{precision + recall}\n\n\\end{align}\\]\nEach of these metrics have a variety of benefits and tradeoffs.\n\nAccuracy\n\nAccuracy is nice and intuitive what proportion of correct predictions are we making? In a perfect world this is the only thing that we would use when evaluating models. However, if we have a lot of positives and very few negatives or vice versa our model is going to get good at predicting positives. But not that great at predicting negatives. When the class balance is bad enough we are going to get high accuracy because it is good at predicting the dominant class.\n\nPrecision and Recall\n\nPrecision is useful if we decompose what is in the denominator. If false positives are costly meaning that if flagging something as a positive would lead to not great outcomes we may need to maximize precision. However, we should always consider how it does with recall. Recall is the compliment to precision. In this case we are looking at the proportion of true positives and false negatives.\nIf we take the case of fraud and think about it like along these lines we want to strike a balance between the two maybe slightly favoring recall. While false positives are something we want to mininmize because it can cause frustrations and eat up company resources. Which isn’t good but significantly more costly. We don’t want to miss actual cases of fraud.\n\nF1 Score\n\nF1 score tries to strike this balance because maximizing precision or recall will lead the model to overcorrect. F1 score does a bit better with imbalanced datasets than accuracy the big drawback is we lose the interpretibility of whether the model is doing better minimizing false negatives or false positives.\n\n\n\n\nBrier Score: The Brier Score is closer in spirit to the mean squared error. So in effect we are talking about the average squared distance in probability from the true class label. This may seem kind of hard to get your head around since we are just classifying something as yes or no in a binary context or yes, no, or maybe in a multiclass context. Instead of taking the predicted class which will take on a value of 1 or 0 we are instead going to use the predicted probablity of the class. So the equation looks something like\n\n\\[\n\\text{Brier Score} = \\frac{1}{n} \\sigma^{n}_{t = 1} (\\text{Predicted Probablilty of class} - \\text{True Class Label})^2\n\\]\nSo much like the MSE a lower Brier score would indicate that the squared probability distance from the actual class on average. What this also means is that in the face of class imbalances it is going to bias the dominant class. If we think about the predicted probablity as simplistically as possible then the numerator is just the predicted frequency of events. If one class dominants the set then it is likely that they are also going to dominate the the predicted frequency.\n\nMatthew’s Correlation Coefficient\n\nThe MCC measures the correlation of predicted class labels with true class labels\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FN) \\times (TN + FN) }}\n\\]\nThe nice thing about the MCC is that it has the same interpretation as the correlation coefficient. So values closer to one indicate that there is a strong positive correlation between the predicted class and the observed class. A value closer to negative one indicates that their is a strong negative correlation. As holistic measure of how well our classifier is doing it is less sensitive to class imbalance. However, it does not neccessarily tell us how well it is doing with respect to telling us the proportion of false positives like precision does or false negatives like recall does.\n\n\n\n\nSome times employers will test you on things that aren’t just going through these. One real example you ran into is that you needed to hand calculate the True postives and the True negatives which you failed miserably and wasn’t able to even get to the rest.\nYou are working on a classification problem with two classes: Class 1 and Class 2. There are a total of 2000 observations in the dataset, with 1200 observations in Class 1 and 800 observations in Class 2. Your classifier produces the following predictions:\nIt assigns 1000 observations to Class 1. It assigns 800 observations to Class 2. Additionally, your classifier correctly classifies 1000 observations in total.\nUsing this information:\nHow many observations are true positives for Class 1 and Class 2? How many observations are false positives for each class?\nFor the True Postives we would do some\n\n# Define the known values\ncorrectly_classified &lt;- 1000\npredicted_class1 &lt;- 1000\npredicted_class2 &lt;- 800\nactual_class1 &lt;- 1200\nactual_class2 &lt;- 800\n\n\nTP2 &lt;- (predicted_class2 - actual_class1 + correctly_classified) / 2\n\n# Solve for TP1 using TP1 + TP2 = correctly_classified\nTP1 &lt;- correctly_classified - TP2\n\n# Solve for FP1 and FP2\nFP1 &lt;- predicted_class1 - TP1\nFP2 &lt;- predicted_class2 - TP2\n\nTP1 + TP2\n\n[1] 1000\n\n# Print the results\ncat(\"True Positives for Class 1 (TP1):\", TP1, \"\\n\")\n\nTrue Positives for Class 1 (TP1): 700 \n\ncat(\"True Positives for Class 2 (TP2):\", TP2, \"\\n\")\n\nTrue Positives for Class 2 (TP2): 300 \n\ncat(\"False Positives for Class 1 (FP1):\", FP1, \"\\n\")\n\nFalse Positives for Class 1 (FP1): 300 \n\ncat(\"False Positives for Class 2 (FP2):\", FP2, \"\\n\")\n\nFalse Positives for Class 2 (FP2): 500 \n\n\n\n\n\nIn the stock market example we can do something like this.\n\n\nx = stocks[['Lag1', 'Lag2']]\n\ny = stocks['Direction']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1994)\n\nThen we fit our models like this we initiate a logit object same as we would in tidy models same goes for various other. To pair this down we are going to just use two features. One thing to note is that scikit learn regularizs the logit by default with an l2 norm aka the ridge penalty.\n\n\n\nlogit = LogisticRegression()\n\nSo lets go ahead and fit the logit and see how it does\n\nlogit_mod = logit.fit(x_train, y_train)\n\nlogit_preds = logit_mod.predict(x_test)\n\n\naccuracy_score(y_test, logit_preds)\n\n0.508\n\nconfusion_matrix(y_test, logit_preds)\n\narray([[33, 83],\n       [40, 94]])\n\nclassification_report(y_test, logit_preds)\n\n'              precision    recall  f1-score   support\\n\\n        Down       0.45      0.28      0.35       116\\n          Up       0.53      0.70      0.60       134\\n\\n    accuracy                           0.51       250\\n   macro avg       0.49      0.49      0.48       250\\nweighted avg       0.49      0.51      0.49       250\\n'\n\n\nSo it has about a coin toss chance on being right. If we wanted to give it specific values we can give it some values\n\n\nnew_dat = pl.DataFrame({\n    'Lag1': [0.5, 0.8, 1.1],\n    'Lag2': [1.2, -0.5, 0.3],\n})\n\npredicted_probs = logit_mod.predict(new_dat)\n\n\n\n\nLDA is like a lot of these a dimensionality reduction machine. We make some assumptions one being that the classes are linearly separable hence the L in LDA. We also assume equal variance covariance matrices that follow a multivariate normal distribution. To check this we plot the matrices and they should look like an ellipsis.\n\nWhen classes are pretty close to perfectly seperable. This is because the MLE starts to break down. Even a firth correction may not be optimal.\nIf we have small sample size and the distribution of the predictors is approx normal.\n\n\nlda = LinearDiscriminantAnalysis(store_covariance=True)\n\nlda_mod = lda.fit(x_train, y_train)\n\nlda_preds = lda_mod.predict(x_test)\n\nconfusion_matrix(y_test, lda_preds)\n\narray([[33, 83],\n       [40, 94]])\n\n\nI will comeback to this but for the most part we are still doing a bad job of predicting the down direction. We are also seeing some bad things in the diagnostics.\n\n\n\nQDA is pretty similar to LDA in a lot of respects howevr it assumes that each class has its own mean and covariance rather than enforcing and equality assumptions\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\n\nqda_mod = qda.fit(x_train, y_train)\n\n\nqda_preds = qda_mod.predict(x_test)\n\n\nconfusion_matrix(y_test, qda_preds)\n\narray([[ 27,  89],\n       [ 33, 101]])\n\n\n\n\n\nNaive Bayes is a classic we make the assumption that our predictors are drawn from a gaussian distribution, that each of the features is conditionally independent, and we make the assumption that the classes are linearly seperable. What is interesting about Naive Bayes is that it works pretty well\n\nnb = GaussianNB()\n\nnb_mod = nb.fit(x_train, y_train)\n\n\nnb_preds = nb.predict(x_test)\n\nconfusion_matrix(y_test, nb_preds)\n\narray([[32, 84],\n       [36, 98]])\n\n\n\n\n\nFinally the most “machine-learny” of the models of these classifiers that we have covered so far is K-Nearest neightbors. KNN is fairly intuitive things that are close to each other are more likely to be related to each other. We don’t make any assumptions of the functional form of the decision boundary. For the most part each of the classifiers so far we make linearity assumptions or that the classification boundary follows a Bernoulli distribution. We also make no assumptions about the distribution of the data. This is kind of cool but as we make less and less assumptions about the data we start needing more of it. However we need to ensure that each of our features are on the same scale or the algorithm is not going to do well. If we have the difference in years versus 1,000 or millions of dollars. A jump of 100 yeasrs is substantively larger than a jump in a 100 dollars but K-nearest neighbors is going to let the larger numbers dominate. So we need to rescale everything.\nThe other thing is we don’t have any a priori knowledge of the optimal number of neighbors. We have have some idea but for machine learning models we use something called a hyperparameter to improve our model. There are mechancical parts of our models that we don’t have control over. In this setting we are not going to change how we calculate Euclidean distance. However, the number of neighbors to set that determines the classification boundaries are. Nothing in dataset or model can tell us what is the correct number of neighbors. We basically iterate over these to find the optimal k aka the optimal number of neighbors\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ncaravan = pl.read_csv('data/Caravan.csv')\n\nx_df = caravan.select(pl.exclude('Purchase'))\n\nscaler.fit(x_df)\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\nx_std = scaler.transform(x_df)\n\nfeature_sd = pl.DataFrame(x_std, schema=x_df.columns)\n\n\nx_train, x_test, y_train, y_test = train_test_split(np.asarray(feature_sd),\n                                                    caravan['Purchase'],\n                                                    test_size = 1000)\n\n\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn_preds = knn.fit(x_train, y_train).predict(x_test)\n\nknn.fit(x_train, y_train).predict_proba\n\n&lt;bound method KNeighborsClassifier.predict_proba of KNeighborsClassifier(n_neighbors=1)&gt;\n\nconfusion_matrix(y_test, knn_preds)\n\narray([[879,  61],\n       [ 56,   4]])\n\n\nSo one neighbor does pretty well but what if we could do better? We can perform a grid search over the number of neighors. 10 Neighbors is probably unreasonable. Since this is not actually all that intensive we could theoretically just use a for loop to tune this parameter. However, thats not really the best way since we have built in tools.\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors' : list(range(1,10))}\n\ngrid_search = GridSearchCV(knn, param_grid, cv = 5)\n\ngrid_search.fit(x_train, y_train)\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]}) best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=8) KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=8) \n\ngrid_search.best_score_\n\nnp.float64(0.9400666480338831)\n\ngrid_search.score(x_test, y_test)\n\n0.939\n\nbest_grid = grid_search.best_estimator_\n\n\nbest_grid_preds = best_grid.predict(x_test)\n\nconfusion_matrix(y_test, best_grid_preds)\n\narray([[939,   1],\n       [ 60,   0]])\n\nbest_grid.n_neighbors\n\n8\n\n\nThis is kind of nice. So lets breakdown what we did. We k-fold cross-validation meaning we created 5 evenly sized folds where the model will be trained on k-1 fold. Meaning we trained the model on 4 folds. Then repeat this process. In a grid search we are kind of just going through each individual combination of hyperparameters. So we are doing k =1 distance = manhattan, k =1 distance = euclidean etc. So this maybe fine if we don’t have a ton of things to do but if we have a ton of hyperparameters than that is not all that efficient.\n\nimport itertools\n\np_grid_2 = {'n_neighbors': list(range(1,5)), 'metric': list(['euclidean', 'manhattan', 'minkowski'])}\n\ncombos = itertools.product(p_grid_2['n_neighbors'], p_grid_2['metric'])\n\nfor comb in combos:\n    print(comb)\n\n(1, 'euclidean')\n(1, 'manhattan')\n(1, 'minkowski')\n(2, 'euclidean')\n(2, 'manhattan')\n(2, 'minkowski')\n(3, 'euclidean')\n(3, 'manhattan')\n(3, 'minkowski')\n(4, 'euclidean')\n(4, 'manhattan')\n(4, 'minkowski')\n\n\nWhereas random search is will take a random sample of these combos\n\nparam_dist = {'n_neighbors': np.arange(1,5), 'metric': ['euclidean', 'manhattan', 'minkowski']}\n\nrandom_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter = 4)\n\nrandom_search.fit(x_train, y_train)\n\nRandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={'metric': ['euclidean', 'manhattan',\n                                                   'minkowski'],\n                                        'n_neighbors': array([1, 2, 3, 4])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={'metric': ['euclidean', 'manhattan',\n                                                   'minkowski'],\n                                        'n_neighbors': array([1, 2, 3, 4])}) best_estimator_: KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=np.int64(4)) KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=np.int64(4)) \n\nbest_random = random_search.best_estimator_\n\nbest_preds_random = best_random.predict(x_test)\n\nbest_random.score(x_test, y_test)\n\n0.935\n\nconfusion_matrix(y_test, best_preds_random)\n\narray([[935,   5],\n       [ 60,   0]])\n\n\n\n\n\nOften times we do something akin to this where we train a bunch of models and then have to compare which one is the best. This would be a huge pain to do manually. However this is why computers are nice\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_curve, auc\n\nscaler = StandardScaler()\n\n\nx = stocks.select(pl.col('Lag1', 'Lag2', 'Volume')).to_numpy()\n\ny =  stocks.select(pl.col('Direction')).to_numpy().flatten()\n\nlabel_encoder = LabelEncoder()\n\ny_encoded = label_encoder.fit_transform(y)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y_encoded,\n                                                    test_size = 0.2)\n\n\nx_train_scaled = scaler.fit_transform(x_train)\n\nx_test_scaled = scaler.fit_transform(x_test)\n\nclassifiers = [\n    {'label':'QDA',  'model': QuadraticDiscriminantAnalysis()},\n    {'label':\"Logit\",'model': LogisticRegression()},\n    {'label':\"LDA\",  'model': LinearDiscriminantAnalysis()}, \n    {'label':\"KNN\",  'model': KNeighborsClassifier(n_neighbors=5)}\n]\n\n\nfor m in classifiers:\n    model = m['model']\n    model.fit(x_train_scaled, y_train)\n    pred = model.predict(x_test_scaled)\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test_scaled)[:,0])# direction down \n    plt.plot(fpr, tpr, label=f'{m[\"label\"]} ROC')\n\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\n\n(0.0, 1.0)\n\nplt.ylim([0.0, 1.05])\n\n(0.0, 1.05)\n\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()   \n\n\n\n\n\n\n\n\nSo we plotting ROC-AUC curves. Generally we want it to look a lot better than this. But this will give us a chance to talk about these. When it comes to evaluating models ROC-AUC curves are a favorite. These complement each other in a lot of respects.\nIn our accuracy, precision, recall, and F1 scores they all try to evaluate the proportion of true positives in comparision to either total classifications or something else. However, we don’t really ever have a good intuition at what level we should be cutting off these judgements. ROC-AUC lets us plot the performance of our models at various thresholds and against the random chance. In this case there are lots of instances whwer our model is not even as good as the random classifier. We probably need more features in order to improve our sensitivity.\nIn general we want it to look more curvy where the ROC-AUC is a lot closer to the 1 on the Y axis. A flat curve indicates that our model performs as well as flipping a coin. In any classification task we are going to have some mistakes in classification no matter the threshold. A AUC of 0.8 would indicate that our classifier is going to classify that point correctly close to 80% of the time.\n\n\n\n\nlibrary(tidyverse)\nlibrary(tinytable)\n\ndf = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv\") |&gt;\n    janitor::clean_names() \n\n df |&gt;\n    group_by(success, died) |&gt;\n    summarise(total = n()) |&gt;\n    mutate(prop = total/sum(total)) |&gt;\n                tt()\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                success\n                died\n                total\n                prop\n              \n        \n        \n        \n                \n                  FALSE\n                  FALSE\n                  46452\n                  0.981656805\n                \n                \n                  FALSE\n                   TRUE\n                    868\n                  0.018343195\n                \n                \n                   TRUE\n                  FALSE\n                  28961\n                  0.991849036\n                \n                \n                   TRUE\n                   TRUE\n                    238\n                  0.008150964\n                \n        \n      \n    \n\n\n\nSo for the most part nicely balanced class data is pretty rare! Lets take a fairly simple from Julia Silge’s excellent blog post. For the most part mountaneering trips don’t tend to have a lot of deaths which is good! However, when we want to start predicting what makes a succesful climb this can be a problem if we aren’t careful.\n\n\nSo we can think about fraud and war along similar lines in the class imbalance space. For the most part we have a ton of not war or not fraud cases in these kinds of dataset. I am sure if we start to skew the class imbalances a little bit more or even just added a third class like ‘maybe fraud’ or ‘mid’ to the COW database we are going to start pushing the limits of what the model could handle.\nOne of the real important things is choosing evaluation metrics that can handle imbalanced classes. As we saw in the last section accuracy tends to be a poor metric with big class imbalances becuase its the total number of correct predictions divided by the total number of predicitions. The thing about these models is that they tend to do pretty well in predicting things after they are trained on some data. So the model gets really good at predicting the dominant class but gets really bad at predicting the less dominant class.\nWe can pray that we get more data and just by pure chance we got a bad draw with lots of class imbalance. But, for the most part those prayers will never be answered. Fundamentally if the DGP of war or fraud change we have kind of a big problem on our hands. Instead what we can do is use as variety of resampling methods to artificially create balance between the two classes. In Table 1 I outline the broad strokes of each resampling method.\n\n\n\n\nTable 1\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Method\n                What it Does\n              \n        \n        \n        \n                \n                  Oversampling                                     \n                  Bias the classifier towards the minority class by duplicating the minority class                                                      \n                \n                \n                  Undersampling                                    \n                  Bias the classifier towards the minority class by removing examples of the dominant class                                             \n                \n                \n                  Random Oversampling Examples (ROSE)              \n                  Generate new synthetic points with some noise to the minority class                                                                   \n                \n                \n                  Synthetic Minority Oversampling Technique (SMOTE)\n                  Generates new synthetic points by interpolating between existing points                                                               \n                \n                \n                  Adaptive Synthetic Sampling (ADASYN)             \n                  Identify hard to classify examples meaning they do not have a ton of neighbors. Generate some synthetic points via K-nearest neighbors\n                \n        \n      \n    \n\n\n\n\n\n\nEffectively what each of these are doing from the perspective of the classifier because it imposes non-uniform missclassfication costs. The simplest approach is to simply randomly add data to the minority class or randomly delete data from the dominant class. However, these approaches have some obvious drawbacks. One thing that you will notice is that we have to manually set a ratio. What ratio do we set 🤷. We may want to shoot for equal balance but we have kind of fundamentaly changed the DGP which does not come at zero cost. By oversampling we are increasing our computational cost because we are copying our data and we risk over fitting. For undersampling we are chucking useful information away a lot of useful information that may improve predictive power down the line.\nTo combat this there are various procedures to use more data driven approaches to make up data or sample data. SMOTE generates synthetic examples by selecting a minority class instance, finding one of its k-nearest neighbors (from the same class), and creating a new point along the line segment between them. The synthetic point is placed at a random position between the two, ensuring that the newly generated data follows the distribution of the minority class via K-NN. SMOTE is viable strategy if the skew isn’t really bad. One of the problems is that while k-NN is kind of clever we have a hyperparameter to tune which can be computationally expensive. Since SMOTE is an oversampling technique some of the same draw backs of oversampling still apply.\nTo handle some of the weakpoints of SMOTE we have a family of synthetic data generators try to combat overfitting. A slightly modified version of the SMOTE framework is ADASYN which generates more and more synthetic data points near points closer to the decision boundary. Effectively what is happening is that we end up generating more and more points closer to the decision boundary with the goal of class balance. Another technique, ROSE which is based on bootstrap re-sampling techniques. Effectively what that means is that we are going to randomly draw a row from our dataset, setting the probability of the drawing the minority and the majority class to be the same. Then we are going to generate a synthetic example in the same neighborhood with a small amount of noise estimated via a kernel density estimate. We are going to keep doing this over and over again till we get a balanced dataset.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#logistic-regression",
    "href": "classification.html#logistic-regression",
    "title": "Classification",
    "section": "",
    "text": "Logistic regression is probably your first classifier and is an incredibly important. They are a subfamily of Maximum Likelihood estimation. In a canned example lets say we have the a coin flip and we do it like a ton of times. We record each outcome where the probability is defined something like this.\n\\[\\begin{align}\nP(Heads) = \\frac{number of heads}{number of tosses} \\\\\nP(Tails) = 1 - P(Heads)\n\n\n\\end{align}\n\n\\]\nWe have some data and now we want to model it. We take the joint probablility as the function of some parameters aka the likelihood function. The value of theta that maxmimizes the likelihood function is called the maximum likelihood estimator.\nLogit is just a sub estimator of these estimators where we make the assumption that the DGP follows a binomial distribution and that the most appropriate link function is the logistic link which is just a technical sounding way of saying we are taking the log of \\(\\frac{p}{1-p}\\). This is really usefull because we bound the odds of an event happening between 0-1. The problem with MLE is that when underneath the hood we are logging the likelihood function because well prior to the invention of computer with this much power you would never be able to hand derive these things, but we still had data to model.\n\n\nLets take our stock market data and try to classify whether the direction is up or down using contemporaneous volumne and what appears to be lags of volume. Using the Stats model api we can do it like this\n\nstocks_pd = stocks.with_columns(\n    direction = pl.when(pl.col('Direction') == 'Up').then(1).otherwise(0)\n)\n\nform_version = smf.logit('direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume', data = stocks_pd.to_pandas()).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.691034\n         Iterations 4\n\nx = stocks_pd.drop(['Direction', 'direction', 'Year', 'Today']).to_numpy()\n\nx = sm.add_constant(x)\n\ny = stocks_pd['direction'].to_numpy()\n\nstats_version = sm.GLM(y, x, family = sm.families.Binomial()).fit()\n\nstats_version.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1250\n\n\nModel:\nGLM\nDf Residuals:\n1243\n\n\nModel Family:\nBinomial\nDf Model:\n6\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-863.79\n\n\nDate:\nWed, 26 Feb 2025\nDeviance:\n1727.6\n\n\nTime:\n16:23:38\nPearson chi2:\n1.25e+03\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.002868\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.1260\n0.241\n-0.523\n0.601\n-0.598\n0.346\n\n\nx1\n-0.0731\n0.050\n-1.457\n0.145\n-0.171\n0.025\n\n\nx2\n-0.0423\n0.050\n-0.845\n0.398\n-0.140\n0.056\n\n\nx3\n0.0111\n0.050\n0.222\n0.824\n-0.087\n0.109\n\n\nx4\n0.0094\n0.050\n0.187\n0.851\n-0.089\n0.107\n\n\nx5\n0.0103\n0.050\n0.208\n0.835\n-0.087\n0.107\n\n\nx6\n0.1354\n0.158\n0.855\n0.392\n-0.175\n0.446\n\n\n\n\nform_version.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\ndirection\nNo. Observations:\n1250\n\n\nModel:\nLogit\nDf Residuals:\n1243\n\n\nMethod:\nMLE\nDf Model:\n6\n\n\nDate:\nWed, 26 Feb 2025\nPseudo R-squ.:\n0.002074\n\n\nTime:\n16:23:38\nLog-Likelihood:\n-863.79\n\n\nconverged:\nTrue\nLL-Null:\n-865.59\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.7319\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.1260\n0.241\n-0.523\n0.601\n-0.598\n0.346\n\n\nLag1\n-0.0731\n0.050\n-1.457\n0.145\n-0.171\n0.025\n\n\nLag2\n-0.0423\n0.050\n-0.845\n0.398\n-0.140\n0.056\n\n\nLag3\n0.0111\n0.050\n0.222\n0.824\n-0.087\n0.109\n\n\nLag4\n0.0094\n0.050\n0.187\n0.851\n-0.089\n0.107\n\n\nLag5\n0.0103\n0.050\n0.208\n0.835\n-0.087\n0.107\n\n\nVolume\n0.1354\n0.158\n0.855\n0.392\n-0.175\n0.446\n\n\n\n\n\nTo assess our classifier we should go through several steps first we can look at the confusion matrix. Basically we can get as very general look at how well our predictions lineup with the actual data. First we need to get our predictions and then bin them into 1’s and zero\n\nform_preds = form_version.predict(stocks_pd.to_pandas())\n\ny_pred = (form_preds &gt;= 0.5).astype(int)\n\ny_actual = stocks_pd['direction'].to_pandas()\n\nconf_mat = confusion_matrix(y_actual, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n\ndisp.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x15cb4f090&gt;\n\n\n\n\n\n\n\n\n\nThis is a nice intuitive display. We are seeing what the model predicted versus what actually happens. When it comes to classification we have a variety of metrics.\n\\[\\begin{align}\n\\text{Accuracy} = \\frac{\\text{True Postive} + \\text{True Negative}}{TP + TN + FN + FP} \\\\\n\\text{Precision} = \\frac{TP}{TP + FP} \\\\\n\\text{Recall} = \\frac{TP}{TP + FN} \\\\\n\\text{F1 Score} = 2 \\times \\frac{precision \\times recall}{precision + recall}\n\n\\end{align}\\]\nEach of these metrics have a variety of benefits and tradeoffs.\n\nAccuracy\n\nAccuracy is nice and intuitive what proportion of correct predictions are we making? In a perfect world this is the only thing that we would use when evaluating models. However, if we have a lot of positives and very few negatives or vice versa our model is going to get good at predicting positives. But not that great at predicting negatives. When the class balance is bad enough we are going to get high accuracy because it is good at predicting the dominant class.\n\nPrecision and Recall\n\nPrecision is useful if we decompose what is in the denominator. If false positives are costly meaning that if flagging something as a positive would lead to not great outcomes we may need to maximize precision. However, we should always consider how it does with recall. Recall is the compliment to precision. In this case we are looking at the proportion of true positives and false negatives.\nIf we take the case of fraud and think about it like along these lines we want to strike a balance between the two maybe slightly favoring recall. While false positives are something we want to mininmize because it can cause frustrations and eat up company resources. Which isn’t good but significantly more costly. We don’t want to miss actual cases of fraud.\n\nF1 Score\n\nF1 score tries to strike this balance because maximizing precision or recall will lead the model to overcorrect. F1 score does a bit better with imbalanced datasets than accuracy the big drawback is we lose the interpretibility of whether the model is doing better minimizing false negatives or false positives.\n\n\n\n\nBrier Score: The Brier Score is closer in spirit to the mean squared error. So in effect we are talking about the average squared distance in probability from the true class label. This may seem kind of hard to get your head around since we are just classifying something as yes or no in a binary context or yes, no, or maybe in a multiclass context. Instead of taking the predicted class which will take on a value of 1 or 0 we are instead going to use the predicted probablity of the class. So the equation looks something like\n\n\\[\n\\text{Brier Score} = \\frac{1}{n} \\sigma^{n}_{t = 1} (\\text{Predicted Probablilty of class} - \\text{True Class Label})^2\n\\]\nSo much like the MSE a lower Brier score would indicate that the squared probability distance from the actual class on average. What this also means is that in the face of class imbalances it is going to bias the dominant class. If we think about the predicted probablity as simplistically as possible then the numerator is just the predicted frequency of events. If one class dominants the set then it is likely that they are also going to dominate the the predicted frequency.\n\nMatthew’s Correlation Coefficient\n\nThe MCC measures the correlation of predicted class labels with true class labels\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FN) \\times (TN + FN) }}\n\\]\nThe nice thing about the MCC is that it has the same interpretation as the correlation coefficient. So values closer to one indicate that there is a strong positive correlation between the predicted class and the observed class. A value closer to negative one indicates that their is a strong negative correlation. As holistic measure of how well our classifier is doing it is less sensitive to class imbalance. However, it does not neccessarily tell us how well it is doing with respect to telling us the proportion of false positives like precision does or false negatives like recall does.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#test-questions",
    "href": "classification.html#test-questions",
    "title": "Classification",
    "section": "",
    "text": "Some times employers will test you on things that aren’t just going through these. One real example you ran into is that you needed to hand calculate the True postives and the True negatives which you failed miserably and wasn’t able to even get to the rest.\nYou are working on a classification problem with two classes: Class 1 and Class 2. There are a total of 2000 observations in the dataset, with 1200 observations in Class 1 and 800 observations in Class 2. Your classifier produces the following predictions:\nIt assigns 1000 observations to Class 1. It assigns 800 observations to Class 2. Additionally, your classifier correctly classifies 1000 observations in total.\nUsing this information:\nHow many observations are true positives for Class 1 and Class 2? How many observations are false positives for each class?\nFor the True Postives we would do some\n\n# Define the known values\ncorrectly_classified &lt;- 1000\npredicted_class1 &lt;- 1000\npredicted_class2 &lt;- 800\nactual_class1 &lt;- 1200\nactual_class2 &lt;- 800\n\n\nTP2 &lt;- (predicted_class2 - actual_class1 + correctly_classified) / 2\n\n# Solve for TP1 using TP1 + TP2 = correctly_classified\nTP1 &lt;- correctly_classified - TP2\n\n# Solve for FP1 and FP2\nFP1 &lt;- predicted_class1 - TP1\nFP2 &lt;- predicted_class2 - TP2\n\nTP1 + TP2\n\n[1] 1000\n\n# Print the results\ncat(\"True Positives for Class 1 (TP1):\", TP1, \"\\n\")\n\nTrue Positives for Class 1 (TP1): 700 \n\ncat(\"True Positives for Class 2 (TP2):\", TP2, \"\\n\")\n\nTrue Positives for Class 2 (TP2): 300 \n\ncat(\"False Positives for Class 1 (FP1):\", FP1, \"\\n\")\n\nFalse Positives for Class 1 (FP1): 300 \n\ncat(\"False Positives for Class 2 (FP2):\", FP2, \"\\n\")\n\nFalse Positives for Class 2 (FP2): 500",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#the-workflow",
    "href": "classification.html#the-workflow",
    "title": "Classification",
    "section": "",
    "text": "In the stock market example we can do something like this.\n\n\nx = stocks[['Lag1', 'Lag2']]\n\ny = stocks['Direction']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1994)\n\nThen we fit our models like this we initiate a logit object same as we would in tidy models same goes for various other. To pair this down we are going to just use two features. One thing to note is that scikit learn regularizs the logit by default with an l2 norm aka the ridge penalty.\n\n\n\nlogit = LogisticRegression()\n\nSo lets go ahead and fit the logit and see how it does\n\nlogit_mod = logit.fit(x_train, y_train)\n\nlogit_preds = logit_mod.predict(x_test)\n\n\naccuracy_score(y_test, logit_preds)\n\n0.508\n\nconfusion_matrix(y_test, logit_preds)\n\narray([[33, 83],\n       [40, 94]])\n\nclassification_report(y_test, logit_preds)\n\n'              precision    recall  f1-score   support\\n\\n        Down       0.45      0.28      0.35       116\\n          Up       0.53      0.70      0.60       134\\n\\n    accuracy                           0.51       250\\n   macro avg       0.49      0.49      0.48       250\\nweighted avg       0.49      0.51      0.49       250\\n'\n\n\nSo it has about a coin toss chance on being right. If we wanted to give it specific values we can give it some values\n\n\nnew_dat = pl.DataFrame({\n    'Lag1': [0.5, 0.8, 1.1],\n    'Lag2': [1.2, -0.5, 0.3],\n})\n\npredicted_probs = logit_mod.predict(new_dat)",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#lda",
    "href": "classification.html#lda",
    "title": "Classification",
    "section": "",
    "text": "LDA is like a lot of these a dimensionality reduction machine. We make some assumptions one being that the classes are linearly separable hence the L in LDA. We also assume equal variance covariance matrices that follow a multivariate normal distribution. To check this we plot the matrices and they should look like an ellipsis.\n\nWhen classes are pretty close to perfectly seperable. This is because the MLE starts to break down. Even a firth correction may not be optimal.\nIf we have small sample size and the distribution of the predictors is approx normal.\n\n\nlda = LinearDiscriminantAnalysis(store_covariance=True)\n\nlda_mod = lda.fit(x_train, y_train)\n\nlda_preds = lda_mod.predict(x_test)\n\nconfusion_matrix(y_test, lda_preds)\n\narray([[33, 83],\n       [40, 94]])\n\n\nI will comeback to this but for the most part we are still doing a bad job of predicting the down direction. We are also seeing some bad things in the diagnostics.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#qda",
    "href": "classification.html#qda",
    "title": "Classification",
    "section": "",
    "text": "QDA is pretty similar to LDA in a lot of respects howevr it assumes that each class has its own mean and covariance rather than enforcing and equality assumptions\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\n\nqda_mod = qda.fit(x_train, y_train)\n\n\nqda_preds = qda_mod.predict(x_test)\n\n\nconfusion_matrix(y_test, qda_preds)\n\narray([[ 27,  89],\n       [ 33, 101]])",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#naive-bayes",
    "href": "classification.html#naive-bayes",
    "title": "Classification",
    "section": "",
    "text": "Naive Bayes is a classic we make the assumption that our predictors are drawn from a gaussian distribution, that each of the features is conditionally independent, and we make the assumption that the classes are linearly seperable. What is interesting about Naive Bayes is that it works pretty well\n\nnb = GaussianNB()\n\nnb_mod = nb.fit(x_train, y_train)\n\n\nnb_preds = nb.predict(x_test)\n\nconfusion_matrix(y_test, nb_preds)\n\narray([[32, 84],\n       [36, 98]])",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#k-nearest-neighbors",
    "href": "classification.html#k-nearest-neighbors",
    "title": "Classification",
    "section": "",
    "text": "Finally the most “machine-learny” of the models of these classifiers that we have covered so far is K-Nearest neightbors. KNN is fairly intuitive things that are close to each other are more likely to be related to each other. We don’t make any assumptions of the functional form of the decision boundary. For the most part each of the classifiers so far we make linearity assumptions or that the classification boundary follows a Bernoulli distribution. We also make no assumptions about the distribution of the data. This is kind of cool but as we make less and less assumptions about the data we start needing more of it. However we need to ensure that each of our features are on the same scale or the algorithm is not going to do well. If we have the difference in years versus 1,000 or millions of dollars. A jump of 100 yeasrs is substantively larger than a jump in a 100 dollars but K-nearest neighbors is going to let the larger numbers dominate. So we need to rescale everything.\nThe other thing is we don’t have any a priori knowledge of the optimal number of neighbors. We have have some idea but for machine learning models we use something called a hyperparameter to improve our model. There are mechancical parts of our models that we don’t have control over. In this setting we are not going to change how we calculate Euclidean distance. However, the number of neighbors to set that determines the classification boundaries are. Nothing in dataset or model can tell us what is the correct number of neighbors. We basically iterate over these to find the optimal k aka the optimal number of neighbors\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ncaravan = pl.read_csv('data/Caravan.csv')\n\nx_df = caravan.select(pl.exclude('Purchase'))\n\nscaler.fit(x_df)\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\nx_std = scaler.transform(x_df)\n\nfeature_sd = pl.DataFrame(x_std, schema=x_df.columns)\n\n\nx_train, x_test, y_train, y_test = train_test_split(np.asarray(feature_sd),\n                                                    caravan['Purchase'],\n                                                    test_size = 1000)\n\n\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn_preds = knn.fit(x_train, y_train).predict(x_test)\n\nknn.fit(x_train, y_train).predict_proba\n\n&lt;bound method KNeighborsClassifier.predict_proba of KNeighborsClassifier(n_neighbors=1)&gt;\n\nconfusion_matrix(y_test, knn_preds)\n\narray([[879,  61],\n       [ 56,   4]])\n\n\nSo one neighbor does pretty well but what if we could do better? We can perform a grid search over the number of neighors. 10 Neighbors is probably unreasonable. Since this is not actually all that intensive we could theoretically just use a for loop to tune this parameter. However, thats not really the best way since we have built in tools.\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors' : list(range(1,10))}\n\ngrid_search = GridSearchCV(knn, param_grid, cv = 5)\n\ngrid_search.fit(x_train, y_train)\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]}) best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=8) KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=8) \n\ngrid_search.best_score_\n\nnp.float64(0.9400666480338831)\n\ngrid_search.score(x_test, y_test)\n\n0.939\n\nbest_grid = grid_search.best_estimator_\n\n\nbest_grid_preds = best_grid.predict(x_test)\n\nconfusion_matrix(y_test, best_grid_preds)\n\narray([[939,   1],\n       [ 60,   0]])\n\nbest_grid.n_neighbors\n\n8\n\n\nThis is kind of nice. So lets breakdown what we did. We k-fold cross-validation meaning we created 5 evenly sized folds where the model will be trained on k-1 fold. Meaning we trained the model on 4 folds. Then repeat this process. In a grid search we are kind of just going through each individual combination of hyperparameters. So we are doing k =1 distance = manhattan, k =1 distance = euclidean etc. So this maybe fine if we don’t have a ton of things to do but if we have a ton of hyperparameters than that is not all that efficient.\n\nimport itertools\n\np_grid_2 = {'n_neighbors': list(range(1,5)), 'metric': list(['euclidean', 'manhattan', 'minkowski'])}\n\ncombos = itertools.product(p_grid_2['n_neighbors'], p_grid_2['metric'])\n\nfor comb in combos:\n    print(comb)\n\n(1, 'euclidean')\n(1, 'manhattan')\n(1, 'minkowski')\n(2, 'euclidean')\n(2, 'manhattan')\n(2, 'minkowski')\n(3, 'euclidean')\n(3, 'manhattan')\n(3, 'minkowski')\n(4, 'euclidean')\n(4, 'manhattan')\n(4, 'minkowski')\n\n\nWhereas random search is will take a random sample of these combos\n\nparam_dist = {'n_neighbors': np.arange(1,5), 'metric': ['euclidean', 'manhattan', 'minkowski']}\n\nrandom_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter = 4)\n\nrandom_search.fit(x_train, y_train)\n\nRandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={'metric': ['euclidean', 'manhattan',\n                                                   'minkowski'],\n                                        'n_neighbors': array([1, 2, 3, 4])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={'metric': ['euclidean', 'manhattan',\n                                                   'minkowski'],\n                                        'n_neighbors': array([1, 2, 3, 4])}) best_estimator_: KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=np.int64(4)) KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=np.int64(4)) \n\nbest_random = random_search.best_estimator_\n\nbest_preds_random = best_random.predict(x_test)\n\nbest_random.score(x_test, y_test)\n\n0.935\n\nconfusion_matrix(y_test, best_preds_random)\n\narray([[935,   5],\n       [ 60,   0]])",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#selecting-the-best-classifier",
    "href": "classification.html#selecting-the-best-classifier",
    "title": "Classification",
    "section": "",
    "text": "Often times we do something akin to this where we train a bunch of models and then have to compare which one is the best. This would be a huge pain to do manually. However this is why computers are nice\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_curve, auc\n\nscaler = StandardScaler()\n\n\nx = stocks.select(pl.col('Lag1', 'Lag2', 'Volume')).to_numpy()\n\ny =  stocks.select(pl.col('Direction')).to_numpy().flatten()\n\nlabel_encoder = LabelEncoder()\n\ny_encoded = label_encoder.fit_transform(y)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y_encoded,\n                                                    test_size = 0.2)\n\n\nx_train_scaled = scaler.fit_transform(x_train)\n\nx_test_scaled = scaler.fit_transform(x_test)\n\nclassifiers = [\n    {'label':'QDA',  'model': QuadraticDiscriminantAnalysis()},\n    {'label':\"Logit\",'model': LogisticRegression()},\n    {'label':\"LDA\",  'model': LinearDiscriminantAnalysis()}, \n    {'label':\"KNN\",  'model': KNeighborsClassifier(n_neighbors=5)}\n]\n\n\nfor m in classifiers:\n    model = m['model']\n    model.fit(x_train_scaled, y_train)\n    pred = model.predict(x_test_scaled)\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test_scaled)[:,0])# direction down \n    plt.plot(fpr, tpr, label=f'{m[\"label\"]} ROC')\n\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\n\n(0.0, 1.0)\n\nplt.ylim([0.0, 1.05])\n\n(0.0, 1.05)\n\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()   \n\n\n\n\n\n\n\n\nSo we plotting ROC-AUC curves. Generally we want it to look a lot better than this. But this will give us a chance to talk about these. When it comes to evaluating models ROC-AUC curves are a favorite. These complement each other in a lot of respects.\nIn our accuracy, precision, recall, and F1 scores they all try to evaluate the proportion of true positives in comparision to either total classifications or something else. However, we don’t really ever have a good intuition at what level we should be cutting off these judgements. ROC-AUC lets us plot the performance of our models at various thresholds and against the random chance. In this case there are lots of instances whwer our model is not even as good as the random classifier. We probably need more features in order to improve our sensitivity.\nIn general we want it to look more curvy where the ROC-AUC is a lot closer to the 1 on the Y axis. A flat curve indicates that our model performs as well as flipping a coin. In any classification task we are going to have some mistakes in classification no matter the threshold. A AUC of 0.8 would indicate that our classifier is going to classify that point correctly close to 80% of the time.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "classification.html#what-should-we-do-if-we-have-some-class-imbalance",
    "href": "classification.html#what-should-we-do-if-we-have-some-class-imbalance",
    "title": "Classification",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tinytable)\n\ndf = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv\") |&gt;\n    janitor::clean_names() \n\n df |&gt;\n    group_by(success, died) |&gt;\n    summarise(total = n()) |&gt;\n    mutate(prop = total/sum(total)) |&gt;\n                tt()\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                success\n                died\n                total\n                prop\n              \n        \n        \n        \n                \n                  FALSE\n                  FALSE\n                  46452\n                  0.981656805\n                \n                \n                  FALSE\n                   TRUE\n                    868\n                  0.018343195\n                \n                \n                   TRUE\n                  FALSE\n                  28961\n                  0.991849036\n                \n                \n                   TRUE\n                   TRUE\n                    238\n                  0.008150964\n                \n        \n      \n    \n\n\n\nSo for the most part nicely balanced class data is pretty rare! Lets take a fairly simple from Julia Silge’s excellent blog post. For the most part mountaneering trips don’t tend to have a lot of deaths which is good! However, when we want to start predicting what makes a succesful climb this can be a problem if we aren’t careful.\n\n\nSo we can think about fraud and war along similar lines in the class imbalance space. For the most part we have a ton of not war or not fraud cases in these kinds of dataset. I am sure if we start to skew the class imbalances a little bit more or even just added a third class like ‘maybe fraud’ or ‘mid’ to the COW database we are going to start pushing the limits of what the model could handle.\nOne of the real important things is choosing evaluation metrics that can handle imbalanced classes. As we saw in the last section accuracy tends to be a poor metric with big class imbalances becuase its the total number of correct predictions divided by the total number of predicitions. The thing about these models is that they tend to do pretty well in predicting things after they are trained on some data. So the model gets really good at predicting the dominant class but gets really bad at predicting the less dominant class.\nWe can pray that we get more data and just by pure chance we got a bad draw with lots of class imbalance. But, for the most part those prayers will never be answered. Fundamentally if the DGP of war or fraud change we have kind of a big problem on our hands. Instead what we can do is use as variety of resampling methods to artificially create balance between the two classes. In Table 1 I outline the broad strokes of each resampling method.\n\n\n\n\nTable 1\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Method\n                What it Does\n              \n        \n        \n        \n                \n                  Oversampling                                     \n                  Bias the classifier towards the minority class by duplicating the minority class                                                      \n                \n                \n                  Undersampling                                    \n                  Bias the classifier towards the minority class by removing examples of the dominant class                                             \n                \n                \n                  Random Oversampling Examples (ROSE)              \n                  Generate new synthetic points with some noise to the minority class                                                                   \n                \n                \n                  Synthetic Minority Oversampling Technique (SMOTE)\n                  Generates new synthetic points by interpolating between existing points                                                               \n                \n                \n                  Adaptive Synthetic Sampling (ADASYN)             \n                  Identify hard to classify examples meaning they do not have a ton of neighbors. Generate some synthetic points via K-nearest neighbors\n                \n        \n      \n    \n\n\n\n\n\n\nEffectively what each of these are doing from the perspective of the classifier because it imposes non-uniform missclassfication costs. The simplest approach is to simply randomly add data to the minority class or randomly delete data from the dominant class. However, these approaches have some obvious drawbacks. One thing that you will notice is that we have to manually set a ratio. What ratio do we set 🤷. We may want to shoot for equal balance but we have kind of fundamentaly changed the DGP which does not come at zero cost. By oversampling we are increasing our computational cost because we are copying our data and we risk over fitting. For undersampling we are chucking useful information away a lot of useful information that may improve predictive power down the line.\nTo combat this there are various procedures to use more data driven approaches to make up data or sample data. SMOTE generates synthetic examples by selecting a minority class instance, finding one of its k-nearest neighbors (from the same class), and creating a new point along the line segment between them. The synthetic point is placed at a random position between the two, ensuring that the newly generated data follows the distribution of the minority class via K-NN. SMOTE is viable strategy if the skew isn’t really bad. One of the problems is that while k-NN is kind of clever we have a hyperparameter to tune which can be computationally expensive. Since SMOTE is an oversampling technique some of the same draw backs of oversampling still apply.\nTo handle some of the weakpoints of SMOTE we have a family of synthetic data generators try to combat overfitting. A slightly modified version of the SMOTE framework is ADASYN which generates more and more synthetic data points near points closer to the decision boundary. Effectively what is happening is that we end up generating more and more points closer to the decision boundary with the goal of class balance. Another technique, ROSE which is based on bootstrap re-sampling techniques. Effectively what that means is that we are going to randomly draw a row from our dataset, setting the probability of the drawing the minority and the majority class to be the same. Then we are going to generate a synthetic example in the same neighborhood with a small amount of noise estimated via a kernel density estimate. We are going to keep doing this over and over again till we get a balanced dataset.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "SQL",
    "section": "",
    "text": "In this chapter we will briefly go over the basics of SQL and also do some SQL practice since I have a very limited way to practice.\nSince I am going on the non-academic job market it is high time I learned SQL. I have tried lots of amazing resources but find it hard for me to navigate between notes and various and learning SQL since they are just familiar enough to trip me up and lots of them send you off to various editors. This blog post will serve as my notes and hopefully as a resource for not myself. The general idea is I am just going to work through R4DS and the various dplyr verbs. Then move onto some more advanced SQL stuff like window functions and what not.\n\n\nFor the majority of this palmerpenguins dataset not because you really need to use SQL for a dataset this small but copying over the nyc-taxi dataset is incredibly annoying for blogging purposes.\n\nlibrary(DBI)\nlibrary(arrow)\nlibrary(tinytable)\nlibrary(dbplyr)\nlibrary(tidyverse)\n\npengs = palmerpenguins::penguins\n\ncon =  src_memdb()\n\npengs = copy_to(con, pengs,\n       overwrite = TRUE)\n\nWe are going to go back and forth using dbplyr and SQL to query the dataset. What impressed me throughout this process was how seamless dbplyr works with dplyr verbs work. With the exception of some string functions it can work as a drop in replacement for SQL. What really helped throughout this process was writing out my queries and using show_query.\n\npengs |&gt;\n    select(species) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT `species`\nFROM `pengs`\n\n\nWhich will give us a SQL query. Obviously this is a pretty simple query but as we get more and more complex this is going to be helpful. For the most part show_query outputs the right query but can be a little bit difficult to debug because of the limitations of showing things in the R console.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#setup",
    "href": "SQL.html#setup",
    "title": "SQL",
    "section": "",
    "text": "For the majority of this palmerpenguins dataset not because you really need to use SQL for a dataset this small but copying over the nyc-taxi dataset is incredibly annoying for blogging purposes.\n\nlibrary(DBI)\nlibrary(arrow)\nlibrary(tinytable)\nlibrary(dbplyr)\nlibrary(tidyverse)\n\npengs = palmerpenguins::penguins\n\ncon =  src_memdb()\n\npengs = copy_to(con, pengs,\n       overwrite = TRUE)\n\nWe are going to go back and forth using dbplyr and SQL to query the dataset. What impressed me throughout this process was how seamless dbplyr works with dplyr verbs work. With the exception of some string functions it can work as a drop in replacement for SQL. What really helped throughout this process was writing out my queries and using show_query.\n\npengs |&gt;\n    select(species) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT `species`\nFROM `pengs`\n\n\nWhich will give us a SQL query. Obviously this is a pretty simple query but as we get more and more complex this is going to be helpful. For the most part show_query outputs the right query but can be a little bit difficult to debug because of the limitations of showing things in the R console.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#select",
    "href": "SQL.html#select",
    "title": "SQL",
    "section": "Select",
    "text": "Select\nOne convention in SQL which I don’t really get but is a thing is that functions are defined using all caps. Luckily for us the SQL and dplyr versions are pretty much the same one is just shouty. If we wanted all the columns like we may when we are importing the dataset for the first time we are just going to do SELECT * FROM taxis. There is really not like a perfect equivalent in R except for maybe head. But even then it is not a perfect one to one.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#r",
    "href": "SQL.html#r",
    "title": "SQL",
    "section": "R",
    "text": "R\n\nhead(pengs)\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#sql-1",
    "href": "SQL.html#sql-1",
    "title": "SQL",
    "section": "SQL",
    "text": "SQL\n\ntbl(con, sql(\"SELECT * FROM pengs\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#filter",
    "href": "SQL.html#filter",
    "title": "SQL",
    "section": "Filter",
    "text": "Filter\nThe first major difference syntactically between dplyr and SQL is with filter statements aka WHERE statements in SQL. So let’s say we want only penguins that are Adelie penguins.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species == 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nBecomes.\n\n\n\ntbl(con,sql( \"\n    SELECT * from pengs\n    WHERE species = 'Adelie'\n\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\nSome flavors of SQL make you end lines with ‘;’\nAs dplyr users will notice the way we specified the equality position uses the = instead of ==. This is going to come up a lot. The same thing goes for negation operations.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species != 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\ntbl(con, sql(\"SELECT * from pengs \n             WHERE NOT species = 'Adelie'\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nIf we want multiple conditions in our where statements instead of | or &/, we actually just use the words or and and\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species == 'Chinstrap' | species == 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nbecomes\n\n\n\ntbl(con, sql(\"SELECT * from pengs \n            WHERE species = 'Adelie' OR species = 'Chinstrap'\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nYou could easily sub in AND but that feels a bit excessive to continue this process for each possible combination. One thing that I do all the time is use sets to subset my data.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species %in% c('Chinstrap', \"Gentoo\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nBecomes\n\n\n\ntbl(con, sql(\"SELECT * from pengs\n            WHERE species IN ('Chinstrap', 'Gentoo')\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nin this case we define a set in a similar way. If we wanted to negate this statement all we would do is\n\ntbl(con, sql(\"SELECT * from pengs\n            WHERE NOT species IN ('Chinstrap', 'Gentoo')\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nLets say we want to find penguins that are less than the average body mass in R this is fairly straightforward\n\npengs |&gt;\n    filter(body_mass_g &lt; mean(body_mass_g, na.rm = TRUE))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nHowever when we do this in some flavor of SQL it is not as straightforward. These are aggregation functions that where can’t handle because thats not its job. So if we did\n\ntbl(con, \"SELECT * from pengs WHERE body_mass_g &lt; AVG(body_mass_g)\")\n\nError in `db_query_fields.DBIConnection()`:\n! Can't query fields.\nℹ Using SQL: SELECT * FROM `SELECT * from pengs WHERE body_mass_g &lt;\n  AVG(body_mass_g)` AS `q01` WHERE (0 = 1)\nCaused by error:\n! no such table: SELECT * from pengs WHERE body_mass_g &lt; AVG(body_mass_g)\n\n\nWe get an error. If we wanted to use aggregation functions we have to change how we do this\n\npengs |&gt;\n    filter(body_mass_g &lt; mean(body_mass_g, na.rm = TRUE)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `island`,\n  `bill_length_mm`,\n  `bill_depth_mm`,\n  `flipper_length_mm`,\n  `body_mass_g`,\n  `sex`,\n  `year`\nFROM (\n  SELECT `pengs`.*, AVG(`body_mass_g`) OVER () AS `col01`\n  FROM `pengs`\n) AS `q01`\nWHERE (`body_mass_g` &lt; `col01`)\n\n\nWhat is this OVER thing? OVER in SQL is a window function. There is a more technical way to explain this but heuristically when we pass AVG to WHERE we are effectively doing this. So there is not really anything to compare it too.\n\npengs |&gt;\n    summarise(mean(body_mass_g, na.rm = TRUE))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.47.1 [:memory:]\n  `mean(body_mass_g, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             4202.\n\n\nIf we wanted to filter penguins that are less than the average body mass we have to prevent this aggregation process by creating a column and then creating a less than statement like this\n\ntbl(con, sql(\"SELECT * FROM(\n              SELECT pengs .*, AVG(body_mass_g) OVER () AS avg\n               FROM pengs)\n              WHERE (body_mass_g &lt; avg)\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg &lt;dbl&gt;\n\n\nIt is a little clunky but the tl;dr is that we basically have two FROM statements so if we wanted all penguins between the minimum and the average we could do\n\nRSQL\n\n\n\npalmerpenguins::penguins |&gt;\n    filter(between(body_mass_g, left = min(body_mass_g, na.rm = TRUE), right = mean(body_mass_g, na.rm = TRUE)))\n\n# A tibble: 193 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ 183 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\ntbl(con, sql(\"SELECT * FROM(\n             SELECT pengs .*, AVG(body_mass_g) OVER() AS avg, MIN(body_mass_g) OVER() AS min\n            FROM pengs)\n            WHERE body_mass_g BETWEEN min AND avg\"))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg &lt;dbl&gt;, min &lt;int&gt;\n\n\n\n\n\nIf you notice in all our examples, we have lots and lots of missing values. This is one of the most common tasks in like any data science task. Let’s say that we can safely ignore the missing valus. In R we have a lot of options whether we are using filter or drop_na from tidyr. However, in SQL missing values are usually represented by NULL\n\ntbl(con, sql(\"SELECt * FROM pengs \n                WHERE NOT sex IS NULL\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#rename",
    "href": "SQL.html#rename",
    "title": "SQL",
    "section": "Rename",
    "text": "Rename\nThe AS function is kind the work horse for the next few sections. The naming convention differs a little bit so instead of new_name = old_name we do SELECT old_name as new_name\n\ntbl(con, sql(\"SELECT species AS kinds_of_penguins\n          FROM pengs\"))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.47.1 [:memory:]\n   kinds_of_penguins\n   &lt;chr&gt;            \n 1 Adelie           \n 2 Adelie           \n 3 Adelie           \n 4 Adelie           \n 5 Adelie           \n 6 Adelie           \n 7 Adelie           \n 8 Adelie           \n 9 Adelie           \n10 Adelie           \n# ℹ more rows",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#mutate",
    "href": "SQL.html#mutate",
    "title": "SQL",
    "section": "Mutate",
    "text": "Mutate\nAs lots of things go we need to be able to create our own variables. So to do this in R we do this\n\npengs |&gt;\n    mutate(sqr_body_mass = body_mass_g^2)\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, sqr_body_mass &lt;dbl&gt;\n\n\nIn SQL to get the equivalent statement we use SELECT transformation AS new_var_name when we need to do things that are not in the dataset. So we basically need to define the column before we do anything.\n\ntbl(con, sql(\"SELECT pengs .*, POWER(body_mass_g,2) AS sqr_body_mass\n            FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, sqr_body_mass &lt;dbl&gt;\n\n\nSo if we needed wanted to make a ratio of bill depth to bill length we would do\n\ntbl(con, sql(\"SELECT pengs .*, bill_depth_mm/bill_length_mm AS ratio \n            FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, ratio &lt;dbl&gt;\n\n\nA very important thing we do all the time is generate indicator variables for treatment status gender etc. Oddly enough if we peep the output of show query we see a familiar face!",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#r-7",
    "href": "SQL.html#r-7",
    "title": "SQL",
    "section": "R",
    "text": "R\n\npengs |&gt;\n    mutate(male = ifelse(sex == 'Male', 1, 0)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `pengs`.*,\n  CASE WHEN (`sex` = 'Male') THEN 1.0 WHEN NOT (`sex` = 'Male') THEN 0.0 END AS `male`\nFROM `pengs`\n\n\nSo to make an indicator variable we would just do\n\ntbl(con, sql(\"SELECT pengs.*, CASE WHEN (sex = 'male') THEN 1.0 WHEN not (sex = 'male') THEN 0.0 END AS male\n             FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, male &lt;dbl&gt;\n\n\nLet’s combine our window functions with our friend case_when\n\ntbl(con, sql(\"SELECT * FROM(SELECT pengs .*,\n           AVG(body_mass_g) AS avg, MIN(body_mass_g) AS min, MAX(body_mass_g) AS max,\n            CASE WHEN (body_mass_g = min) THEN 'This penguins is small' WHEN (body_mass_g = avg) THEN 'This is an average sized penguin' WHEN (body_mass_g = max) THEN 'this is a really big penguin' END AS note \n            FROM pengs)\"))\n\nI will spare you the long output of the error message. But needless to say this was wrong. If we translate what I was trying to do into dplyr we get this\n\npengs |&gt;\n    mutate(note = case_when(\n            body_mass_g == min(body_mass_g) ~ 'This is a small peng',\n            body_mass_g == mean(body_mass_g) ~ 'Average sized peng',\n            body_mass_g == max(body_mass_g) ~ 'Big sized peng',\n             .default = 'Penguin is some size')) |&gt;\n        show_query()\n\n&lt;SQL&gt;\nSELECT\n  `pengs`.*,\n  CASE\nWHEN (`body_mass_g` = MIN(`body_mass_g`) OVER `win1`) THEN 'This is a small peng'\nWHEN (`body_mass_g` = AVG(`body_mass_g`) OVER `win1`) THEN 'Average sized peng'\nWHEN (`body_mass_g` = MAX(`body_mass_g`) OVER `win1`) THEN 'Big sized peng'\nELSE 'Penguin is some size'\nEND AS `note`\nFROM `pengs`\nWINDOW `win1` AS ()\n\n\nSo it looks like we need to change the window function\n\ncheck = tbl(con, sql(\"SELECT pengs .*,\n              CASE\n            WHEN (body_mass_g &gt;= MIN(body_mass_g) OVER win1) THEN 'this is a small penguin'\n            WHEN (body_mass_g = AVG(body_mass_g) OVER win1) THEN 'this is an average sized penguin'\n            WHEN (body_mass_g = MAX(body_mass_g) OVER win1) THEN 'this is a big penguin'\n            ELSE 'This penguin is not big, small or average'\n            END AS note\n            FROM pengs \n            WINDOW win1 AS ()\")) |&gt;\n                collect()\n\nLets look at this a little closer to make sure this worked. We would probably want to make this a little more robust. So lets go ahead and define a range.\n\ntbl(con, sql(\"SELECT pengs .*,\n              CASE\n            WHEN (body_mass_g &gt;= MIN(body_mass_g) OR body_mass_g &lt; AVG(body_mass_g)  OVER win1) THEN 'this is a small penguin'\n            WHEN (body_mass_g &gt;= AVG(body_mass_g) OR body_mass_g &lt; MAX(body_mass_G) OVER win1) THEN 'this is an average sized penguin'\n            WHEN (body_mass_g &gt;= MAX(body_mass_g) OVER win1) THEN 'this is a big penguin'\n            ELSE 'This penguin is not big, small or average'\n            END AS note\n            FROM pengs \n            WINDOW win1 AS ()\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            46.9          16.6               192        2700\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, note &lt;chr&gt;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#group-by-and-summarize",
    "href": "SQL.html#group-by-and-summarize",
    "title": "SQL",
    "section": "Group by and summarize",
    "text": "Group by and summarize\nAs established earlier we can use SQL to summarize like this.\n\ntbl(con, sql('SELECT AVG(bill_depth_mm) AS avg\n           FROM pengs'))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.47.1 [:memory:]\n    avg\n  &lt;dbl&gt;\n1  17.2\n\n\nBut the actual practical utility is somewhat limited. Often we want group specific differences. Oddly enough I expected this to be a window function thing, but we actually delay computing of the mean by different groups to the end. I guess this makes sense if we are dealing with big data\n\ntbl(con, sql(\"SELECT species, AVG(body_mass_g) AS avg_body_mass\n            FROM pengs\n            GROUP BY species\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  species   avg_body_mass\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Adelie            3701.\n2 Chinstrap         3733.\n3 Gentoo            5076.\n\n\nSo if we wanted to count of the species we would do something along this line\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total\n            FROM pengs \n            GROUP BY species\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  species   total\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nFor multiple grouping variables we would define the grouping variables the same way as we would in dplyr\n\ntbl(con, sql(\"SELECT species, sex, COUNT(species) AS total\n            FROM pengs \n            GROUP BY species, sex\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.47.1 [:memory:]\n  species   sex    total\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    &lt;NA&gt;       6\n2 Adelie    female    73\n3 Adelie    male      73\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    &lt;NA&gt;       5\n7 Gentoo    female    58\n8 Gentoo    male      61\n\n\nThe same would go for multiple summary functions\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total, AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS median_bill_depth\n             FROM pengs \n             GROUP BY sex\"))\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.47.1 [:memory:]\n  species total avg_bill_depth median_bill_depth\n  &lt;chr&gt;   &lt;int&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie     11           16.6              17.1\n2 Adelie    165           16.4              17  \n3 Adelie    168           17.9              18.4\n\n\nAfter aggregation we may only be interesed in a certain value in this aggregated set. However, if we do this we will get an error because WHERE will only look for rows in unaggregated data. Effectively SQL splits filter into a grouped and ungrouped version.\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total, AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS median_bill_depth\n             FROM pengs \n             GROUP BY sex\n             WHERE avg_bill_depth &gt; 600\"))\n\nError in `db_query_fields.DBIConnection()`:\n! Can't query fields.\nℹ Using SQL: SELECT * FROM ( SELECT species, COUNT(species) AS total,\n  AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS\n  median_bill_depth FROM pengs GROUP BY sex WHERE avg_bill_depth &gt; 600 ) AS\n  `q01` WHERE (0 = 1)\nCaused by error:\n! near \"WHERE\": syntax error\n\n\nSo if we wanted to filter a grouped data frame we have to use HAVING\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total, AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS median_bill_depth\n             FROM pengs \n             GROUP BY sex\n             HAVING avg_bill_depth &gt; 10\")) |&gt;\n                collect()\n\n# A tibble: 3 × 4\n  species total avg_bill_depth median_bill_depth\n  &lt;chr&gt;   &lt;int&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie     11           16.6              17.1\n2 Adelie    165           16.4              17  \n3 Adelie    168           17.9              18.4",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#joinsappending-rows",
    "href": "SQL.html#joinsappending-rows",
    "title": "SQL",
    "section": "Joins/Appending Rows",
    "text": "Joins/Appending Rows\nIn the real world it is rare that we will have all our data in one place. Companies keep information in lots of different places because well it would be bad if we kept credit card information with all the necessary components to make a purchase. Instead of having to figure out three different things malicious actors would just need to access one database. Replacing entire data tables can also skyrocket costs. So instead, it is more efficient to simply insert rows.\n\nApppending Rows\nTo kind of mimic this we are just going to slice this data frame roughly in half. While not entirely realistic the general process will be similar enough\n\n\nCode\npengs_top = palmerpenguins::penguins |&gt;\n    slice(1:172)\n\npengs_bottom = palmerpenguins::penguins |&gt;\n    slice(173:344)\n\ncon2 = src_memdb()\n\ncon3 = src_memdb()\n\npengs_top = copy_to(con2, pengs_top)\n\npengs_bottom = copy_to(con3, pengs_bottom)\n\n\nFor whatever reason show_query is not working with this so we are going to have to consult the interwebs. The SQL equivalent of bind_rows is UNION.\n\ntbl(con2, sql(\"SELECT * FROM pengs_top\n             UNION ALL \n             SELECT * FROM pengs_bottom\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nOne of the key things in this query is ALL which is somewhat new to me. Basically the ALL tells SQL that we don’t really care about duplicates so just add the rows regardless. So if we wanted to exclude duplicates we would do something like this\n\n\nCode\ntbl(con2, sql(\"SELECt * FROM pengs_top \n              UNION \n              SELECT * FROM pengs_top\")) |&gt;\n                collect() |&gt;\n                nrow()\n\n\n[1] 172\n\n\nCode\ntbl(con2,sql(\"SELECT * FROM pengs_top\") ) |&gt;\n    collect() |&gt;\n    nrow()\n\n\n[1] 172\n\n\n\n\nJoins\nLuckily for us the join syntax from dplyr is pretty directly taken SQL so lefts create some dummy data to join.\n\n\nCode\nnational_data &lt;- tribble(\n  ~state, ~year, ~unemployment, ~inflation, ~population,\n  \"GA\",   2018,  5,             2,          100,\n  \"GA\",   2019,  5.3,           1.8,        200,\n  \"GA\",   2020,  5.2,           2.5,        300,\n  \"NC\",   2018,  6.1,           1.8,        350,\n  \"NC\",   2019,  5.9,           1.6,        375,\n  \"NC\",   2020,  5.3,           1.8,        400,\n  \"CO\",   2018,  4.7,           2.7,        200,\n  \"CO\",   2019,  4.4,           2.6,        300,\n  \"CO\",   2020,  5.1,           2.5,        400\n)\n\nnational_libraries &lt;- tribble(\n  ~state, ~year, ~libraries, ~schools,\n  \"CO\",   2018,  230,        470,\n  \"CO\",   2019,  240,        440,\n  \"CO\",   2020,  270,        510,\n  \"NC\",   2018,  200,        610,\n  \"NC\",   2019,  210,        590,\n  \"NC\",   2020,  220,        530,\n)\n\ncon3 = src_memdb()\n\ncon4 = src_memdb()\n\nnational_data = copy_to(con4, national_data, overwrite = TRUE)\n\nnational_libraries = copy_to(con3, national_libraries, overwrite = TRUE)\n\n\nSo we have some fake national level data that we would like to join in to the dataset. We could do something like this but what we notice is that it is going to decide the join keys for us and probably create some headaches for us later on. To solve this we need to use our keys if we expose the underlying logic\n\nnational_data |&gt;\n    left_join(national_libraries, join_by(state, year)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT `national_data`.*, `libraries`, `schools`\nFROM `national_data`\nLEFT JOIN `national_libraries`\n  ON (\n    `national_data`.`state` = `national_libraries`.`state` AND\n    `national_data`.`year` = `national_libraries`.`year`\n  )\n\n\nWe will notice that join_by is shorthand for equality joins. What changes is that instead of left_key = right_key we have to specify what is coming from what table using .\n\ndb_con = con4$con\n\nquery = \"SELECT *\n             FROM national_data\n             LEFT JOIN national_libraries\n             ON (\n             national_data.state = national_libraries.state AND\n             national_data.year = national_libraries.year\n             )\n             \"\n\ndbGetQuery(db_con, sql(query))       \n\n  state year unemployment inflation population state year libraries schools\n1    GA 2018          5.0       2.0        100  &lt;NA&gt;   NA        NA      NA\n2    GA 2019          5.3       1.8        200  &lt;NA&gt;   NA        NA      NA\n3    GA 2020          5.2       2.5        300  &lt;NA&gt;   NA        NA      NA\n4    NC 2018          6.1       1.8        350    NC 2018       200     610\n5    NC 2019          5.9       1.6        375    NC 2019       210     590\n6    NC 2020          5.3       1.8        400    NC 2020       220     530\n7    CO 2018          4.7       2.7        200    CO 2018       230     470\n8    CO 2019          4.4       2.6        300    CO 2019       240     440\n9    CO 2020          5.1       2.5        400    CO 2020       270     510\n\n\n\n\nFor whatever reason with SQLite gets a little grumpy with the join syntax.\nIf we wanted to do various other joins like inner and anti joins we would do a similar thing.\n\nquery = \"SELECT * \n        FROM national_data\n    INNER JOIN national_libraries \n    ON(\n    national_data.state = national_libraries.state AND\n    national_data.year = national_libraries.year\n    )\n\"\n\ndbGetQuery(db_con, sql(query))\n\n  state year unemployment inflation population state year libraries schools\n1    CO 2018          4.7       2.7        200    CO 2018       230     470\n2    CO 2019          4.4       2.6        300    CO 2019       240     440\n3    CO 2020          5.1       2.5        400    CO 2020       270     510\n4    NC 2018          6.1       1.8        350    NC 2018       200     610\n5    NC 2019          5.9       1.6        375    NC 2019       210     590\n6    NC 2020          5.3       1.8        400    NC 2020       220     530\n\n\n\n\nInequality joins\nConfession I have never really understood how inequality joins work in regular dplyr but I am sure at some point I am going to need them and now when the stakes are so low is a good time to do it. So lets just take the data from the dplyr 1.1.0 announcement to do this since we know what the output should be.\n\ncompanies &lt;- tibble(\n  id = c(\"A\", \"B\", \"B\"),\n  since = c(1973, 2009, 2022),\n  name = c(\"Patagonia\", \"RStudio\", \"Posit\")\n)\n\ntransactions &lt;- tibble(\n  company = c(\"A\", \"A\", \"B\", \"B\"),\n  year = c(2019, 2020, 2021, 2023),\n  revenue = c(50, 4, 10, 12)\n)\n\ncompanies = copy_to(con3, companies, overwrite = TRUE)\n\ntransactions = copy_to(con4, transactions, overwrite = TRUE)\n\ndb_con = con3$con\n\nSo the main idea of an inequality join is that we can join by a key in this case company but only keep records from a certain date. The blog post kind of equates it with a filter/WHERE that happens during the join phase. So we would see something like this\n\ntransactions |&gt;\n  inner_join(companies, join_by(company == id, year &gt;= since)) \n\n# Source:   SQL [?? x 5]\n# Database: sqlite 3.47.1 [:memory:]\n  company  year revenue since name     \n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1 A        2019      50  1973 Patagonia\n2 A        2020       4  1973 Patagonia\n3 B        2021      10  2009 RStudio  \n4 B        2023      12  2009 RStudio  \n5 B        2023      12  2022 Posit    \n\n\nInstead of two equality statements we would effectively use the same syntax just swapping out the = with &gt;=\n\nquery = \"\n      SELECT * FROM transactions\n      INNER JOIN companies \n      ON(\n      transactions.company = companies.id AND\n      transactions.year &gt;= companies.since\n      )\n\n\"\n\ndbGetQuery(db_con, sql(query))\n\n  company year revenue id since      name\n1       A 2019      50  A  1973 Patagonia\n2       A 2020       4  A  1973 Patagonia\n3       B 2021      10  B  2009   RStudio\n4       B 2023      12  B  2009   RStudio\n5       B 2023      12  B  2022     Posit",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#pivots",
    "href": "SQL.html#pivots",
    "title": "SQL",
    "section": "Pivots",
    "text": "Pivots\nIn tidyverse parlance we use pivots to change the “shape of the data.” If you are unfamiliar with this idea consider the religion and income data below. You will notice that we have a column for each income bracket or what is sometimes called “wide” data. This may be useful for some question but generally if we want to plot things or do things it will be easier if they are “long” data.\n\n\nCode\ncon5 = src_memdb()\n\nrelig = copy_to(con5, relig_income, overwrite = TRUE)\n\nhead(relig_income, n = 2)\n\n\n# A tibble: 2 × 11\n  religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Agnostic      27        34        60        81        76       137        122\n2 Atheist       12        27        37        52        35        70         73\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\nTo make our data “long” we use pivot_longer and to make data “wide” we use pivot_wider each has their own quirks but the general idea is that we have to tell these functions where to put the old names/where to get the new names and where to put the old values/where to get the new values. So if we wanted to make our data longer we would do something like this.\n\nlong = relig_income |&gt;\n    pivot_longer(-religion,\n                names_to = 'income_bracket',\n                values_to = 'income')\n\nhead(long, n = 2)\n\n# A tibble: 2 × 3\n  religion income_bracket income\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;\n1 Agnostic &lt;$10k              27\n2 Agnostic $10-20k            34\n\n\nIf we wanted to make this wide again all we would do is reverse this with pivot_wider\n\nwide = long |&gt;\n    pivot_wider(names_from = income_bracket, values_from = income)\n\n\n\nThere are ton of additional functionality that will not be covered like dealing with not uniquely identified columns.\nTo get a sense of how to do this let’s consult our old friend show_query\n\nrelig |&gt;\n    pivot_longer(-religion,\n                names_to = 'income_bracket',\n                values_to = 'income') |&gt;\n                    show_query()\n\nWe are not going to actually show the results because it is quite the query. The summary of what is happening is that SQLite doesn’t have a perfect equivalent of pivot_longer. Basically, what you need to do is to keep appending smaller and smaller data frames to each other until you get to a long data frame. In other flavors of SQL this process is a lot more humane with explicit PIVOT and UNPIVOT but I am not in one of those flavors. To spare myself a bit I am just going to do two columns\n\ntbl(con5, sql(\"\n    SELECT religion, '&lt;$10k' AS income_bracket, '&lt;$10k' AS income\n    FROM relig_income \n\n    UNION ALL\n\n    SELECT religion, '$10-20k' AS income_bracket, '$10-20k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$20-30k' AS income_bracket, '$20-30k' AS income\n    FROM relig_income\n\n    \n\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.47.1 [:memory:]\n   religion                income_bracket income\n   &lt;chr&gt;                   &lt;chr&gt;          &lt;chr&gt; \n 1 Agnostic                &lt;$10k          &lt;$10k \n 2 Atheist                 &lt;$10k          &lt;$10k \n 3 Buddhist                &lt;$10k          &lt;$10k \n 4 Catholic                &lt;$10k          &lt;$10k \n 5 Don’t know/refused      &lt;$10k          &lt;$10k \n 6 Evangelical Prot        &lt;$10k          &lt;$10k \n 7 Hindu                   &lt;$10k          &lt;$10k \n 8 Historically Black Prot &lt;$10k          &lt;$10k \n 9 Jehovah's Witness       &lt;$10k          &lt;$10k \n10 Jewish                  &lt;$10k          &lt;$10k \n# ℹ more rows\n\n\nI am a little scared to see what this looks for pivot_wider but we should at least give it a go.\n\nlong = relig |&gt;\n    pivot_longer(-religion,\n                 names_to = 'income_bracket',\n                 values_to = 'income')\n\nlong |&gt;\n    pivot_wider(names_from = income_bracket, values_from = income) |&gt;\n    show_query()\n\nOkay again this is a little unwieldy to show. Basically what happens is that we are creating a big case_when condition and then from there we are going to use the same binding trick and then group the data. So lets go ahead and copy and paste some of this.\n\n\nCode\nquery = \"\nSELECT\n    religion,\n    MAX(CASE WHEN (income_bracket = '&lt;$10k') THEN income END) AS '&lt;$10K',\n    MAX(CASE WHEN (income_bracket = '$10-20k') THEN income END) AS '$10-20k',\n    MAX(CASE WHEN (income_bracket = '$20-30k') THEN income END) AS '$20-30k'\nFROM (\n    SELECT religion, '&lt;$10k' AS income_bracket, '&lt;$10k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$10-20k' AS income_bracket, '$10-20k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$20-30k' AS income_bracket, '$20-30k' AS income\n    FROM relig_income\n) AS wide_religion\nGROUP BY religion\n\"\n\ntbl(con5, sql(query))\n\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.47.1 [:memory:]\n   religion                `&lt;$10K` `$10-20k` `$20-30k`\n   &lt;chr&gt;                   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n 1 Agnostic                &lt;$10k   $10-20k   $20-30k  \n 2 Atheist                 &lt;$10k   $10-20k   $20-30k  \n 3 Buddhist                &lt;$10k   $10-20k   $20-30k  \n 4 Catholic                &lt;$10k   $10-20k   $20-30k  \n 5 Don’t know/refused      &lt;$10k   $10-20k   $20-30k  \n 6 Evangelical Prot        &lt;$10k   $10-20k   $20-30k  \n 7 Hindu                   &lt;$10k   $10-20k   $20-30k  \n 8 Historically Black Prot &lt;$10k   $10-20k   $20-30k  \n 9 Jehovah's Witness       &lt;$10k   $10-20k   $20-30k  \n10 Jewish                  &lt;$10k   $10-20k   $20-30k  \n# ℹ more rows",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#unnesta-brief-aside",
    "href": "SQL.html#unnesta-brief-aside",
    "title": "SQL",
    "section": "Unnest/a brief aside",
    "text": "Unnest/a brief aside\nSo one thing that you come across from time to time in R and python data wrangling are list columns. These happen for a variety of reasons and are pretty innocuous to handle.\n\nlist_starwars = starwars |&gt;\n    select(name, films)\n\n list_starwars |&gt;\n    unnest_longer(films)\n\n# A tibble: 173 × 2\n   name           films                  \n   &lt;chr&gt;          &lt;chr&gt;                  \n 1 Luke Skywalker A New Hope             \n 2 Luke Skywalker The Empire Strikes Back\n 3 Luke Skywalker Return of the Jedi     \n 4 Luke Skywalker Revenge of the Sith    \n 5 Luke Skywalker The Force Awakens      \n 6 C-3PO          A New Hope             \n 7 C-3PO          The Empire Strikes Back\n 8 C-3PO          Return of the Jedi     \n 9 C-3PO          The Phantom Menace     \n10 C-3PO          Attack of the Clones   \n# ℹ 163 more rows\n\n\nHowever, per this Stack overflow answer and the linked question this is not really a thing or like really not advised. Even when you try to copy the starwars dataset to a database you get an error.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#misc",
    "href": "SQL.html#misc",
    "title": "SQL",
    "section": "Misc",
    "text": "Misc",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#ranking",
    "href": "SQL.html#ranking",
    "title": "SQL",
    "section": "Ranking",
    "text": "Ranking\nThere are lots of different ways to rank things in R if we want to return the min/max you can do\n\npengs |&gt;\n    slice_max(bill_length_mm, n = 3)\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nThere are also various ranking functions.\n\nexample = tribble(~id, ~col1,\n                   1, 1,\n                   2, 2,\n                   3, 2,\n                   4, 3,\n                   5, 4)\n\nexample |&gt;\n    mutate(rank_one = dense_rank(col1),\n           rank_two = min_rank(col1))\n\n# A tibble: 5 × 4\n     id  col1 rank_one rank_two\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     1     1        1        1\n2     2     2        2        2\n3     3     2        2        2\n4     4     3        3        4\n5     5     4        4        5\n\n\nLike our dplyr join functions the dense_rank and min_rank function actually takes inspiration from SQL. So in our example where the two functions differ is how they handle ties. So in dense_rank and min_rank both id 2 and 3 get assigned the same rank where they differ is dense_rank will assign id 4 the rank of 3 and min_rank will assign id 4 the rank of 4.\nSo how would we do this in SQL\n\ncon7 = src_memdb()\n\nteam_rankings = copy_to(con7, example)\n\nteam_rankings |&gt;\n    mutate(rank_one = dense_rank(col1)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `example`.*,\n  CASE\nWHEN (NOT((`col1` IS NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((`col1` IS NULL)) THEN 1 ELSE 0 END) ORDER BY `col1`)\nEND AS `rank_one`\nFROM `example`\n\n\nThis is deceptively a bit more complex. So lets break it down.\n\ntbl(con7, sql(\"\nSELECT\nexample .*,\n    CASE \nWHEN (NOT((col1 is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((col1 is NULL)) THEN 1 ELSE 0 END) ORDER BY col1)\nEND AS rank_one\nFROM example\n\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.47.1 [:memory:]\n     id  col1 rank_one\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;\n1     1     1        1\n2     2     2        2\n3     3     2        2\n4     4     3        3\n5     5     4        4\n\n\nSo basically the PARTITION BY bit is used to divide the data into groups before we rank them. The CASE WHEN handles when we have missing values. Then the window function is applying dense rank over these partions. This was a somewhat silly example so lets do something a bit more realistic. Lets say we actually want to rank the penguins by average bill length and then return the penguins in the top 3.\n\ntbl(con, sql(\n    \"\n    SELECT\n    ranked_pengs .*,\n    CASE\n    WHEN (NOT((avg_bill_length is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((avg_bill_length is NULL)) THEN 1 ELSE 0 END) ORDER BY avg_bill_length)\n    END AS rank\n    FROM( \n     SELECT pengs .*, AVG(bill_length_mm) OVER () AS avg_bill_length\n     FROM pengs)\n     AS ranked_pengs \n     LIMIT 3\n    \"\n))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.47.1 [:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg_bill_length &lt;dbl&gt;, rank &lt;int&gt;\n\n\nWe could also do this by groups by just inserting a group by statement before the limit bit\n\ntbl(con, sql(\n    \"\n    SELECT\n    ranked_pengs .*,\n    CASE\n    WHEN (NOT((avg_bill_length is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((avg_bill_length is NULL)) THEN 1 ELSE 0 END) ORDER BY avg_bill_length)\n    END AS rank\n    FROM( \n     SELECT pengs .*, AVG(bill_length_mm) OVER () AS avg_bill_length\n     FROM pengs)\n     AS ranked_pengs \n     GROUP BY species\n     LIMIT 3\n    \"\n))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Torgersen           39.1          18.7               181        3750\n2 Chinstrap Dream               46.5          17.9               192        3500\n3 Gentoo    Biscoe              46.1          13.2               211        4500\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg_bill_length &lt;dbl&gt;, rank &lt;int&gt;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#distinct-values",
    "href": "SQL.html#distinct-values",
    "title": "SQL",
    "section": "Distinct Values",
    "text": "Distinct Values\nDuplicates are a fact of life but depending on your question or what information you are trying to show repeated records may not be desirable. We handle these with the same function but kind of like mutate we have to let select handle these. If we wanted one row per column without having to specify every column in our dataset than we could do something like this\n\ntbl(con, sql(\"SELECT *\n            FROM(\n            SELECT pengs .*,\n            ROW_NUMBER() OVER (PARTITION BY species ORDER BY species) AS id \n            FROM PENGS) AS small_pengs\n            WHERE id = 1\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Torgersen           39.1          18.7               181        3750\n2 Chinstrap Dream               46.5          17.9               192        3500\n3 Gentoo    Biscoe              46.1          13.2               211        4500\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, id &lt;int&gt;\n\n\nHowever if we have a slightly less complex query than we can feed distinct multiple columns\n\ntbl(con, sql(\"SELECT DISTINCT species, island\n            FROM pengs\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  species   island   \n  &lt;chr&gt;     &lt;chr&gt;    \n1 Adelie    Torgersen\n2 Adelie    Biscoe   \n3 Adelie    Dream    \n4 Gentoo    Biscoe   \n5 Chinstrap Dream",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#sql-concepts-translated",
    "href": "SQL.html#sql-concepts-translated",
    "title": "SQL",
    "section": "SQL Concepts translated",
    "text": "SQL Concepts translated\n\nCTES and Subqueries\nHere we are going to go over the general ideas of more advanged SQL. These depart fairly significantly from dplyr in a lot of ways. Both SQL and dplyr are flexible and you can get them to do a lot. However, since we are working with things that are on the company computer that are really big the SQL parser will enforce some optimization/when you do something to the data it is no longer available. We saw this a little bit with WHERE and HAVING but this was never made explicit.\nSQL shifts away from my mental model of how to manipulate data. We never explicitly assign anything and in dplyr, data.table, polars etc have an explicit logic of chaining built into it with not a lot of rules baked into the order of execution as long as the column is available. Whereas SQL enforces execution because it is going to optimize our query for us. This requires a little sacrafice in flexibility in how we write the code to make the optimization process easier.\nWhat this means practically is that in a lot of cases as a dplyr or coder I will assign intermediate objects to check on the health of the data cleaning pipeline as to not explicitly overwrite results. Polars kind of more clearly reflects where we aren’t allowed to use columns we just created we have to separate them into another operation. Effectively we can kind of think of CTE’s and subqueries as creating intermediate objects where we do some data cleaning and then do filter or do something else to it. So think of something to the effect of the below code.\n\npenguins = palmerpenguins::penguins\n\ncte_examp = penguins |&gt;\n    mutate(sqr_mass = body_mass_g^2,\n           sqr_length = bill_length_mm^2)\n\ncte_filt = cte_examp |&gt;\n    filter(sqr_mass &gt; 60 | sqr_length &gt; 20)\n\nsubquery_examp = penguins |&gt;\n    mutate(sqr_mass = body_mass_g^2,\n           sqr_length = bill_length_mm^2) |&gt;\nfilter(sqr_mass &gt; 60 | sqr_length &gt; 20)\n\nHowever, often times we may need to do something akin to assigning intermediate operations to solve our data processing needs. SQL has built in ways to do this but they definitely look a little wonky. We have Common Table Expressions and Subqueries\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Subquery\n                Common Table Expression\n              \n        \n        \n        \n                \n                  Inner queries that allow us to filter and perform calculations to be used later. Only used in the FROM Clause\n                  Effectively a table we can refer back to using FROM small_table\n                \n        \n      \n    \n\n\n\n\n\nCTE\nSo lets say we have a table where we are interested in ranking the top selling artist within each genre based on revenue per member.\n\nartists_con = dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\nartistis_data = copy_to(artists_con, artists_data)\n\nSo in dplyr we would just do something like this\n\nartists_data = tibble(\n  artist_id = c(103, 104, 105, 109, 110),\n  artist_name = c(\"Taylor Swift\", \"BTS\", \"Adele\", \"Blackpink\", \"Maroon 5\"),\n  genre = c(\"Pop\", \"K-Pop\", \"Pop\", \"K-Pop\", \"Pop\"),\n  concert_revenue = c(700000, 800000, 600000, 450000, 550000),\n  year_of_formation = c(2004, 2013, 2006, 2016, 1994),\n  country = c(\"United States\", \"South Korea\", \"United Kingdom\", \"South Korea\", \"United States\"),\n  number_of_members = c(1, 7, 1, 4, 5),\n  album_released = c(9, 7, 3, 5, 7),\n  label = c(\"Republic Records\", \"Big Hit Music\", \"Columbia Records\", \"YG Entertainment\", \"Interscope Records\"),\n  stringsAsFactors = FALSE\n)\n\nartists_data |&gt;\n    mutate(rev_per_member = concert_revenue / number_of_members) |&gt;\n    # forcing rank to reverse the order its ranking htins\n    mutate(rank = rank(-rev_per_member, ties.method = 'first'), .by = genre) |&gt;\n    filter(rank == 1) |&gt;\n    group_by(genre) |&gt;\n    arrange(desc(rev_per_member), .by_group = TRUE)\n\n# A tibble: 2 × 12\n# Groups:   genre [2]\n  artist_id artist_name  genre concert_revenue year_of_formation country      \n      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;        \n1       104 BTS          K-Pop          800000              2013 South Korea  \n2       103 Taylor Swift Pop            700000              2004 United States\n# ℹ 6 more variables: number_of_members &lt;dbl&gt;, album_released &lt;dbl&gt;,\n#   label &lt;chr&gt;, stringsAsFactors &lt;lgl&gt;, rev_per_member &lt;dbl&gt;, rank &lt;int&gt;\n\n\nFor a CTE we would do something like this\n\ntbl(artists_con, sql(\n    \"\n    WITH ranked_concerts_cte AS(\n    SELECT \n    artist_name,\n    concert_revenue,\n    genre,\n    number_of_members,\n    (concert_revenue / number_of_members) AS revenue_per_member,\n    RANK() OVER (\n    PARTITION BY genre\n    ORDER BY (concert_revenue / number_of_members) DESC) AS ranked_concerts\n    FROM artists_data)\n\n    SELECT\n  artist_name,\n  concert_revenue,\n  genre,\n  number_of_members,\n  revenue_per_member\nFROM ranked_concerts_cte\nWHERE ranked_concerts = 1\nORDER BY revenue_per_member DESC\n    \"\n))\n\nIn effect we are going through kind of just making an intermediate object in order to use it in a more familiar looking query. Here the logic is a bit obscured but imagine we are pulling millions of columns of with millions of rows from a big table. Here we are just goint to make our life a bit easier by just creating an intermediate object to query from\n\n\nSubquery\nIn this case we can shorten the steps by writing a subquery where we are using our FROM statement do the heavy lifting. So instead of making a smaller table we can use our from statement to do the processing like this\n\ntbl(artists_con, sql(\n    \"\n    SELECT \n        artist_name,\n        concert_revenue,\n        genre,\n        number_of_members,\n        revenue_per_member\n    FROM (\n        SELECT\n            artist_name, \n            concert_revenue, \n            genre,\n            number_of_members,\n            (concert_revenue / number_of_members) AS revenue_per_member,\n            RANK() OVER (\n                PARTITION BY genre\n                ORDER BY (concert_revenue / number_of_members) DESC\n            ) AS ranked_concerts\n        FROM artists_data\n    ) AS sub\n    WHERE ranked_concerts = 1\n    ORDER BY revenue_per_member DESC\n    \"\n))\n\n\n\nInner Joins\nInner joins come up a lot in SQL interview questions. Part of the reason is that they have a lot of utility in data cleaning generally but especially when we have to kind of contort ourselves around our tools. Take two different problems\nSuppose we want to know which manager makes less than their direct report from this table.\n\nemployees = tribble(\n~employee_id,   ~name,  ~salary,    ~department_id, ~manager_id,\n1,  \"Emma Thompson\",    3800,   1,  6,\n2,  \"Daniel Rodriguez\", 2230,   1,  7,\n3,  'Olivia Smith',    7000,    1,   8,\n4,  'Noah Johnson',   6800, 2,  9,\n5,  'Sophia Martinez',  1750,   1,  11,\n6,  'Liam Brown',   13000,   3, NA,\n7,  'Ava Garcia',  12500,   3,  NA,\n8,  'William Davis',    6800,   2,  NA,\n10, 'James Anderson',   4000,   1,  11) \n\ntt(employees)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                employee_id\n                name\n                salary\n                department_id\n                manager_id\n              \n        \n        \n        \n                \n                  1\n                  Emma Thompson\n                  3800\n                  1\n                  6\n                \n                \n                  2\n                  Daniel Rodriguez\n                  2230\n                  1\n                  7\n                \n                \n                  3\n                  Olivia Smith\n                  7000\n                  1\n                  8\n                \n                \n                  4\n                  Noah Johnson\n                  6800\n                  2\n                  9\n                \n                \n                  5\n                  Sophia Martinez\n                  1750\n                  1\n                  11\n                \n                \n                  6\n                  Liam Brown\n                  13000\n                  3\n                  NA\n                \n                \n                  7\n                  Ava Garcia\n                  12500\n                  3\n                  NA\n                \n                \n                  8\n                  William Davis\n                  6800\n                  2\n                  NA\n                \n                \n                  10\n                  James Anderson\n                  4000\n                  1\n                  11\n                \n        \n      \n    \n\n\n\nI think the data in the original data lemur question is a bit messed up since it should indicate that Olivia Smith is in the same department as her manager William Davis.\n\nemployees_fixed = employees |&gt;\n    mutate(department_id = ifelse(name == 'Olivia Smith', 2, department_id)) \n\nSince we know managers don’t have a manager id and the manager id is denoted by their employee id we can just create a separate tibble and then join on the manager id column\n\nmanagers = employees_fixed |&gt;\n    filter(is.na(manager_id)) |&gt;\n    select(-manager_id) |&gt;\n    rename(manager_id = employee_id, \n          manager_salary = salary) \n\n\nemployees_fixed |&gt;\n    filter(!is.na(manager_id)) |&gt;\n    left_join(managers, join_by(manager_id)) |&gt;\n    filter(salary &gt; manager_salary)\n\n# A tibble: 1 × 8\n  employee_id name.x     salary department_id.x manager_id name.y manager_salary\n        &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1           3 Olivia Sm…   7000               2          8 Willi…           6800\n# ℹ 1 more variable: department_id.y &lt;dbl&gt;\n\n\nWhich gives us the correct answer. In SQL we would have to do something a bit different. We have to join the table to itself to create the same effect.\n\ninner_join_con = src_memdb()\n\nemployees_fixed_db = copy_to(inner_join_con, employees_fixed)\n\ntbl(inner_join_con, sql(\n    \"\n    SELECT \n    mgr.employee_id AS manager_id,\n    mgr.name AS manager_name,\n    mgr.salary AS manager_salary,\n    emp.employee_id AS employee_id,\n    emp.salary AS salary,\n    emp.name AS employee_name\n    FROM employees_fixed AS mgr \n    INNER JOIN employees_fixed AS emp\n    ON mgr.employee_id = emp.manager_id\n    WHERE emp.salary &gt; mgr.salary\n    \"\n))\n\n# Source:   SQL [?? x 6]\n# Database: sqlite 3.47.1 [:memory:]\n  manager_id manager_name  manager_salary employee_id salary employee_name\n       &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        \n1          8 William Davis           6800           3   7000 Olivia Smith \n\n\nEffectively we are using select to create a mini table and then from there we would just reference the\n\n\nWindow Functions\nWindow functions are kid of just grouped mutates in dplyr. We define the windows using OVER and PARTITION BY. We are diving up an existing table into smaller windows that that let us run various aggregation functions or define groups. There are three parts to a window function\n\nOVER which tells SQL we are going to do a window function\nPARTION BY which tells SQL what to do the grouping by\nThen our operation So effectively we can think of it as this\n\n\npenguins |&gt;\n    mutate(avg_pengs = mean(body_mass_g, na.rm = TRUE), .by = species)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, avg_pengs &lt;dbl&gt;\n\n\nSo what we would do in dplyr would be something like this. In SQL we would do something broadly similar just in the SELECT part of the query so this would become\n\nemployees_fixed2 = employees |&gt;\n    mutate(department_id = ifelse(name == 'Olivia Smith', 2, department_id), \n           is_manager = ifelse(is.na(manager_id), TRUE, FALSE)) \n\nwindow_con = src_memdb()\n\nemployees_fixed_db = copy_to(window_con, employees_fixed2)\n\ntbl(window_con, sql(\n    \"\n    SELECT \n    name,\n    AVG(salary) OVER(PARTITION BY is_manager) AS avg_salary\n    FROM employees_fixed2\n    \"\n))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.47.1 [:memory:]\n  name             avg_salary\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Emma Thompson         4263.\n2 Daniel Rodriguez      4263.\n3 Olivia Smith          4263.\n4 Noah Johnson          4263.\n5 Sophia Martinez       4263.\n6 James Anderson        4263.\n7 Liam Brown           10767.\n8 Ava Garcia           10767.\n9 William Davis        10767.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#sql-practice",
    "href": "SQL.html#sql-practice",
    "title": "SQL",
    "section": "SQL practice",
    "text": "SQL practice\n\nHistogram of Tweets\nAssume you’re given a table Twitter tweet data, write a query to obtain a histogram of tweets posted per user in 2022. Output the tweet count per user as the bucket and the number of Twitter users who fall into that bucket.\nIn other words, group the users by the number of tweets they posted in 2022 and count the number of users in each group.\nThis question is a little bit weird since I didn’t quite understand what is going on. Basically we need to do something like this.\n\nfake_user_data = tibble(\n    tweet_id = c(12321,12312123,1234231231,1232342341,1234234231212),\n    user_id = c(111,111,111,254,148),\n    tweet_date = c(mdy_hms('12/30/2021 00:00:00'), mdy_hms('01/01/2022 00:00:00'), mdy_hms('02/14/2022 00:00:00'),\n                   mdy_hms('03/01/2022 00:00:00'), mdy_hms('03/23/2022 00:00:00'))\n)\n\nfake_user_data |&gt;\n    filter(year(tweet_date) == 2022) |&gt;\n    group_by(user_id) |&gt;\n    summarise(tweet_count_per_user = n(), .groups = 'drop') |&gt;\n    summarise(tweet_bucket = n(), .by = tweet_count_per_user) |&gt;\n    rename(users_num = tweet_count_per_user)\n\n# A tibble: 2 × 2\n  users_num tweet_bucket\n      &lt;int&gt;        &lt;int&gt;\n1         2            1\n2         1            2\n\n\nSo kind of something that I don’t do a lot. The question is effectively just testing if you can use subqueries. So this mess becomes. A query that looks like this\n\ntwitter_bucket = dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\n\nfake_tweet_post = copy_to(twitter_bucket, fake_user_data, overwrite = TRUE)\n\ntwitter_bucket &lt;- tbl(twitter_bucket, sql(\"\n    SELECT \n        tweet_count_per_user AS tweet_bucket, \n        COUNT(user_id) AS users_num\n    FROM (\n        SELECT \n            user_id, \n            COUNT(tweet_id) AS tweet_count_per_user\n        FROM fake_user_data\n        WHERE EXTRACT(YEAR FROM tweet_date) = 2022\n        GROUP BY user_id\n    ) AS total_tweets\n    GROUP BY tweet_count_per_user\n\"))\n\ntwitter_bucket |&gt;\n    collect()",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "SQL.html#getting-qualified-candidates",
    "href": "SQL.html#getting-qualified-candidates",
    "title": "SQL",
    "section": "Getting qualified candidates",
    "text": "Getting qualified candidates\nGiven a table of candidates and their skills, you’re tasked with finding the candidates best suited for an open Data Science job. You want to find candidates who are proficient in Python, Tableau, and PostgreSQL.\nWrite a query to list the candidates who possess all of the required skills for the job. Sort the output by candidate ID in ascending order.\nSo one thing that is interesting is that we have SQL has this having count function that makes life a little easier. Basically WHERE clauses can’t be done sequentially so instead of doing somethign like this\n\ncanidates = tribble(\n~candidate_id, ~skill,\n123,    'Python',\n123,    'Tableau',\n123,    'PostgreSQL',\n234,    'R',\n234,    'PowerBI',\n234,    'SQL Server',\n345,    'Python',\n345,    'Tableau')\n\n\ncanidates |&gt;\n    filter(skill %in% c('Python', 'PostgreSQL', 'Tableau')) |&gt;\n    group_by(candidate_id) |&gt;\n    summarise(total = n(), .groups = 'drop') |&gt;\n    filter(total == 3)  |&gt;\n    select(candidate_id)\n\n# A tibble: 1 × 1\n  candidate_id\n         &lt;dbl&gt;\n1          123\n\n\nso this becomes\n\nlibrary(DBI)\nlibrary(RPostgres)\n\ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\n\ncandidates_data = copy_to(con, canidates)\n\n\nfind_qual = tbl(con, sql(\"\nSELECT candidate_id \nFROM canidates\nWHERE skill IN ('Python', 'Tableau', 'PostgreSQL')\nGROUP BY candidate_id\nHAVING COUNT(skill) = 3\n\"))\n\nfind_qual |&gt;\n    collect()\n\nOne thing that can be kind of annoying is that when we have something like a left join we need to be pretty explicit about what we are grabbing things from and what we are referencing from\n\npages_con &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = Sys.getenv('PASSWORD'))\n\npages = tribble(\n~page_id, ~page_name,\n20001,  'SQL Solutions',\n20045,  'Brain Exercises',\n20701,  'Tips for Data Analysts',\n31111,  'Postgres Crash Course',\n32728,  'Break the thread'\n\n)\n\npages = copy_to(pages_con, pages)\n\npage_likes_con &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password =  Sys.getenv('PASSWORD'))\n\npage_likes = tribble(\n~user_id, ~page_id, ~liked_date,\n111,    20001,  '04/08/2022 00:00:00',\n121,    20045,  '03/12/2022 00:00:00',\n156,    20001,  '07/25/2022 00:00:00',\n255,    20045,  '07/19/2022 00:00:00',\n125,    20001,  '07/19/2022 00:00:00',\n144,    31111,  '06/21/2022 00:00:00',\n125,    31111,  '07/04/2022 00:00:00') |&gt;\nmutate(liked_date = mdy_hms(liked_date))\n\npage_likes = copy_to(page_likes_con, page_likes, overwrite = TRUE, temporary = FALSE)\n\ntbl(pages_con, sql(\n    '\n    SELECT pages.page_id\n    FROM pages\n    LEFT JOIN page_likes ON(\n    pages.page_id = page_likes.page_id)\n    WHERE liked_date is NULL\n    ORDER BY page_id ASC\n    '\n)) |&gt;\n    collect()\n\nTesla is investigating production bottlenecks and they need your help to extract the relevant data. Write a query to determine which parts have begun the assembly process but are not yet finished.\nAssume you’re given the table on user viewership categorised by device type where the three types are laptop, tablet, and phone.\nWrite a query that calculates the total viewership for laptops and mobile devices where mobile is defined as the sum of tablet and phone viewership. Output the total viewership for laptops as laptop_reviews and the total viewership for mobile devices as mobile_views\n\ncon_nyt = dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"postgres\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password =  Sys.getenv('PASSWORD'))\n\nviewership = tribble(\n    ~user_id    ,~device_type   , ~view_time,\n123,    'tablet',   '01/02/2022 00:00:00',\n125,    'laptop',   '01/07/2022 00:00:00',\n128,    'laptop',   '02/09/2022 00:00:00',\n129,    'phone',    '02/09/2022 00:00:00',\n145,    'tablet',   '02/24/2022 00:00:00'\n)\n\nviewership = copy_to(con_nyt, viewership)\n\n\ntbl(con_nyt, sql(\n    \"\n    SELECT\n  SUM(laptop_views) AS laptop_views_count,\n  SUM(mobile_views) AS mobile_views_count\nFROM (\n  SELECT *,\n         CASE WHEN device_type = 'laptop' THEN 1 ELSE 0 END AS laptop_views, \n         CASE WHEN device_type IN ('tablet', 'phone') THEN 1 ELSE 0 END AS mobile_views\n  FROM viewership\n) AS viwership\n    \"\n)) |&gt;\n    collect()",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "Neural-Networks.html",
    "href": "Neural-Networks.html",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "Right now this is going to cover lots of different things that feel a little scatter shot. This is because an interview I am prepping for is scant on detail but explicitly mentions a whole host of deep learning stuff.\n\n\n\nSo I am just going to go through the zero to neural network video. One of the important basic concepts is back propagation.\n\nfrom micrograd.engine import Value \nfrom micrograd import nn \n\n\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f\nprint(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\n\n24.7041\n\ng.backward()\n\nprint(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\n\n138.8338\n\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db\n\n645.5773\n\n\nBack propagation is going to start at g and recursively go back and apply the chain rule. We are going to be able to evaluate the derivative of g through each of the nodes c, d, e, f or the inputs of a and b. What this back propagation will tell us is what will end up happening to g if we nudge a or b. So the 138 of is the slope of the growth. Neural networks are simply just little math machines that take input data and the weights of a neural network as inputs into this math machine and output its predicitions.\nNone of the math changes we just end up bundling them to make it more efficient to actually train these things.\n\n\n\n\nimport math \nimport numpy as np \nimport matplotlib.pyplot as plt \n\ndef f(x):\n    return 3*x**2-4*x+5\n\n\nxs = np.arange(-5,5, 0.25)\n\nys = f(xs)\n\nplt.plot(xs, ys)\n\n\n\n\n\n\n\n\nSo we are just going to define a scalar value function that takes a single scalar and returns a single scalar. What is the derivative of different points of x for this function. What a derivative is when we slightly increase an input say x how does the function respond with what sensitivity what is the slope how much does it go up or down.\n\nh = 0.001\n\nx = 3.0\n\nf(x + h)\n\n20.014003000000002\n\n(f(x + h) - f(x))/h\n\n14.00300000000243\n\nz = -3\n\n(f(z + h) - f(z))/h\n\n-21.996999999998934\n\n\nas we start fiddling with it we get closer and closer to the true slope. When its at three its about 14. If we do this same excercise with a negative number. In a bit more of a complex case where we have three scalars as inputs and a single output. We are again going to look at the deriviatives for d for the inputs of a, b, and c\n\nh = 0.001\n\na = 2.0 \n\nb = -3\n\nc = 10.0 \n\nd1 = a * b + c \n\n# inputs \n\na += h \n\nd2 = a * b + c\n\nprint(f\"d1 = {d1}, d2 = {d2}, slope = {(d2-d1)/h}\")\n\nd1 = 4.0, d2 = 3.997, slope = -3.0000000000001137\n\n\nHere we are looking at the derivative of with respect to a so we are going to bump it by h. If we did it with b by a little bit\n\nh = 0.001\n\na = 2.0 \n\nb = -3\n\nc = 10.0 \n\nd1 = a * b + c \n\n# inputs \n\nb += h \n\nd2 = a * b + c\n\nprint(f\"d1 = {d1}, d2 = {d2}, slope = {(d2-d1)/h}\")\n\nd1 = 4.0, d2 = 4.002, slope = 1.9999999999997797\n\n\n\n\nCode\nfrom graphviz import Digraph\na = Value(2.0)\n\nb = Value(-3.0)\n\nz = Value(4.0)\n\nc = a + b * z\n\n\nd = c * a\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef trace(root):\n  # builds a set of all nodes and edges in a graph\n  nodes, edges = set(), set()\n  def build(v):\n    if v not in nodes:\n      nodes.add(v)\n      for child in v._prev:\n        edges.add((child, v))\n        build(child)\n  build(root)\n  return nodes, edges\n\ndef draw_dot(root):\n  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n  \n  nodes, edges = trace(root)\n  for n in nodes:\n    uid = str(id(n))\n    # for any value in the graph, create a rectangular ('record') node for it\n    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n    if n._op:\n      # if this value is a result of some operation, create an op node for it\n      dot.node(name = uid + n._op, label = n._op)\n      # and connect this node to it\n      dot.edge(uid + n._op, uid)\n\n  for n1, n2 in edges:\n    # connect n1 to the op node of n2\n    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n  return dot\n\nx = Value(1.0)\ny = (x * 2 + 1).relu()\ny.backward()\ndot = draw_dot(y)\n\n\nWhere this gets a little bitter harder mentally as a political scientist is that there are a lot of these layers. The input of the next layer is effectively a weighted transformation of the previous layer with a nonlinear function applied to it. So it is not neccessarily ‘independent’ in the strictest sense of the term. W\nThe thing about neural networks it will keep track all of all these little steps. Then using backpropagation to see how all these little weights impact the loss function. Backpropagation lets us keep track of what weights actually led us down the wrong path.\nWe use backpropagation to compute partial derivatives of the loss with respect to the weights using the chain rule. Neurons apply activation functions to introduce non-linearity. We don’t control the input data, but we can tune the weights, which are initially random. If doing classification, we often apply softmax in the final layer to produce predicted probabilities. These predictions are compared to true labels via a loss function, which guides weight updates using gradient descent (which uses first-order derivatives, not second-order). The learning rate controls the step size, and we iterate this process over multiple epochs to optimize the model.\n\n\n\nOne really intersting usage of Neural Networks is the NFL. We have a lot of game film that we need to sort through and do it efficiently. We need to track tendencies which entails us correctly being able to determine coverages and where players are positionally on the field. However, this is a really complex task when we break it out into what we are asking the model to do. For an army of football nerds this is not that difficult it is just time consuming.\nWe have lots of things that are going on we have sequential data in the time, but we also have sequential data in the down and distance which are dependent on one another. We are less likely to see a team pass on 3rd and 1 then we are if they are 3rd and 10. How we got to those down and distances are also dependent on what happened the down before. Play calling is also dictated by the game clock and score. If we are down 6 in the 4th quarter with two minutes left to go with no timeouts running is going to be less likely. How players shift and where they are on the field is also dependent on playclock. Where a player is on offense is going to be dictated by the playclock. If we send a receiver in motion they have to be where they are supposed to be by the time that ball is snapped, and same with defensive players. Using neural networks we can at least make this process more efficient and through careful retraining we can get the model pretty good at identifying and predicting various things.\nWithin this example there are two intermingled tasks that require different neural network architectures. We have a time series classification task to predict what coverages or plays the offense is running. We also have a prediction task where we are trying to predict who is most likely to be the blitzer or who is going to get the ball. These are then intermingled with a computer vision task. While related they require changes in the underlying linear algebra.\n\n\nNeural networks are pretty adept and vision processing tasks. Part of the reason this they are really good at this is because we can use convulations to classify and identify images. Convolutions are really just a filter matrix where we take a matrix of some width and length from the image and then mutliplies it by another matrix which we call a filter. This filter is a smaller part of the image. We then multiply these two matrices to get a score. A high score indicates that the input images is similar to the filter. The filters will be learned via training. So if we take an NFL field we have two directions that the model can learn. North-South where the end zone the offense is trying to get to is North. We also have East-West where the sidelines to the quarterback’s throwing arm could be east if they are a right handed quarterback. We would take these two directions and generate “two” images for the filters. We then multiply the input image matrix by these two different filter matrices. As the model gets more complex we get more of these filters and multiply the input image by more of these filters. So in the next layer we would capture the width of the widest players on the East-West axis and the distance from the receiver and the cornerback on the North-South Axis. We can then differentiate the teams by differences in there jersey colors and the details of the decals as we get further down the layers of the network. Each time we convolve we apply multiply a region by this filter and then sum all those dot product to get a scalar. So we would do something to this effect.\n\ninput_matrix = np.array([[8, 4, 4,2 ,8],\n                         [6, 7, 5,4, 6],\n                         [2, 1, 3,7, 2]])\n\nfilter_matrix = np.array([[1, 0], \n                          [-1, 2]])\n\n\nflipped_filter = np.flipud(np.fliplr(filter_matrix))\n\nregion_one = np.array([[8,4],\n                       [6,7],\n                       [2,1]])\n\n\nconvolution_result = np.sum(np.dot(region_one, flipped_filter))\n\nconvolution_result = np.sum(convolution_result)\nprint(\"\\nConvolution result:\", convolution_result)\n\n\nConvolution result: 28\n\nprint(\"\\nWhats happening:\")\n\n\nWhats happening:\n\nprint(f\"{flipped_filter[0,0]} * {region_one[0,0]} + {flipped_filter[0,1]} * {region_one[0,1]} + \"\n      f\"{flipped_filter[1,0]} * {region_one[1,0]} + {flipped_filter[1,1]} * {region_one[1,1]} = {convolution_result}\")\n\n2 * 8 + -1 * 4 + 0 * 6 + 1 * 7 = 28\n\n\nAfter these convolutions we then pool the convolutions by taking, typically, non-overlapping 2x2 blocks and place them in a matrix with the maximum of those two by two blocks. So if we go ahead and convolve the entire input matrix we get something like this\n\nfrom scipy.signal import convolve2d\n\nconvolution_result = convolve2d(input_matrix, filter_matrix, mode = 'valid')\n\nconvolution_result\n\narray([[19,  9, 10,  2],\n       [ 6, 12, 13,  4]])\n\n\nThen in the pooling result we would get a pooled matrix that would effectively be a matrix with the values of 19 and 13. Except this would be a much much bigger matrix. We then take this slightly smaller matrix and then pass it through another convolution layer with smaller number of filters and so on and so forth until we get a single image. We can do a similar thing with lower quality images where we deconvolve them. This is less that we are reversing the convolution process but we are convolving the inputs like usual but we change the filter a bit.\n\nflip_one = np.flipud(filter_matrix)\n\nflip_two = np.fliplr(flip_one)\n\nprint(f\"Original Filter Matrix:\\n {filter_matrix}\\n\"\n      f\"After Flipping North-South:\\n {flip_one}\\n\",\n      f\"After Flipping East-West: \\n {flip_two}\")\n\nOriginal Filter Matrix:\n [[ 1  0]\n [-1  2]]\nAfter Flipping North-South:\n [[-1  2]\n [ 1  0]]\n After Flipping East-West: \n [[ 2 -1]\n [ 0  1]]\n\n\nWhat we are effectively doing is expanding the search over many grainy pixels rather than focusing on narrower regions of a high quality images. If we have many big pixels this has the advantage of expanding the underlying matrix and in the best case scenarios filling them with the correct values that then get turned back into colors, textures, etc to make a higher quality image.\n\n\n\nOne nice feature that people have found is that Neural Networks are actually pretty good at modeling time series for multiclass prediction problems. If we cheat a little bit and add units to this problem in a two class problem this just becomes a survival model. What is nice is that we can model duration dependence by creating duration dummies and then including duration, \\(duration^2\\), and \\(duration^3\\) as variables in our models or splines (Carter and Signorino 2010). This becomes a bit more difficult when we are in a multiclass prediction world, but this would effectively just be a competing risks model.\nIn effect what goes on in a recurrent neural network is that weights are reused between layers by keeping them stored in the hidden layers. In a cannonical NN each layer will generate its own weights independent of the prior layer. We then just use back propagation to track everything and update the weights once we pass everything to through the softmax function. However, in an RNN we store everything we learn in a vector containing information we learned in a prior layer which are then used to learn the weights for the next layer. Then we repeat this process for the next layer.\nTo make this a bit clearer. We start with training our model with the first point in our time series where some combination of inputs values, hyperparameters, and some randomly initialized weights plus an activation function. We then capture the outputs of that layer and store it as a vector. We then effectively use these outputs as a lagged dependent variable to help us predict the next output. This repeats for each time then we use back propagation through time to understand which hidden state is responsible for bad forecasts or classification.\n\n\n\n\n\n\nThese were interesting and new to me. These are used in a lot of image, video, and audio tasks mostly of the generative variety. So if you think of deep fakes these are done via a GAN in a lot of cases. It is kind of funny because the opening example of one of the most cited reviews is that of a forger and an art expert. So imagine we have a new art expert who is learning the tricks of the trade pitted against a new art forger. The art forger will generate images without access to the reference material and the art expert will authenticate them against real images. This is happening in an iterative process the art forger given an infinite number of iterations will eventually become a good enough forger to slip some images past the art forger.\nWhile they are in competition with each other they are intimately linked. The loss function that both the forger and the expert are trying to minimize are intimately linked. The tasks that are GANs are often tasked with are incredibly difficult. Grounding it in our art example mixing colors correctly and making putting them on the canvas correctly is difficult. Lets take a very simple example where the forger is trying to get the correct color. In 1000 tries to get the colors correctly the forger is going to get some correct but it is going to get a lot wrong\n\nlibrary(tidyverse)\nlibrary(patchwork)\nn = 1000\n\n\nart_forger_examp = tibble(`Fake Color` = rnorm(n, mean = 8),\n                              `Real Color` = rnorm(n, mean = 2)) |&gt;\n                            pivot_longer(everything())\n\nggplot(art_forger_examp, aes(x = value, fill = name)) +\n  geom_density(alpha = 0.5) +\n  labs(fill = NULL, x = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs the number of adversarial interactions increase the forger is going to get better at getting the color correct.\n\niterative_interactions &lt;- function(n, m = 2, m2_start = 8, steps = 5) {\n  mean_fake_values &lt;- seq(m2_start, m, length.out = steps)\n\n  plots &lt;- map(mean_fake_values, function(m2) {\n    plot_dat &lt;- data.frame(\n      `Fake Color` = rnorm(n, mean = m2),\n      `Real Color` = rnorm(n, mean = m)\n    ) |&gt; tidyr::pivot_longer(everything())\n\n    ggplot(plot_dat, aes(x = value, fill = name)) +\n      geom_density(alpha = 0.5) +\n      labs(fill = NULL, x = NULL, title = paste(\"Mean Fake:\", round(m2, 2))) +\n      theme_minimal()\n  })\n\n  # Combine the plots using patchwork\n  wrap_plots(plots, ncol = 2)\n}\n\n# Run simulation for different sample sizes\nsims &lt;- iterative_interactions(n = 10000, steps = 4)\n\n\nsims\n\n\n\n\n\n\n\n\nEffectively as the art forger banks more and more information they will start get better at this. This step repeats for every part of the image.\n\n\n\n\nFor LLM’s and these generative models we have to look at Attention, Encoders, and decoders the entire architecture is known as a transformer. The foundational paper in these models gets the idea from translations. In gendered languages like French or Spanish is going to point the translator in the wrong direction. Take le homme mange or the man eats in French. Homme doesn’t depend on any other word in the sentence but the and eats depends on man. Le could be la, nous, or ils depending the number of men and mange could mangeons or mangent depending on tense. For a sentence in any language a word is going to attend to another word in the sentence but not all of them.\nIn the model attention works kind of like a key value pair where attention is the dot product of the query and the key. Or in another words\n\\[\n\\text{Attention Weights} = \\text{What the work is looking for in another words} \\times \\text{The actual representation of the word that gets passed along}\n\\]\nWhere we get a score of how much one word should pay attention to another. We then feed it into a sofmax function to effectively spit out weights that can be interpreted as probabilities. So an attention score of 0.000001 means that a word has a low probability of attending to another. So in our French example homme would have a low probability of attending to any other word in the example. For computational purposes we stack these into multiple layers to create a multiheaded attention check. So instead of doing the attention of one word to another sequentially we can compute the relation of each word to each other much quicker.\nAn encoder in these models is really just comprised of N layers of these attention scores and a neural network. The NN is critical to actually compute word embeddings and to calculate where the word is in N dimensions and capture the relationship between words positionally and with respect to the whole context window and relationships in the entire sentence. So in an example sentence of I am really really hungry and haven’t had fried chicken in awhile lets go eat at KFC. The first layer could have a multiheaded attention check and a NN that looks at the relationship between words directly next to each other where we would focus on the sematantic relationship between “had”, “fried”, and “chicken” then the last layer would look at the temporal relationship between awhile and fried chicken. The stack is also important because we are taking the residuals of one layer and using it in the next layer. So the model would “understand” that ‘fried’ and ‘chicken’ have high attention scores. Then this result would be passed onto the next layer so it doesn’t have to relearn the semantic relationship and focus on other semantic relationship. In effect we are looking at a form of boosting where the results of one ‘tree’ impacts the way the next ‘tree’ is going to learn.\nYou can think of the encoder as producing a kind of “cumulative” attention score for each word, reflecting how each word relates to the others. The decoder then takes those cumulative attention scores and applies a second layer of attention—focusing on different parts of the encoder’s output to generate the next part of the output sequence. This makes the decoder’s understanding of the input sequence more nuanced and dynamic.\nAn additional revolution in this literature comes from Devlin et al. (2018) where they add a subtle but powerful augmentation of this transformer architecture. For the most part, the majority of languages read right to left. Instead of simple left-right attention BERT uses bidirectional self-attention letting the model consider prior and preceding words. Self-attention is simply the idea that we focus the attention locally where the word’s meaning is determined within the sentence. Attention would pick up on a larger global definition . So if we think about sports we could have two sentences with dog in them. For example lets take this passage &gt; “Matthew Stafford was a Georgia Bulldog. Matthew Stafford is known for his toughness he once led a comeback of a game where he separated his shoulder, he is a dog. The Staffords have a dog named Jack”\nIf we tasked a model with a Next Sentence Prediction (NSP) task using a standard transformer, it might predict the second sentence based on the global frequency of the word “dog” as the animal, especially if it has not seen enough context to disambiguate the term. This could result in the model misunderstanding that “dog” refers to toughness in the second instance.\nFor example, if we asked the model, “What are some examples of NFL players being dogs?”, the model might produce responses based only on the animal interpretation, because the word “dog” in the input text was not disambiguated properly. However, with self-attention, the model can better capture the contextual differences, focusing on the surrounding words like “toughness” and “comeback” to understand that “dog” here refers to toughness, rather than the literal animal\nThe combination of self-attention and bidirectionality is an inherently powerful idea. However, by combining these layers of complexity this can lead to overfitting. In the context of a NLP task all this means is that we will start to get semantic relationships that don’t make a ton of sense. To guard against this we could, theoretically, impose some regularization on the underlying neural networks. This would likely significantly impact the training time making an already expensive process more expensive. A cheaper alternative is to simply hide some words randomly and then after the encoder layer have the model predict the masked word. This ensures that the model isn’t just using one direction to improve its predictions and we are getting semantically meaningful relationships.\nOpenAI is a major player in the space of large language models (LLMs). Text data is inherently multi-dimensional, and fine-tuning a model for a specific task is both expensive and requires a large corpus of task-specific data, which might not be feasible for smaller datasets. Instead of starting with a task-specific model, OpenAI and other organizations working with LLMs often use a task-agnostic approach.\nThe process starts with feeding the model a diverse corpus of data rather than just task-specific documents. This is effective because GPTs (Generative Pre-trained Transformers) leverage the pretrained knowledge from a wide array of texts. Unlike some earlier models, GPTs do not rely solely on RNNs (Recurrent Neural Networks), but rather on the transformer architecture, which doesn’t store information between layers in the same way RNNs do. Instead, transformers use self-attention mechanisms that enable the model to better capture contextual relationships between words across the entire input sequence.\nDespite not being specifically trained for a task initially, the model can still apply its preexisting, general knowledge learned during pretraining to new, task-specific corpora. This approach allows for more efficient fine-tuning with smaller datasets, as the model is able to build on the knowledge it has already acquired. This approach is related to the Retrieval-Augmented Generation (RAG) framework, where the model can retrieve relevant information from a large corpus and generate task-specific outputs.\nGPT models also have a variety of hyperparameters that influence their behavior. One of the most well-known is temperature, which controls the probability distribution from which words are sampled. Higher temperatures (e.g., 1.0 or above) allow the model to pick words from the tail of the distribution, leading to more diverse but less predictable output. Conversely, a lower temperature (e.g., 0.2 or 0.5) narrows the distribution, making the model more deterministic and focused on high-probability words.\nWhile the temperature is just one of many hyperparameters, there are potentially billions of parameters in the model, each contributing to various aspects of the language and its understanding. These parameters allow the model to grasp complex semantic relationships and context, enhancing its ability to perform diverse tasks.\nA crucial element of OpenAI’s approach is the use of semi-supervised learning, which integrates reinforcement learning into the training process. One way this is applied is through few-shot learning, where the model is provided with a few labeled examples of the task it is being trained to perform. These labeled examples serve as a form of ground truth, which helps guide the model’s predictions.\nIn addition to few-shot learning, fine-tuning is a key process in improving the model’s output for specific tasks. For example, if the model is tasked with generating Python code but the output is inefficient, an expert can provide a more efficient code example to refine the model’s performance. This feedback loop allows the model to continuously improve based on real-world feedback and domain expertise.\n\n\n\nEffectively we are doing the same thing with images. If we are generating art we are making attention scores with the position of the colors and the location in an image and the orientation of the pixels. Our forger is effectively just as series of encoders and decoders that makes art and our expert is effectively just a series of encoders and decoders that score the image as fake or real by taking the generated image and a real image and comparing the attention scores\n\n\n\nOne of the major things companies are interested in these days is Retrieval Augmented Generation with vector search (RAG). The general idea of RAG is that lets say we have a question about pandas. LLM’s are likely going to have a large pool of examples where they can pull relevant code from. However, lets say pandas updates its API at time + 1 from when the model was last trained and deprecates well known functions in favor of doing something. We are running the model locally and don’t have access to the internet but we have downloaded a copy of the pandas documentation. The model is going to do its best to output the right answer but is never really going to acknowledge that it doesn’t know something so it will ‘hallucinate’ answers. We can use this new document to augment things and get the appropriate answers we want.\nThis is kind of the entire idea of the RAG framework. As a company its not neccessarily cost effective or legal to have Anthropic or OpenAi train a bespoke model or turn over confidential records to train a model on. This data is going to pop up in ways we can’t anticipate. Instead we can use RAG to augment our model.\n\n\n\\[\n\\text{user} \\overbrace{\\rightarrow}^{\\text{prompt}} \\text{Vector Data Base of Word emeddings} \\overbrace{\\rightarrow}^{\\text{Relevant Sources}} \\text{User prompt meets relevant source} \\rightarrow Model \\rightarrow Output\n\\]\nThe general idea is that we take our relevant documents and convert them into embeddings and take the user’s prompt and convert them to embeddings. We are kind of effectively taking our prompt and a document with a very close semantic relationship and then generate relevant output based on the most comparable document.\nWhat does this look like? Lets take Positron the new IDE by Posit. It is still a pretty young so it is not in the training dataset\n\n\n\n\n\n\nWithout RAG\n\n\n\nUser: What is Positron?\nOutput: The Positron IDE is a lightweight, fast, and customizable code editor designed to provide a simple and intuitive environment for software development. It was originally built as a plugin for Atom, a popular open-source text editor. Positron aimed to combine the best aspects of an editor (like Atom) with a focus on speed and simplicity.\n\n\nNone of this is actually correct in the context of what I was looking for. ChatGpt gave kind of a general definition of an IDE. Lets say that the internal documents were available for Zed and Positron and we wanted to ensure that an LLM would output useful information for our IDE. We could take these documents and then make embeddings then add it to a vector database. Now our query would look something like this\n\n\n\n\n\n\nWith RAG\n\n\n\nUser: What is Positron Model: A next-generation data science IDE built by Posit PBC An extensible, polyglot tool for writing code and exploring data A familiar environment for reproducible authoring and publishing",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#preface",
    "href": "Neural-Networks.html#preface",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "Right now this is going to cover lots of different things that feel a little scatter shot. This is because an interview I am prepping for is scant on detail but explicitly mentions a whole host of deep learning stuff.",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#neural-networks",
    "href": "Neural-Networks.html#neural-networks",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "So I am just going to go through the zero to neural network video. One of the important basic concepts is back propagation.\n\nfrom micrograd.engine import Value \nfrom micrograd import nn \n\n\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f\nprint(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\n\n24.7041\n\ng.backward()\n\nprint(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\n\n138.8338\n\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db\n\n645.5773\n\n\nBack propagation is going to start at g and recursively go back and apply the chain rule. We are going to be able to evaluate the derivative of g through each of the nodes c, d, e, f or the inputs of a and b. What this back propagation will tell us is what will end up happening to g if we nudge a or b. So the 138 of is the slope of the growth. Neural networks are simply just little math machines that take input data and the weights of a neural network as inputs into this math machine and output its predicitions.\nNone of the math changes we just end up bundling them to make it more efficient to actually train these things.",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#what-is-a-derivative",
    "href": "Neural-Networks.html#what-is-a-derivative",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "import math \nimport numpy as np \nimport matplotlib.pyplot as plt \n\ndef f(x):\n    return 3*x**2-4*x+5\n\n\nxs = np.arange(-5,5, 0.25)\n\nys = f(xs)\n\nplt.plot(xs, ys)\n\n\n\n\n\n\n\n\nSo we are just going to define a scalar value function that takes a single scalar and returns a single scalar. What is the derivative of different points of x for this function. What a derivative is when we slightly increase an input say x how does the function respond with what sensitivity what is the slope how much does it go up or down.\n\nh = 0.001\n\nx = 3.0\n\nf(x + h)\n\n20.014003000000002\n\n(f(x + h) - f(x))/h\n\n14.00300000000243\n\nz = -3\n\n(f(z + h) - f(z))/h\n\n-21.996999999998934\n\n\nas we start fiddling with it we get closer and closer to the true slope. When its at three its about 14. If we do this same excercise with a negative number. In a bit more of a complex case where we have three scalars as inputs and a single output. We are again going to look at the deriviatives for d for the inputs of a, b, and c\n\nh = 0.001\n\na = 2.0 \n\nb = -3\n\nc = 10.0 \n\nd1 = a * b + c \n\n# inputs \n\na += h \n\nd2 = a * b + c\n\nprint(f\"d1 = {d1}, d2 = {d2}, slope = {(d2-d1)/h}\")\n\nd1 = 4.0, d2 = 3.997, slope = -3.0000000000001137\n\n\nHere we are looking at the derivative of with respect to a so we are going to bump it by h. If we did it with b by a little bit\n\nh = 0.001\n\na = 2.0 \n\nb = -3\n\nc = 10.0 \n\nd1 = a * b + c \n\n# inputs \n\nb += h \n\nd2 = a * b + c\n\nprint(f\"d1 = {d1}, d2 = {d2}, slope = {(d2-d1)/h}\")\n\nd1 = 4.0, d2 = 4.002, slope = 1.9999999999997797\n\n\n\n\nCode\nfrom graphviz import Digraph\na = Value(2.0)\n\nb = Value(-3.0)\n\nz = Value(4.0)\n\nc = a + b * z\n\n\nd = c * a\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef trace(root):\n  # builds a set of all nodes and edges in a graph\n  nodes, edges = set(), set()\n  def build(v):\n    if v not in nodes:\n      nodes.add(v)\n      for child in v._prev:\n        edges.add((child, v))\n        build(child)\n  build(root)\n  return nodes, edges\n\ndef draw_dot(root):\n  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n  \n  nodes, edges = trace(root)\n  for n in nodes:\n    uid = str(id(n))\n    # for any value in the graph, create a rectangular ('record') node for it\n    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n    if n._op:\n      # if this value is a result of some operation, create an op node for it\n      dot.node(name = uid + n._op, label = n._op)\n      # and connect this node to it\n      dot.edge(uid + n._op, uid)\n\n  for n1, n2 in edges:\n    # connect n1 to the op node of n2\n    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n  return dot\n\nx = Value(1.0)\ny = (x * 2 + 1).relu()\ny.backward()\ndot = draw_dot(y)\n\n\nWhere this gets a little bitter harder mentally as a political scientist is that there are a lot of these layers. The input of the next layer is effectively a weighted transformation of the previous layer with a nonlinear function applied to it. So it is not neccessarily ‘independent’ in the strictest sense of the term. W\nThe thing about neural networks it will keep track all of all these little steps. Then using backpropagation to see how all these little weights impact the loss function. Backpropagation lets us keep track of what weights actually led us down the wrong path.\nWe use backpropagation to compute partial derivatives of the loss with respect to the weights using the chain rule. Neurons apply activation functions to introduce non-linearity. We don’t control the input data, but we can tune the weights, which are initially random. If doing classification, we often apply softmax in the final layer to produce predicted probabilities. These predictions are compared to true labels via a loss function, which guides weight updates using gradient descent (which uses first-order derivatives, not second-order). The learning rate controls the step size, and we iterate this process over multiple epochs to optimize the model.",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#flavors-of-neural-networks",
    "href": "Neural-Networks.html#flavors-of-neural-networks",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "One really intersting usage of Neural Networks is the NFL. We have a lot of game film that we need to sort through and do it efficiently. We need to track tendencies which entails us correctly being able to determine coverages and where players are positionally on the field. However, this is a really complex task when we break it out into what we are asking the model to do. For an army of football nerds this is not that difficult it is just time consuming.\nWe have lots of things that are going on we have sequential data in the time, but we also have sequential data in the down and distance which are dependent on one another. We are less likely to see a team pass on 3rd and 1 then we are if they are 3rd and 10. How we got to those down and distances are also dependent on what happened the down before. Play calling is also dictated by the game clock and score. If we are down 6 in the 4th quarter with two minutes left to go with no timeouts running is going to be less likely. How players shift and where they are on the field is also dependent on playclock. Where a player is on offense is going to be dictated by the playclock. If we send a receiver in motion they have to be where they are supposed to be by the time that ball is snapped, and same with defensive players. Using neural networks we can at least make this process more efficient and through careful retraining we can get the model pretty good at identifying and predicting various things.\nWithin this example there are two intermingled tasks that require different neural network architectures. We have a time series classification task to predict what coverages or plays the offense is running. We also have a prediction task where we are trying to predict who is most likely to be the blitzer or who is going to get the ball. These are then intermingled with a computer vision task. While related they require changes in the underlying linear algebra.\n\n\nNeural networks are pretty adept and vision processing tasks. Part of the reason this they are really good at this is because we can use convulations to classify and identify images. Convolutions are really just a filter matrix where we take a matrix of some width and length from the image and then mutliplies it by another matrix which we call a filter. This filter is a smaller part of the image. We then multiply these two matrices to get a score. A high score indicates that the input images is similar to the filter. The filters will be learned via training. So if we take an NFL field we have two directions that the model can learn. North-South where the end zone the offense is trying to get to is North. We also have East-West where the sidelines to the quarterback’s throwing arm could be east if they are a right handed quarterback. We would take these two directions and generate “two” images for the filters. We then multiply the input image matrix by these two different filter matrices. As the model gets more complex we get more of these filters and multiply the input image by more of these filters. So in the next layer we would capture the width of the widest players on the East-West axis and the distance from the receiver and the cornerback on the North-South Axis. We can then differentiate the teams by differences in there jersey colors and the details of the decals as we get further down the layers of the network. Each time we convolve we apply multiply a region by this filter and then sum all those dot product to get a scalar. So we would do something to this effect.\n\ninput_matrix = np.array([[8, 4, 4,2 ,8],\n                         [6, 7, 5,4, 6],\n                         [2, 1, 3,7, 2]])\n\nfilter_matrix = np.array([[1, 0], \n                          [-1, 2]])\n\n\nflipped_filter = np.flipud(np.fliplr(filter_matrix))\n\nregion_one = np.array([[8,4],\n                       [6,7],\n                       [2,1]])\n\n\nconvolution_result = np.sum(np.dot(region_one, flipped_filter))\n\nconvolution_result = np.sum(convolution_result)\nprint(\"\\nConvolution result:\", convolution_result)\n\n\nConvolution result: 28\n\nprint(\"\\nWhats happening:\")\n\n\nWhats happening:\n\nprint(f\"{flipped_filter[0,0]} * {region_one[0,0]} + {flipped_filter[0,1]} * {region_one[0,1]} + \"\n      f\"{flipped_filter[1,0]} * {region_one[1,0]} + {flipped_filter[1,1]} * {region_one[1,1]} = {convolution_result}\")\n\n2 * 8 + -1 * 4 + 0 * 6 + 1 * 7 = 28\n\n\nAfter these convolutions we then pool the convolutions by taking, typically, non-overlapping 2x2 blocks and place them in a matrix with the maximum of those two by two blocks. So if we go ahead and convolve the entire input matrix we get something like this\n\nfrom scipy.signal import convolve2d\n\nconvolution_result = convolve2d(input_matrix, filter_matrix, mode = 'valid')\n\nconvolution_result\n\narray([[19,  9, 10,  2],\n       [ 6, 12, 13,  4]])\n\n\nThen in the pooling result we would get a pooled matrix that would effectively be a matrix with the values of 19 and 13. Except this would be a much much bigger matrix. We then take this slightly smaller matrix and then pass it through another convolution layer with smaller number of filters and so on and so forth until we get a single image. We can do a similar thing with lower quality images where we deconvolve them. This is less that we are reversing the convolution process but we are convolving the inputs like usual but we change the filter a bit.\n\nflip_one = np.flipud(filter_matrix)\n\nflip_two = np.fliplr(flip_one)\n\nprint(f\"Original Filter Matrix:\\n {filter_matrix}\\n\"\n      f\"After Flipping North-South:\\n {flip_one}\\n\",\n      f\"After Flipping East-West: \\n {flip_two}\")\n\nOriginal Filter Matrix:\n [[ 1  0]\n [-1  2]]\nAfter Flipping North-South:\n [[-1  2]\n [ 1  0]]\n After Flipping East-West: \n [[ 2 -1]\n [ 0  1]]\n\n\nWhat we are effectively doing is expanding the search over many grainy pixels rather than focusing on narrower regions of a high quality images. If we have many big pixels this has the advantage of expanding the underlying matrix and in the best case scenarios filling them with the correct values that then get turned back into colors, textures, etc to make a higher quality image.\n\n\n\nOne nice feature that people have found is that Neural Networks are actually pretty good at modeling time series for multiclass prediction problems. If we cheat a little bit and add units to this problem in a two class problem this just becomes a survival model. What is nice is that we can model duration dependence by creating duration dummies and then including duration, \\(duration^2\\), and \\(duration^3\\) as variables in our models or splines (Carter and Signorino 2010). This becomes a bit more difficult when we are in a multiclass prediction world, but this would effectively just be a competing risks model.\nIn effect what goes on in a recurrent neural network is that weights are reused between layers by keeping them stored in the hidden layers. In a cannonical NN each layer will generate its own weights independent of the prior layer. We then just use back propagation to track everything and update the weights once we pass everything to through the softmax function. However, in an RNN we store everything we learn in a vector containing information we learned in a prior layer which are then used to learn the weights for the next layer. Then we repeat this process for the next layer.\nTo make this a bit clearer. We start with training our model with the first point in our time series where some combination of inputs values, hyperparameters, and some randomly initialized weights plus an activation function. We then capture the outputs of that layer and store it as a vector. We then effectively use these outputs as a lagged dependent variable to help us predict the next output. This repeats for each time then we use back propagation through time to understand which hidden state is responsible for bad forecasts or classification.",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#generative-models",
    "href": "Neural-Networks.html#generative-models",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "These were interesting and new to me. These are used in a lot of image, video, and audio tasks mostly of the generative variety. So if you think of deep fakes these are done via a GAN in a lot of cases. It is kind of funny because the opening example of one of the most cited reviews is that of a forger and an art expert. So imagine we have a new art expert who is learning the tricks of the trade pitted against a new art forger. The art forger will generate images without access to the reference material and the art expert will authenticate them against real images. This is happening in an iterative process the art forger given an infinite number of iterations will eventually become a good enough forger to slip some images past the art forger.\nWhile they are in competition with each other they are intimately linked. The loss function that both the forger and the expert are trying to minimize are intimately linked. The tasks that are GANs are often tasked with are incredibly difficult. Grounding it in our art example mixing colors correctly and making putting them on the canvas correctly is difficult. Lets take a very simple example where the forger is trying to get the correct color. In 1000 tries to get the colors correctly the forger is going to get some correct but it is going to get a lot wrong\n\nlibrary(tidyverse)\nlibrary(patchwork)\nn = 1000\n\n\nart_forger_examp = tibble(`Fake Color` = rnorm(n, mean = 8),\n                              `Real Color` = rnorm(n, mean = 2)) |&gt;\n                            pivot_longer(everything())\n\nggplot(art_forger_examp, aes(x = value, fill = name)) +\n  geom_density(alpha = 0.5) +\n  labs(fill = NULL, x = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs the number of adversarial interactions increase the forger is going to get better at getting the color correct.\n\niterative_interactions &lt;- function(n, m = 2, m2_start = 8, steps = 5) {\n  mean_fake_values &lt;- seq(m2_start, m, length.out = steps)\n\n  plots &lt;- map(mean_fake_values, function(m2) {\n    plot_dat &lt;- data.frame(\n      `Fake Color` = rnorm(n, mean = m2),\n      `Real Color` = rnorm(n, mean = m)\n    ) |&gt; tidyr::pivot_longer(everything())\n\n    ggplot(plot_dat, aes(x = value, fill = name)) +\n      geom_density(alpha = 0.5) +\n      labs(fill = NULL, x = NULL, title = paste(\"Mean Fake:\", round(m2, 2))) +\n      theme_minimal()\n  })\n\n  # Combine the plots using patchwork\n  wrap_plots(plots, ncol = 2)\n}\n\n# Run simulation for different sample sizes\nsims &lt;- iterative_interactions(n = 10000, steps = 4)\n\n\nsims\n\n\n\n\n\n\n\n\nEffectively as the art forger banks more and more information they will start get better at this. This step repeats for every part of the image.",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#so-how-do-these-work",
    "href": "Neural-Networks.html#so-how-do-these-work",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "For LLM’s and these generative models we have to look at Attention, Encoders, and decoders the entire architecture is known as a transformer. The foundational paper in these models gets the idea from translations. In gendered languages like French or Spanish is going to point the translator in the wrong direction. Take le homme mange or the man eats in French. Homme doesn’t depend on any other word in the sentence but the and eats depends on man. Le could be la, nous, or ils depending the number of men and mange could mangeons or mangent depending on tense. For a sentence in any language a word is going to attend to another word in the sentence but not all of them.\nIn the model attention works kind of like a key value pair where attention is the dot product of the query and the key. Or in another words\n\\[\n\\text{Attention Weights} = \\text{What the work is looking for in another words} \\times \\text{The actual representation of the word that gets passed along}\n\\]\nWhere we get a score of how much one word should pay attention to another. We then feed it into a sofmax function to effectively spit out weights that can be interpreted as probabilities. So an attention score of 0.000001 means that a word has a low probability of attending to another. So in our French example homme would have a low probability of attending to any other word in the example. For computational purposes we stack these into multiple layers to create a multiheaded attention check. So instead of doing the attention of one word to another sequentially we can compute the relation of each word to each other much quicker.\nAn encoder in these models is really just comprised of N layers of these attention scores and a neural network. The NN is critical to actually compute word embeddings and to calculate where the word is in N dimensions and capture the relationship between words positionally and with respect to the whole context window and relationships in the entire sentence. So in an example sentence of I am really really hungry and haven’t had fried chicken in awhile lets go eat at KFC. The first layer could have a multiheaded attention check and a NN that looks at the relationship between words directly next to each other where we would focus on the sematantic relationship between “had”, “fried”, and “chicken” then the last layer would look at the temporal relationship between awhile and fried chicken. The stack is also important because we are taking the residuals of one layer and using it in the next layer. So the model would “understand” that ‘fried’ and ‘chicken’ have high attention scores. Then this result would be passed onto the next layer so it doesn’t have to relearn the semantic relationship and focus on other semantic relationship. In effect we are looking at a form of boosting where the results of one ‘tree’ impacts the way the next ‘tree’ is going to learn.\nYou can think of the encoder as producing a kind of “cumulative” attention score for each word, reflecting how each word relates to the others. The decoder then takes those cumulative attention scores and applies a second layer of attention—focusing on different parts of the encoder’s output to generate the next part of the output sequence. This makes the decoder’s understanding of the input sequence more nuanced and dynamic.\nAn additional revolution in this literature comes from Devlin et al. (2018) where they add a subtle but powerful augmentation of this transformer architecture. For the most part, the majority of languages read right to left. Instead of simple left-right attention BERT uses bidirectional self-attention letting the model consider prior and preceding words. Self-attention is simply the idea that we focus the attention locally where the word’s meaning is determined within the sentence. Attention would pick up on a larger global definition . So if we think about sports we could have two sentences with dog in them. For example lets take this passage &gt; “Matthew Stafford was a Georgia Bulldog. Matthew Stafford is known for his toughness he once led a comeback of a game where he separated his shoulder, he is a dog. The Staffords have a dog named Jack”\nIf we tasked a model with a Next Sentence Prediction (NSP) task using a standard transformer, it might predict the second sentence based on the global frequency of the word “dog” as the animal, especially if it has not seen enough context to disambiguate the term. This could result in the model misunderstanding that “dog” refers to toughness in the second instance.\nFor example, if we asked the model, “What are some examples of NFL players being dogs?”, the model might produce responses based only on the animal interpretation, because the word “dog” in the input text was not disambiguated properly. However, with self-attention, the model can better capture the contextual differences, focusing on the surrounding words like “toughness” and “comeback” to understand that “dog” here refers to toughness, rather than the literal animal\nThe combination of self-attention and bidirectionality is an inherently powerful idea. However, by combining these layers of complexity this can lead to overfitting. In the context of a NLP task all this means is that we will start to get semantic relationships that don’t make a ton of sense. To guard against this we could, theoretically, impose some regularization on the underlying neural networks. This would likely significantly impact the training time making an already expensive process more expensive. A cheaper alternative is to simply hide some words randomly and then after the encoder layer have the model predict the masked word. This ensures that the model isn’t just using one direction to improve its predictions and we are getting semantically meaningful relationships.\nOpenAI is a major player in the space of large language models (LLMs). Text data is inherently multi-dimensional, and fine-tuning a model for a specific task is both expensive and requires a large corpus of task-specific data, which might not be feasible for smaller datasets. Instead of starting with a task-specific model, OpenAI and other organizations working with LLMs often use a task-agnostic approach.\nThe process starts with feeding the model a diverse corpus of data rather than just task-specific documents. This is effective because GPTs (Generative Pre-trained Transformers) leverage the pretrained knowledge from a wide array of texts. Unlike some earlier models, GPTs do not rely solely on RNNs (Recurrent Neural Networks), but rather on the transformer architecture, which doesn’t store information between layers in the same way RNNs do. Instead, transformers use self-attention mechanisms that enable the model to better capture contextual relationships between words across the entire input sequence.\nDespite not being specifically trained for a task initially, the model can still apply its preexisting, general knowledge learned during pretraining to new, task-specific corpora. This approach allows for more efficient fine-tuning with smaller datasets, as the model is able to build on the knowledge it has already acquired. This approach is related to the Retrieval-Augmented Generation (RAG) framework, where the model can retrieve relevant information from a large corpus and generate task-specific outputs.\nGPT models also have a variety of hyperparameters that influence their behavior. One of the most well-known is temperature, which controls the probability distribution from which words are sampled. Higher temperatures (e.g., 1.0 or above) allow the model to pick words from the tail of the distribution, leading to more diverse but less predictable output. Conversely, a lower temperature (e.g., 0.2 or 0.5) narrows the distribution, making the model more deterministic and focused on high-probability words.\nWhile the temperature is just one of many hyperparameters, there are potentially billions of parameters in the model, each contributing to various aspects of the language and its understanding. These parameters allow the model to grasp complex semantic relationships and context, enhancing its ability to perform diverse tasks.\nA crucial element of OpenAI’s approach is the use of semi-supervised learning, which integrates reinforcement learning into the training process. One way this is applied is through few-shot learning, where the model is provided with a few labeled examples of the task it is being trained to perform. These labeled examples serve as a form of ground truth, which helps guide the model’s predictions.\nIn addition to few-shot learning, fine-tuning is a key process in improving the model’s output for specific tasks. For example, if the model is tasked with generating Python code but the output is inefficient, an expert can provide a more efficient code example to refine the model’s performance. This feedback loop allows the model to continuously improve based on real-world feedback and domain expertise.",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#so-how-does-this-relate-to-generative-models",
    "href": "Neural-Networks.html#so-how-does-this-relate-to-generative-models",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "Effectively we are doing the same thing with images. If we are generating art we are making attention scores with the position of the colors and the location in an image and the orientation of the pixels. Our forger is effectively just as series of encoders and decoders that makes art and our expert is effectively just a series of encoders and decoders that score the image as fake or real by taking the generated image and a real image and comparing the attention scores",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Neural-Networks.html#rag-architecture",
    "href": "Neural-Networks.html#rag-architecture",
    "title": "Deep Learning (Mostly Neural Networks)",
    "section": "",
    "text": "One of the major things companies are interested in these days is Retrieval Augmented Generation with vector search (RAG). The general idea of RAG is that lets say we have a question about pandas. LLM’s are likely going to have a large pool of examples where they can pull relevant code from. However, lets say pandas updates its API at time + 1 from when the model was last trained and deprecates well known functions in favor of doing something. We are running the model locally and don’t have access to the internet but we have downloaded a copy of the pandas documentation. The model is going to do its best to output the right answer but is never really going to acknowledge that it doesn’t know something so it will ‘hallucinate’ answers. We can use this new document to augment things and get the appropriate answers we want.\nThis is kind of the entire idea of the RAG framework. As a company its not neccessarily cost effective or legal to have Anthropic or OpenAi train a bespoke model or turn over confidential records to train a model on. This data is going to pop up in ways we can’t anticipate. Instead we can use RAG to augment our model.\n\n\n\\[\n\\text{user} \\overbrace{\\rightarrow}^{\\text{prompt}} \\text{Vector Data Base of Word emeddings} \\overbrace{\\rightarrow}^{\\text{Relevant Sources}} \\text{User prompt meets relevant source} \\rightarrow Model \\rightarrow Output\n\\]\nThe general idea is that we take our relevant documents and convert them into embeddings and take the user’s prompt and convert them to embeddings. We are kind of effectively taking our prompt and a document with a very close semantic relationship and then generate relevant output based on the most comparable document.\nWhat does this look like? Lets take Positron the new IDE by Posit. It is still a pretty young so it is not in the training dataset\n\n\n\n\n\n\nWithout RAG\n\n\n\nUser: What is Positron?\nOutput: The Positron IDE is a lightweight, fast, and customizable code editor designed to provide a simple and intuitive environment for software development. It was originally built as a plugin for Atom, a popular open-source text editor. Positron aimed to combine the best aspects of an editor (like Atom) with a focus on speed and simplicity.\n\n\nNone of this is actually correct in the context of what I was looking for. ChatGpt gave kind of a general definition of an IDE. Lets say that the internal documents were available for Zed and Positron and we wanted to ensure that an LLM would output useful information for our IDE. We could take these documents and then make embeddings then add it to a vector database. Now our query would look something like this\n\n\n\n\n\n\nWith RAG\n\n\n\nUser: What is Positron Model: A next-generation data science IDE built by Posit PBC An extensible, polyglot tool for writing code and exploring data A familiar environment for reproducible authoring and publishing",
    "crumbs": [
      "Deep Learning (Mostly Neural Networks)"
    ]
  },
  {
    "objectID": "Time-Series.html",
    "href": "Time-Series.html",
    "title": "Time-Series",
    "section": "",
    "text": "Time is something that we all have to deal with. The problem with time in statistics is that it is SUPER SUPER finnicky. When we start to model a single thing over time much less multiple things shit starts to get weird really quickly. So lets get back into your time series notes.\n\n\nGenerally we have some fixes when linear regression starts to break down. The problem is that they only go so far and are not neccessarily going to gain us that much insightful leverage on the data generating proces. As lots of articles argue. We should take time seriously! So how do we do that?\nWe have several families of Time Series methods:\n\nError Correction Models\nARIMA\nARCH/GARCH\nGAMS\nMachine Learning Stuff\n\nTraditionally in Political Science and Economics we tend to focus on the first 3. There are some that you are just not going to run into much in the wild in political science but are worth learning about since you are probably going to be pushed to use them.\n\n\n\nLets focus on the crux of the issue when we are thinking about time series. Lets say we are interested in modeling the relationship spending and selling over time. Our naive smooth brain may tell us that we should model it like this.\n\\[\n\\text{Spending} = \\alpha_{t} + \\beta_{t} \\text{income} + \\varepsilon{t}\n\\]\nThis would be fine if we only had one time point! But as we start to add time points previous values of spending are going to tell us about future values of spending same with prior values of income. This violates our autocorrelation assumption. What ends up happening is that we can correctly estimate the slope of the regression line but our standard errors will be off. The reason this happens is that the standard error is normally calculated by taking a weighted mean of the deviations of the observed yi. What then ends up happening with this calaculation is that we are kind of adding the previous periods error plus the autoregressive order. We could theoretically correct our standard errors. However, we tend to over reject the null and the use of standard error corrections in this setting tend to point toward bigger problems beneath the surface. In our model we are kind of saying that only contemporaneous values income are the only thing that matters. What is likely the true model is something along the lines of\n\\[\n\\text{Spending} = \\alpha_{0} + \\alpha_{1} \\text{Spending}_{t-1} + \\beta_1 Income_{t} + \\varepsilon_{t}\n\\]\nWhat this implies is that our model cross sectional model is wrong because we are leaving the lag of y out of the model. Something that will propagate in our model. What this means is that \\(Spending_{t-1}\\) is going to be related to contemporaneous spending and our estimates will be unbiased and inconsistent kind of no matter what we do."
  },
  {
    "objectID": "Time-Series.html#why-bother",
    "href": "Time-Series.html#why-bother",
    "title": "Time-Series",
    "section": "",
    "text": "Generally we have some fixes when linear regression starts to break down. The problem is that they only go so far and are not neccessarily going to gain us that much insightful leverage on the data generating proces. As lots of articles argue. We should take time seriously! So how do we do that?\nWe have several families of Time Series methods:\n\nError Correction Models\nARIMA\nARCH/GARCH\nGAMS\nMachine Learning Stuff\n\nTraditionally in Political Science and Economics we tend to focus on the first 3. There are some that you are just not going to run into much in the wild in political science but are worth learning about since you are probably going to be pushed to use them."
  },
  {
    "objectID": "Time-Series.html#dynamic-regression",
    "href": "Time-Series.html#dynamic-regression",
    "title": "Time-Series",
    "section": "",
    "text": "Lets focus on the crux of the issue when we are thinking about time series. Lets say we are interested in modeling the relationship spending and selling over time. Our naive smooth brain may tell us that we should model it like this.\n\\[\n\\text{Spending} = \\alpha_{t} + \\beta_{t} \\text{income} + \\varepsilon{t}\n\\]\nThis would be fine if we only had one time point! But as we start to add time points previous values of spending are going to tell us about future values of spending same with prior values of income. This violates our autocorrelation assumption. What ends up happening is that we can correctly estimate the slope of the regression line but our standard errors will be off. The reason this happens is that the standard error is normally calculated by taking a weighted mean of the deviations of the observed yi. What then ends up happening with this calaculation is that we are kind of adding the previous periods error plus the autoregressive order. We could theoretically correct our standard errors. However, we tend to over reject the null and the use of standard error corrections in this setting tend to point toward bigger problems beneath the surface. In our model we are kind of saying that only contemporaneous values income are the only thing that matters. What is likely the true model is something along the lines of\n\\[\n\\text{Spending} = \\alpha_{0} + \\alpha_{1} \\text{Spending}_{t-1} + \\beta_1 Income_{t} + \\varepsilon_{t}\n\\]\nWhat this implies is that our model cross sectional model is wrong because we are leaving the lag of y out of the model. Something that will propagate in our model. What this means is that \\(Spending_{t-1}\\) is going to be related to contemporaneous spending and our estimates will be unbiased and inconsistent kind of no matter what we do."
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Data Viz with Matplot",
    "section": "",
    "text": "It is time to learn the grammar of matplot. Since it is the most popular library and you are going to use data viz a fair amount. In some ways matplot lib is like ggplot in a lot of other ways it very much is not. A better mental mapping in some cases is how you would do things in base R.\n\n\n\nimport polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\n# Load dataset into a Polars DataFrame\npengs = pl.from_pandas(load_penguins()).drop_nulls()\n\nInstead of simply mapping what you know onto this you should learn it for what it is. There are three layers to matplotlib that makes it quirky. Most of this is going to be taken from scientific visualization with matplotlib. In this case I am directly ripping this script from the book.\n\n\nCode\nfrom matplotlib.ticker import AutoMinorLocator, MultipleLocator, FuncFormatter\n\nnp.random.seed(123)\n\nX = np.linspace(0.5, 3.5, 100)\nY1 = 3 + np.cos(X)\nY2 = 1 + np.cos(1 + X / 0.75) / 2\nY3 = np.random.uniform(Y1, Y2, len(X))\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(1, 1, 1, aspect=1)\n\n\ndef minor_tick(x, pos):\n    if not x % 1.0:\n        return \"\"\n    return \"%.2f\" % x\n\n\nax.xaxis.set_major_locator(MultipleLocator(1.000))\nax.xaxis.set_minor_locator(AutoMinorLocator(4))\nax.yaxis.set_major_locator(MultipleLocator(1.000))\nax.yaxis.set_minor_locator(AutoMinorLocator(4))\nax.xaxis.set_minor_formatter(FuncFormatter(minor_tick))\n\nax.set_xlim(0, 4)\n\n\n(0.0, 4.0)\n\n\nCode\nax.set_ylim(0, 4)\n\n\n(0.0, 4.0)\n\n\nCode\n\nax.tick_params(which=\"major\", width=1.0)\nax.tick_params(which=\"major\", length=10)\nax.tick_params(which=\"minor\", width=1.0, labelsize=10)\nax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n\nax.grid(linestyle=\"--\", linewidth=0.5, color=\".25\", zorder=-10)\n\nax.plot(X, Y1, c=(0.25, 0.25, 1.00), lw=2, label=\"Blue signal\", zorder=10)\nax.plot(X, Y2, c=(1.00, 0.25, 0.25), lw=2, label=\"Red signal\")\nax.plot(X, Y3, linewidth=0, marker=\"o\", markerfacecolor=\"w\", markeredgecolor=\"k\")\n\nax.set_title(\"Anatomy of a figure. Part of Figure\", fontsize=20, verticalalignment=\"bottom\")\nax.set_xlabel(\"X axis label. Part of Axis\")\nax.set_ylabel(\"Y axis label. Part of Axis\")\n\nax.legend()\n\n\ndef circle(x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n\n    circle = Circle(\n        (x, y),\n        radius,\n        clip_on=False,\n        zorder=10,\n        linewidth=1,\n        edgecolor=\"black\",\n        facecolor=(0, 0, 0, 0.0125),\n        path_effects=[withStroke(linewidth=5, foreground=\"w\")],\n    )\n    ax.add_artist(circle)\n\n\ndef text(x, y, text):\n    ax.text(\n        x,\n        y,\n        text,\n        backgroundcolor=\"white\",\n        # fontname=\"Yanone Kaffeesatz\", fontsize=\"large\",\n        ha=\"center\",\n        va=\"top\",\n        weight=\"regular\",\n        color=\"#000099\",\n    )\n\n\n# Minor tick\ncircle(0.50, -0.10)\ntext(0.50, -0.32, \"Minor tick label\")\n\n# Major tick\ncircle(-0.03, 4.00)\ntext(0.03, 3.80, \"Major tick\")\n\n# Minor tick\ncircle(0.00, 3.50)\ntext(0.00, 3.30, \"Minor tick\")\n\n# Major tick label\ncircle(-0.15, 3.00)\ntext(-0.15, 2.80, \"Major tick label\")\n\n# X Label\ncircle(1.80, -0.27)\ntext(1.80, -0.45, \"X axis label\")\n\n# Y Label\ncircle(-0.27, 1.80)\ntext(-0.27, 1.6, \"Y axis label\")\n\n# Title\ncircle(1.60, 4.13)\ntext(1.60, 3.93, \"Title\")\n\n# Blue plot\ncircle(1.75, 2.80)\ntext(1.75, 2.60, \"Line\\n(line plot)\")\n\n# Red plot\ncircle(1.20, 0.60)\ntext(1.20, 0.40, \"Line\\n(line plot)\")\n\n# Scatter plot\ncircle(3.20, 1.75)\ntext(3.20, 1.55, \"Markers\\n(scatter plot)\")\n\n# Grid\ncircle(3.00, 3.00)\ntext(3.00, 2.80, \"Grid\")\n\n# Legend\ncircle(3.70, 3.80)\ntext(3.70, 3.60, \"Legend\")\n\n# Axes\ncircle(0.5, 0.5)\ntext(0.5, 0.3, \"Axes\")\n\n# Figure\ncircle(-0.3, 0.65)\ntext(-0.3, 0.45, \"Figure\")\n\ncolor = \"#000099\"\nax.annotate(\n    \"Spines\",\n    xy=(4.0, 0.35),\n    xytext=(3.3, 0.5),\n    color=color,\n    weight=\"regular\",  # fontsize=\"large\", fontname=\"Yanone Kaffeesatz\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3\", color=color),\n)\n\nax.annotate(\n    \"\",\n    xy=(3.15, 0.0),\n    xytext=(3.45, 0.45),\n    color=color,\n    weight=\"regular\",  # fontsize=\"large\", fontname=\"Yanone Kaffeesatz\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3\", color=color),\n)\n\n\n\n\n\n\n\n\n\nIt is important to note that there are several areas to that differ from ggplot. The logic differs in a lot of ways so there is no clean 1:1 translation of matplotlib and ggplot. So it worth adjudicating some of these differences\n\nFigure: In way this is just bringing the plot into existince. Kind of like when you just do ggplot. This also includes the titles, fontface of the titles\nAxes: This corresponds to the area where the data will be rendered. Sometimes refered to as the subplot. This controls the spines basically the area that is around the plot. This also includes the ticks and the actual plotting of the data.\nAxis: For some reason this is separated but these are really just the equivelent of the scales argument in ggplot\n\n\n\n\nTo initialize a plot we can just do\n\n\nfig,ax = plt.subplots(figsize=(10,6))\n\n\n\n\n\n\n\n\nfig is just a matplotlib object. We don’t neccessarily ‘add’ anything in the same way we do with ggplot. This kind of boils down to differences in R and Python. While ggplot is its own class in R and geoms have to be registered as class methods. Spirtually these all follow the functional programming paradigm. While matplotlib follows a more object oriented paradigm.\nSo instead of adding layers like below\n\nlibrary(ggplot2)\n\npengs = palmerpenguins::penguins\n\nggplot(pengs, aes(x = body_mass_g)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nWe are modifying each of the underlying objects using class methods.\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\n\n\n\n\n\n\n\n\nNotice that one of the key differences in the two plots is that when making the plot we are operating on the axis class which has the plotting ‘geometries’. Whereas in ggplot geom_histogram is a function that will create the bins, plot the data, then go and grab the labels, and then place them on the appropraite axis. So in matplot lib we would need to do somthing to this effect.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\n\n\n\n\n\n\n\n\nOne thing that comes up frequently if you are doing things itertively is that if we wanted to modidfy a plot matplot has the tendency of overlaying things on top of plots that you have previously created. We actually have to close out the plot since we interact with each of the indivual object itertively especially when working in a script.\nWe can clear/close a plot doing one of two things\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\nfig.clf()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\nplt.close(fig)\n\n\n\n\n\n\nLets take a very simple ggplot.\n\nggplot(pengs, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n    geom_point()\n\n\n\n\n\n\n\n\naes is kind of just a function. So when aes is not empty it will look through the unique values of the column and then loop over the values and plot them. Whereas in matplotlib we have to explicitly do this. Seaborn is a bit of a higher level api. Which will abstract some of this work away for you. In matplotlib we have to do this ourselves\n\n\nCode\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y=spec_data[\"body_mass_g\"],\n        c=color,\n        label=species,\n    )\n\nax.set_xlabel(\"Flipper length mm\")\nax.set_ylabel(\"Body Mass g\")\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n\n\n\n\n\n\n\n\nThis would for any number of plot types\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.hist(\n        x=spec_data[\"flipper_length_mm\"],\n        color=color,\n        label=species,\n        alpha=0.5,\n        edgecolor=\"white\",\n    )\n\nax.legend()\nax.set_xlabel(\"Flipper Length mm\")    \n\n\n\n\n\n\n\n\n\n\n\nMatplot has a lot of nice things when we start to need more bespoke transformations to the plot areas. One of the things that it lacks when compared to seaborne or ggplot. So lets take a simple plot that we tend to make a lot in demos or just as a data exploration\n\nggplot(pengs, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n    geom_point() +\n    geom_smooth()\n\n\n\n\n\n\n\n\nThe points we have already covered how to make but there are no built in class methods to matplot lib for a lowess smoother. Which kind of feels like a silly choice. To do this we first need to define a function. We then have to do the same thing\n\n\nCode\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n\ndef bootstrap_lowess(x, y, frac=0.75, it=3, n_boot=100, alpha=0.05):\n    \"\"\"\n    Bootstraps LOWESS smoothing to estimate confidence intervals.\n\n    Parameters:\n    - x, y: Input data (must be numpy arrays)\n    - frac: Smoothing span (same as ggplot default)\n    - it: Number of robustifying iterations\n    - n_boot: Number of bootstrap resamples\n    - alpha: Confidence level (default 95%)\n\n    Returns:\n    - x_sorted: Sorted x values\n    - ysm: Smoothed LOWESS estimate\n    - y_lower, y_upper: Confidence interval bounds\n    \"\"\"\n    # Sort x and y to avoid issues\n    sorted_indices = np.argsort(x)\n    x_sorted, y_sorted = x[sorted_indices], y[sorted_indices]\n\n    # Compute the original LOWESS estimate\n    smoothed = lowess(y_sorted, x_sorted, frac=frac, it=it)\n    ysm = smoothed[:, 1]  # Extract smoothed values\n\n    # Bootstrap resampling\n    y_boot = np.zeros((n_boot, len(x_sorted)))\n    for i in range(n_boot):\n        resample_idx = np.random.choice(\n            len(x_sorted), len(x_sorted), replace=True)\n        y_resampled = y_sorted[resample_idx]\n        x_resampled = x_sorted[resample_idx]\n        smoothed_boot = lowess(y_resampled, x_resampled, frac=frac, it=it)\n        y_boot[i, :] = np.interp(\n            x_sorted, smoothed_boot[:, 0], smoothed_boot[:, 1])\n\n    # Compute confidence intervals\n    lower_bound = np.percentile(y_boot, 100 * (alpha / 2), axis=0)\n    upper_bound = np.percentile(y_boot, 100 * (1 - alpha / 2), axis=0)\n\n    return x_sorted, ysm, lower_bound, upper_bound\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    x = spec_data[\"flipper_length_mm\"].to_numpy()\n    y = spec_data[\"body_mass_g\"].to_numpy()\n\n    # Sort data to avoid issues in plot and LOWESS\n    sorted_indices = np.argsort(x)\n    x_sorted, y_sorted = x[sorted_indices], y[sorted_indices]\n    x_sorted, ysm, y_lower, y_upper = bootstrap_lowess(\n        x, y, frac=0.75, it=3, n_boot=100\n    )\n\n    # Scatter plot\n    ax.scatter(x, y, c=color, label=species, alpha=0.6)\n\n    # Smoothed LOWESS line\n    ax.plot(x_sorted, ysm, color=color, linewidth=2)\n\n    # Confidence interval shading\n    ax.fill_between(x_sorted, y_lower, y_upper, color=color, alpha=0.2)\n\n\nax.set_xlabel(\"Flipper length mm\")\nax.set_ylabel(\"Body Mass g\")\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n\n\n\n\n\n\n\nSimilarly we would need to define a kernel density estimator however it is going to be a whole lot easier to let seaborn take the wheel in some cases\n\nimport seaborn as sns \n\nfig,ax = plt.subplots()\n\nsns.kdeplot(pengs, x = 'flipper_length_mm', fill = 'species', hue = 'species')\n\n\n\n\n\n\n\n\nIn the same way ols and lowess plots get a bit easier and you can maintain some of the flexibility\n\n\nfig, ax = plt.subplots(figsize = (10,6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    sns.regplot(spec_data, x = 'flipper_length_mm', y = 'body_mass_g', lowess = True, scatter = True,\n    ax = ax, color = color, label = species)\n\nax.set_xlabel('Flipper Length(mm)')\nax.set_ylabel('Body Mass(g)')\nax.legend()\n\n\n\n\n\n\n\n\nSimilarly you could do the same thing for the kernel density estimator if you are truly wedded to the matplot lib default style then you could just loop over like this\n\nfig, ax = plt.subplots(figsize = (10,6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    sns.kdeplot(spec_data, x = 'flipper_length_mm',\n    ax = ax, fill = True, color = color, label = species)\n\nax.set_xlabel('Flipper Length(mm)')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nSometimes we need small multiples to show make things look nice or not make our plots to busy. In ggplot this is fairly trivial because we don’t neccessarily need super fine tuned control over the layout. Sometimes we just need two multiples like this.\n\npengs = na.omit(pengs)\n\nggplot(pengs, aes(x = body_mass_g, fill = species)) +\n    geom_histogram() +\n    facet_wrap(vars(sex))\n\n\n\n\n\n\n\n\nIn matplotlib this is where our figure object comes in handy. We are effectively going to tell the figures that we need another plot in the same way that we use par in base R plots. One of the things that really makes matplotlib standout from ggplot is just how finetuned you can make the plots.\n\n\n\npengs = pengs.filter(pl.col('sex').is_not_null())\nsexs = pengs.unique(subset = 'sex')['sex']\n\nfig, axes = plt.subplots(nrows=1, ncols=len(sexs), figsize=(10,5))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\n\n\n\n\nfor i, sex in enumerate(sexs):\n    ax = axes[i]\n    ax.set_title(f\"Sex: {sex}\")\n    \n    for color, species_name in zip(colors, species):\n        # Filter data for the current species and sex\n        spec_data = pengs.filter((pl.col('species') == species_name) & (pl.col('sex') == sex))\n        ax.hist(spec_data['body_mass_g'], bins=30, color=color,\n                edgecolor='white', alpha=0.7, label=species_name)\n    \n    ax.set_xlabel('Body Mass (g)')\n    ax.legend()\n\n\n\n\n\n\n\n\nSimilarly we can do the same thing with our loess plots\n\npengs = pengs.filter(pl.col('sex').is_not_null())\nsexs = pengs.unique(subset = 'sex')['sex']\n\nfig, axes = plt.subplots(nrows=1, ncols=len(sexs), figsize=(10,5))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor i, sex in enumerate(sexs):\n    ax = axes[i]\n    ax.set_title(f\"Sex: {sex}\")\n    \n    for color, species_name in zip(colors, species):\n        # Filter data for the current species and sex\n        spec_data = pengs.filter((pl.col('species') == species_name) & (pl.col('sex') == sex))\n        sns.regplot(spec_data, x = 'flipper_length_mm', y = 'body_mass_g', lowess = True, scatter = True,\n        ax = ax, color = color, label = species_name)\n    \n    ax.set_xlabel('Body Mass (g)')\n    ax.legend()\n\n\n\n\n\n\n\n\n\n\n\nI think generally nobody that does data visualization like the defaults. So adding themes or customizing themes are important. Lets just work with a very simple plot and just keep making it nice.\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y = spec_data['body_mass_g'],\n        color=color,\n        label=species,\n        alpha=0.5\n    )\n\nax.legend()\nax.set_xlabel(\"Flipper Length mm\")    \n\n\n\n\n\n\n\n\nI have a little bit of a bespoke ggplot theme which is really just aligning the axis labels and customizing the fontface to be Assitant.\n\n\nplt.rcParams.update({\n    'font.family': 'Assistant'\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y = spec_data['body_mass_g'],\n        color=color,\n        label=species,\n        alpha=0.5\n    )\n\nax.legend(frameon = False, loc = 'center left', bbox_to_anchor=(1, 0.5))\nax.set_xlabel(\"Flipper Length mm\", loc = 'left', labelpad = 15)   \nax.set_ylabel('Body Mass(g)', labelpad = 15, loc = 'top')\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.tick_params(axis = 'x', which = 'both', length = 0)\nax.tick_params(axis = 'y', which = 'both', length = 0)\nax.grid(True, linestyle = 'solid', which = 'major', linewidth = 0.4)",
    "crumbs": [
      "Data Viz with Matplot"
    ]
  },
  {
    "objectID": "data-viz.html#initializing-a-plot",
    "href": "data-viz.html#initializing-a-plot",
    "title": "Data Viz with Matplot",
    "section": "",
    "text": "import polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\n# Load dataset into a Polars DataFrame\npengs = pl.from_pandas(load_penguins()).drop_nulls()\n\nInstead of simply mapping what you know onto this you should learn it for what it is. There are three layers to matplotlib that makes it quirky. Most of this is going to be taken from scientific visualization with matplotlib. In this case I am directly ripping this script from the book.\n\n\nCode\nfrom matplotlib.ticker import AutoMinorLocator, MultipleLocator, FuncFormatter\n\nnp.random.seed(123)\n\nX = np.linspace(0.5, 3.5, 100)\nY1 = 3 + np.cos(X)\nY2 = 1 + np.cos(1 + X / 0.75) / 2\nY3 = np.random.uniform(Y1, Y2, len(X))\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(1, 1, 1, aspect=1)\n\n\ndef minor_tick(x, pos):\n    if not x % 1.0:\n        return \"\"\n    return \"%.2f\" % x\n\n\nax.xaxis.set_major_locator(MultipleLocator(1.000))\nax.xaxis.set_minor_locator(AutoMinorLocator(4))\nax.yaxis.set_major_locator(MultipleLocator(1.000))\nax.yaxis.set_minor_locator(AutoMinorLocator(4))\nax.xaxis.set_minor_formatter(FuncFormatter(minor_tick))\n\nax.set_xlim(0, 4)\n\n\n(0.0, 4.0)\n\n\nCode\nax.set_ylim(0, 4)\n\n\n(0.0, 4.0)\n\n\nCode\n\nax.tick_params(which=\"major\", width=1.0)\nax.tick_params(which=\"major\", length=10)\nax.tick_params(which=\"minor\", width=1.0, labelsize=10)\nax.tick_params(which=\"minor\", length=5, labelsize=10, labelcolor=\"0.25\")\n\nax.grid(linestyle=\"--\", linewidth=0.5, color=\".25\", zorder=-10)\n\nax.plot(X, Y1, c=(0.25, 0.25, 1.00), lw=2, label=\"Blue signal\", zorder=10)\nax.plot(X, Y2, c=(1.00, 0.25, 0.25), lw=2, label=\"Red signal\")\nax.plot(X, Y3, linewidth=0, marker=\"o\", markerfacecolor=\"w\", markeredgecolor=\"k\")\n\nax.set_title(\"Anatomy of a figure. Part of Figure\", fontsize=20, verticalalignment=\"bottom\")\nax.set_xlabel(\"X axis label. Part of Axis\")\nax.set_ylabel(\"Y axis label. Part of Axis\")\n\nax.legend()\n\n\ndef circle(x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n\n    circle = Circle(\n        (x, y),\n        radius,\n        clip_on=False,\n        zorder=10,\n        linewidth=1,\n        edgecolor=\"black\",\n        facecolor=(0, 0, 0, 0.0125),\n        path_effects=[withStroke(linewidth=5, foreground=\"w\")],\n    )\n    ax.add_artist(circle)\n\n\ndef text(x, y, text):\n    ax.text(\n        x,\n        y,\n        text,\n        backgroundcolor=\"white\",\n        # fontname=\"Yanone Kaffeesatz\", fontsize=\"large\",\n        ha=\"center\",\n        va=\"top\",\n        weight=\"regular\",\n        color=\"#000099\",\n    )\n\n\n# Minor tick\ncircle(0.50, -0.10)\ntext(0.50, -0.32, \"Minor tick label\")\n\n# Major tick\ncircle(-0.03, 4.00)\ntext(0.03, 3.80, \"Major tick\")\n\n# Minor tick\ncircle(0.00, 3.50)\ntext(0.00, 3.30, \"Minor tick\")\n\n# Major tick label\ncircle(-0.15, 3.00)\ntext(-0.15, 2.80, \"Major tick label\")\n\n# X Label\ncircle(1.80, -0.27)\ntext(1.80, -0.45, \"X axis label\")\n\n# Y Label\ncircle(-0.27, 1.80)\ntext(-0.27, 1.6, \"Y axis label\")\n\n# Title\ncircle(1.60, 4.13)\ntext(1.60, 3.93, \"Title\")\n\n# Blue plot\ncircle(1.75, 2.80)\ntext(1.75, 2.60, \"Line\\n(line plot)\")\n\n# Red plot\ncircle(1.20, 0.60)\ntext(1.20, 0.40, \"Line\\n(line plot)\")\n\n# Scatter plot\ncircle(3.20, 1.75)\ntext(3.20, 1.55, \"Markers\\n(scatter plot)\")\n\n# Grid\ncircle(3.00, 3.00)\ntext(3.00, 2.80, \"Grid\")\n\n# Legend\ncircle(3.70, 3.80)\ntext(3.70, 3.60, \"Legend\")\n\n# Axes\ncircle(0.5, 0.5)\ntext(0.5, 0.3, \"Axes\")\n\n# Figure\ncircle(-0.3, 0.65)\ntext(-0.3, 0.45, \"Figure\")\n\ncolor = \"#000099\"\nax.annotate(\n    \"Spines\",\n    xy=(4.0, 0.35),\n    xytext=(3.3, 0.5),\n    color=color,\n    weight=\"regular\",  # fontsize=\"large\", fontname=\"Yanone Kaffeesatz\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3\", color=color),\n)\n\nax.annotate(\n    \"\",\n    xy=(3.15, 0.0),\n    xytext=(3.45, 0.45),\n    color=color,\n    weight=\"regular\",  # fontsize=\"large\", fontname=\"Yanone Kaffeesatz\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3\", color=color),\n)\n\n\n\n\n\n\n\n\n\nIt is important to note that there are several areas to that differ from ggplot. The logic differs in a lot of ways so there is no clean 1:1 translation of matplotlib and ggplot. So it worth adjudicating some of these differences\n\nFigure: In way this is just bringing the plot into existince. Kind of like when you just do ggplot. This also includes the titles, fontface of the titles\nAxes: This corresponds to the area where the data will be rendered. Sometimes refered to as the subplot. This controls the spines basically the area that is around the plot. This also includes the ticks and the actual plotting of the data.\nAxis: For some reason this is separated but these are really just the equivelent of the scales argument in ggplot",
    "crumbs": [
      "Data Viz with Matplot"
    ]
  },
  {
    "objectID": "data-viz.html#plotting",
    "href": "data-viz.html#plotting",
    "title": "Data Viz with Matplot",
    "section": "",
    "text": "To initialize a plot we can just do\n\n\nfig,ax = plt.subplots(figsize=(10,6))\n\n\n\n\n\n\n\n\nfig is just a matplotlib object. We don’t neccessarily ‘add’ anything in the same way we do with ggplot. This kind of boils down to differences in R and Python. While ggplot is its own class in R and geoms have to be registered as class methods. Spirtually these all follow the functional programming paradigm. While matplotlib follows a more object oriented paradigm.\nSo instead of adding layers like below\n\nlibrary(ggplot2)\n\npengs = palmerpenguins::penguins\n\nggplot(pengs, aes(x = body_mass_g)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nWe are modifying each of the underlying objects using class methods.\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\n\n\n\n\n\n\n\n\nNotice that one of the key differences in the two plots is that when making the plot we are operating on the axis class which has the plotting ‘geometries’. Whereas in ggplot geom_histogram is a function that will create the bins, plot the data, then go and grab the labels, and then place them on the appropraite axis. So in matplot lib we would need to do somthing to this effect.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\n\n\n\n\n\n\n\n\nOne thing that comes up frequently if you are doing things itertively is that if we wanted to modidfy a plot matplot has the tendency of overlaying things on top of plots that you have previously created. We actually have to close out the plot since we interact with each of the indivual object itertively especially when working in a script.\nWe can clear/close a plot doing one of two things\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\nfig.clf()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(pengs[\"body_mass_g\"], bins=30)\nax.set_xlabel(\"body_mass_g\")\nax.set_ylabel(\"count\")\nfig.suptitle(\"Distribution of Body Mass\")\nplt.close(fig)",
    "crumbs": [
      "Data Viz with Matplot"
    ]
  },
  {
    "objectID": "data-viz.html#other-elements-of-aes",
    "href": "data-viz.html#other-elements-of-aes",
    "title": "Data Viz with Matplot",
    "section": "",
    "text": "Lets take a very simple ggplot.\n\nggplot(pengs, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n    geom_point()\n\n\n\n\n\n\n\n\naes is kind of just a function. So when aes is not empty it will look through the unique values of the column and then loop over the values and plot them. Whereas in matplotlib we have to explicitly do this. Seaborn is a bit of a higher level api. Which will abstract some of this work away for you. In matplotlib we have to do this ourselves\n\n\nCode\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y=spec_data[\"body_mass_g\"],\n        c=color,\n        label=species,\n    )\n\nax.set_xlabel(\"Flipper length mm\")\nax.set_ylabel(\"Body Mass g\")\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n\n\n\n\n\n\n\n\nThis would for any number of plot types\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.hist(\n        x=spec_data[\"flipper_length_mm\"],\n        color=color,\n        label=species,\n        alpha=0.5,\n        edgecolor=\"white\",\n    )\n\nax.legend()\nax.set_xlabel(\"Flipper Length mm\")",
    "crumbs": [
      "Data Viz with Matplot"
    ]
  },
  {
    "objectID": "data-viz.html#more-complex-plots",
    "href": "data-viz.html#more-complex-plots",
    "title": "Data Viz with Matplot",
    "section": "",
    "text": "Matplot has a lot of nice things when we start to need more bespoke transformations to the plot areas. One of the things that it lacks when compared to seaborne or ggplot. So lets take a simple plot that we tend to make a lot in demos or just as a data exploration\n\nggplot(pengs, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n    geom_point() +\n    geom_smooth()\n\n\n\n\n\n\n\n\nThe points we have already covered how to make but there are no built in class methods to matplot lib for a lowess smoother. Which kind of feels like a silly choice. To do this we first need to define a function. We then have to do the same thing\n\n\nCode\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n\ndef bootstrap_lowess(x, y, frac=0.75, it=3, n_boot=100, alpha=0.05):\n    \"\"\"\n    Bootstraps LOWESS smoothing to estimate confidence intervals.\n\n    Parameters:\n    - x, y: Input data (must be numpy arrays)\n    - frac: Smoothing span (same as ggplot default)\n    - it: Number of robustifying iterations\n    - n_boot: Number of bootstrap resamples\n    - alpha: Confidence level (default 95%)\n\n    Returns:\n    - x_sorted: Sorted x values\n    - ysm: Smoothed LOWESS estimate\n    - y_lower, y_upper: Confidence interval bounds\n    \"\"\"\n    # Sort x and y to avoid issues\n    sorted_indices = np.argsort(x)\n    x_sorted, y_sorted = x[sorted_indices], y[sorted_indices]\n\n    # Compute the original LOWESS estimate\n    smoothed = lowess(y_sorted, x_sorted, frac=frac, it=it)\n    ysm = smoothed[:, 1]  # Extract smoothed values\n\n    # Bootstrap resampling\n    y_boot = np.zeros((n_boot, len(x_sorted)))\n    for i in range(n_boot):\n        resample_idx = np.random.choice(\n            len(x_sorted), len(x_sorted), replace=True)\n        y_resampled = y_sorted[resample_idx]\n        x_resampled = x_sorted[resample_idx]\n        smoothed_boot = lowess(y_resampled, x_resampled, frac=frac, it=it)\n        y_boot[i, :] = np.interp(\n            x_sorted, smoothed_boot[:, 0], smoothed_boot[:, 1])\n\n    # Compute confidence intervals\n    lower_bound = np.percentile(y_boot, 100 * (alpha / 2), axis=0)\n    upper_bound = np.percentile(y_boot, 100 * (1 - alpha / 2), axis=0)\n\n    return x_sorted, ysm, lower_bound, upper_bound\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    x = spec_data[\"flipper_length_mm\"].to_numpy()\n    y = spec_data[\"body_mass_g\"].to_numpy()\n\n    # Sort data to avoid issues in plot and LOWESS\n    sorted_indices = np.argsort(x)\n    x_sorted, y_sorted = x[sorted_indices], y[sorted_indices]\n    x_sorted, ysm, y_lower, y_upper = bootstrap_lowess(\n        x, y, frac=0.75, it=3, n_boot=100\n    )\n\n    # Scatter plot\n    ax.scatter(x, y, c=color, label=species, alpha=0.6)\n\n    # Smoothed LOWESS line\n    ax.plot(x_sorted, ysm, color=color, linewidth=2)\n\n    # Confidence interval shading\n    ax.fill_between(x_sorted, y_lower, y_upper, color=color, alpha=0.2)\n\n\nax.set_xlabel(\"Flipper length mm\")\nax.set_ylabel(\"Body Mass g\")\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n\n\n\n\n\n\n\n\nSimilarly we would need to define a kernel density estimator however it is going to be a whole lot easier to let seaborn take the wheel in some cases\n\nimport seaborn as sns \n\nfig,ax = plt.subplots()\n\nsns.kdeplot(pengs, x = 'flipper_length_mm', fill = 'species', hue = 'species')\n\n\n\n\n\n\n\n\nIn the same way ols and lowess plots get a bit easier and you can maintain some of the flexibility\n\n\nfig, ax = plt.subplots(figsize = (10,6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    sns.regplot(spec_data, x = 'flipper_length_mm', y = 'body_mass_g', lowess = True, scatter = True,\n    ax = ax, color = color, label = species)\n\nax.set_xlabel('Flipper Length(mm)')\nax.set_ylabel('Body Mass(g)')\nax.legend()\n\n\n\n\n\n\n\n\nSimilarly you could do the same thing for the kernel density estimator if you are truly wedded to the matplot lib default style then you could just loop over like this\n\nfig, ax = plt.subplots(figsize = (10,6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    sns.kdeplot(spec_data, x = 'flipper_length_mm',\n    ax = ax, fill = True, color = color, label = species)\n\nax.set_xlabel('Flipper Length(mm)')\nax.legend()",
    "crumbs": [
      "Data Viz with Matplot"
    ]
  },
  {
    "objectID": "data-viz.html#small-multiples",
    "href": "data-viz.html#small-multiples",
    "title": "Data Viz with Matplot",
    "section": "",
    "text": "Sometimes we need small multiples to show make things look nice or not make our plots to busy. In ggplot this is fairly trivial because we don’t neccessarily need super fine tuned control over the layout. Sometimes we just need two multiples like this.\n\npengs = na.omit(pengs)\n\nggplot(pengs, aes(x = body_mass_g, fill = species)) +\n    geom_histogram() +\n    facet_wrap(vars(sex))\n\n\n\n\n\n\n\n\nIn matplotlib this is where our figure object comes in handy. We are effectively going to tell the figures that we need another plot in the same way that we use par in base R plots. One of the things that really makes matplotlib standout from ggplot is just how finetuned you can make the plots.\n\n\n\npengs = pengs.filter(pl.col('sex').is_not_null())\nsexs = pengs.unique(subset = 'sex')['sex']\n\nfig, axes = plt.subplots(nrows=1, ncols=len(sexs), figsize=(10,5))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\n\n\n\n\nfor i, sex in enumerate(sexs):\n    ax = axes[i]\n    ax.set_title(f\"Sex: {sex}\")\n    \n    for color, species_name in zip(colors, species):\n        # Filter data for the current species and sex\n        spec_data = pengs.filter((pl.col('species') == species_name) & (pl.col('sex') == sex))\n        ax.hist(spec_data['body_mass_g'], bins=30, color=color,\n                edgecolor='white', alpha=0.7, label=species_name)\n    \n    ax.set_xlabel('Body Mass (g)')\n    ax.legend()\n\n\n\n\n\n\n\n\nSimilarly we can do the same thing with our loess plots\n\npengs = pengs.filter(pl.col('sex').is_not_null())\nsexs = pengs.unique(subset = 'sex')['sex']\n\nfig, axes = plt.subplots(nrows=1, ncols=len(sexs), figsize=(10,5))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor i, sex in enumerate(sexs):\n    ax = axes[i]\n    ax.set_title(f\"Sex: {sex}\")\n    \n    for color, species_name in zip(colors, species):\n        # Filter data for the current species and sex\n        spec_data = pengs.filter((pl.col('species') == species_name) & (pl.col('sex') == sex))\n        sns.regplot(spec_data, x = 'flipper_length_mm', y = 'body_mass_g', lowess = True, scatter = True,\n        ax = ax, color = color, label = species_name)\n    \n    ax.set_xlabel('Body Mass (g)')\n    ax.legend()",
    "crumbs": [
      "Data Viz with Matplot"
    ]
  },
  {
    "objectID": "data-viz.html#customizing-appearences",
    "href": "data-viz.html#customizing-appearences",
    "title": "Data Viz with Matplot",
    "section": "",
    "text": "I think generally nobody that does data visualization like the defaults. So adding themes or customizing themes are important. Lets just work with a very simple plot and just keep making it nice.\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y = spec_data['body_mass_g'],\n        color=color,\n        label=species,\n        alpha=0.5\n    )\n\nax.legend()\nax.set_xlabel(\"Flipper Length mm\")    \n\n\n\n\n\n\n\n\nI have a little bit of a bespoke ggplot theme which is really just aligning the axis labels and customizing the fontface to be Assitant.\n\n\nplt.rcParams.update({\n    'font.family': 'Assistant'\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = [\"blue\", \"red\", \"green\"]\n\nspecies = pengs.unique(subset=\"species\")[\"species\"]\n\n\nfor color, species in zip(colors, species):\n    spec_data = pengs.filter(pl.col(\"species\") == species)\n    ax.scatter(\n        x=spec_data[\"flipper_length_mm\"],\n        y = spec_data['body_mass_g'],\n        color=color,\n        label=species,\n        alpha=0.5\n    )\n\nax.legend(frameon = False, loc = 'center left', bbox_to_anchor=(1, 0.5))\nax.set_xlabel(\"Flipper Length mm\", loc = 'left', labelpad = 15)   \nax.set_ylabel('Body Mass(g)', labelpad = 15, loc = 'top')\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.tick_params(axis = 'x', which = 'both', length = 0)\nax.tick_params(axis = 'y', which = 'both', length = 0)\nax.grid(True, linestyle = 'solid', which = 'major', linewidth = 0.4)",
    "crumbs": [
      "Data Viz with Matplot"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Baby Stats",
    "section": "",
    "text": "We have our general sense of probabilities where we are just the number of times things occur. This is kind of helpful.\n\nimport numpy as np\nfrom scipy import stats as stats\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nimport polars as pl\n\nnumbs = np.array([1, 3, 4, 3])\n\n1 / numbs.sum()\n\nnp.float64(0.09090909090909091)\n\n\nHowever, for the most part in the real world we don’t neccessarily care about the probability of a single event happening unconditionally. Conditioanl probability is genereallly a little weird but not totally different than just counting.\n\n\n\nThere are two ways we generally think of basic condiitonal probability Frequentistly and Bayesianly. For the most part these are fairly similar. The key difference is how we incorporate what we know about the world.\n\n\nIn Frequentism people often say that we don’t impose any priors on the data. This is not true in any real sense because we impose a flat prior. So the prior is kind of incorporated for us. Typically we see conditional probabilities expressed in two ways\n\\[\n\\text{P(A|B)} = \\frac{\\text{Probability of A and B Hapenning}}{\\text{Probability of B happening}} \\\\\\\n\\text{P(A|B)} = \\frac{\\text{P(A)} \\cap \\text{P(B)}}{\\text{P(B)}}\n\\]\nThis is a bit hard to visualize so lets take an example\n\nsenators.glimpse()\n\nRows: 6\nColumns: 3\n$ party  &lt;str&gt; 'Democrats', 'Republican', 'Independents', 'Democrats', 'Republican', 'Independents'\n$ gender &lt;str&gt; 'Men', 'Men', 'Men', 'Woman', 'Woman', 'Woman'\n$ total  &lt;i64&gt; 33, 40, 2, 15, 9, 1\n\n\n\nSo if we wanted to calculate the conditional probability of drawing a female democratic senator we would really just do this.\n\nwomand_democrat = senators.filter((pl.col('gender') == 'Woman') & (pl.col('party') == 'Democrats'))['total']\n\ndemocrat = senators.filter(pl.col('party') == 'Democrats').with_columns(total = pl.col('total').sum())['total'][0]\n\ndenom = 100\n\n(womand_democrat/100)/(democrat/100)\n\n\nshape: (1,)\n\n\n\ntotal\n\n\nf64\n\n\n\n\n0.3125\n\n\n\n\n\n\nSo this will roughly get us that the odds of drawing a female democratic senator is 0.31.\n\n\n\nBayesians view conditional probability in a slightly different way. But as Richard McCelreath argues that isn’t entirely true in this canned example. Typically we define Bayes Rule as something along these lines.\n\\[\\begin{align}\n\n\\text{P(A|B)} = \\frac{\\text{P(A and B)} \\times \\text{P(B)}}{P(A)}\n\n\\end{align}\\]\nHowever as Rich points out there is nothing uniquely Bayesian about this example. We could theoretically rewrite this to just plug in the numbers into the classic frequentist conditional probability statement. The canoncial example is some sort of testing framework. In statistical rethinking they use “Vampirism”\n\nprob_vampire_positive = 0.95 \n\nprob_positive_mortal = 0.01\n\npr_vamp = 0.001 \n\npr_positive = prob_vampire_positive * pr_vamp + prob_positive_mortal * (1-pr_vamp)\n\nprob_vampire_positive * pr_vamp /  pr_positive\n\n0.08683729433272395\n\n\n\n\n\nThe richness of Bayesian statistics come from using our expertise and updating them in light of data. Informally we can think of a prior as something we know about the data or data generating process before actually seeing. If we know that height in American males is about 5’9 we can write our prior as\n\\[\n\\begin{align}\n\\text{Male Height} \\sim N(\\mu, \\sigma) \\\\\n                        N(70.8, 1)\n\\end{align}\n\\]\nwhere we assume height is normally distributed around 70.8 and a variance of an inch. We can make this prior more restrictive by changing the standard deviation to a smaller value or greedier by increasing it. This works for all kinds of data. Say we want to formalize our prior on the Chiefs getting a questionable call. We could write the prior as a binomial distribution\n\\[\n\\begin{align}\n\\text{Chiefs Get Questionable call} \\sim Binomial(n, \\pi) \\\\\\\n                                         Binomial(n, .6)\n\\end{align}\n\\]\nA posterior is really just when our prior meets our data. Our posterior distribution contains every unique combination of data, likelihood, parameters, and our prior.\nWhat makes Bayesianism interesting is when we start updating the posterior distribution. Under the hood we need integral calculus to do this but hand deriving that either on paper or from scratch. Really what we are kind of doing when we run this through a MCMC is that we are sampling from our posterior distribution and counting the frequencies that something occurs. I am going to use the counting water example\n\n\nCode\ndef calculate_n_ways_possible(observations: str, n_water: int, resolution: int = 4):\n    \"\"\"\n    Calculate the number of ways to observing water ('W') given the toss of a globe\n    with `resolution` number of sides and `n_water` faces.\n    \n    Note: this method results in numerical precision issues (due to the product) when the\n    resolution of 16 or so, depending on your system.\n    \"\"\"\n    assert n_water &lt;= resolution\n    \n    # Convert observation string to an array\n    observations = np.array(list(observations.upper()))\n    \n    # Create n-sided globe with possible outcomes\n    possible = np.array(list(\"L\" * (resolution - n_water)) + list(\"W\" * n_water))\n    \n    # Tally up ways to obtain each observation given the possible outcomes\n    # Here we use brute-force, but we could also use the analytical solution below\n    ways = []\n    for obs in observations:\n        ways.append((possible == obs).sum())\n    \n    p_water = n_water / resolution\n    # perform product in log space for numerical precision\n    n_ways = np.round(np.exp(np.sum(np.log(ways)))).astype(int)\n    return n_ways, p_water\n\n\ndef run_globe_tossing_simulation(observations, resolution, current_n_possible_ways=None):\n    \"\"\"Simulate the number of ways you can observe water ('W') for a globe of `resolution`\n    sides, varying the proportion of the globe that is covered by water.\n    \"\"\"\n    # For Bayesian updates\n    current_n_possible_ways = current_n_possible_ways if current_n_possible_ways is not None else np.array([])\n    \n    print(f\"Observations: '{observations}'\")\n    p_water = np.array([])\n    for n_W in range(0, resolution + 1):\n        n_L = resolution - n_W\n        globe_sides = \"W\" * n_W + \"L\" * n_L\n        n_possible_ways, p_water_ = calculate_n_ways_possible(observations, n_water=n_W, resolution=resolution)\n        print(f\"({n_W+1}) {globe_sides} p(W) = {p_water_:1.2}\\t\\t{n_possible_ways} Ways to Produce\")\n\n        p_water = np.append(p_water, p_water_)\n        current_n_possible_ways = np.append(current_n_possible_ways, n_possible_ways)\n\n    return current_n_possible_ways, p_water\n\nfrom pprint import pprint\nnp.random.seed(1)\ndef simulate_globe_toss(p: float = 0.7, N: int = 9) -&gt; list[str]:\n    \"\"\"Simulate N globe tosses with a specific/known proportion\n    p: float\n        The propotion of water\n    N: int\n        Number of globe tosses\n    \"\"\"\n    return np.random.choice(list(\"WL\"),  size=N, p=np.array([p, 1-p]), replace=True)\n\n\n\nRESOLUTION = 4\nobservations = \"WLW\"\nn_possible_ways, p_water = run_globe_tossing_simulation(observations, resolution=RESOLUTION)\n\nObservations: 'WLW'\n(1) LLLL p(W) = 0.0     0 Ways to Produce\n(2) WLLL p(W) = 0.25        3 Ways to Produce\n(3) WWLL p(W) = 0.5     8 Ways to Produce\n(4) WWWL p(W) = 0.75        9 Ways to Produce\n(5) WWWW p(W) = 1.0     0 Ways to Produce\n\n\nThis will spit out the number of ways we can produce various draws. Now lets simulate the number of globe tosses wher we are just adding tosses.\n\n\nCode\nfrom scipy.special import factorial\n\ndef beta_posterior(n_W: int, n_L: int, p: float) -&gt; float:\n    \"\"\"Calculates the beta posterior over proportions `p` given a set of\n    `N_W` water and `N_L` land observations\n    \"\"\"\n    return factorial(n_W + n_L + 1) / (factorial(n_W) * factorial(n_L)) * p ** n_W * (1-p) ** n_L\n\n\ndef plot_beta_posterior_from_observations(observations: str, resolution: int = 50, **plot_kwargs) -&gt; None:\n    \"\"\"Calculates and plots the beta posterior for a string of observations\"\"\"\n    n_W = len(observations.replace(\"L\", \"\"))\n    n_L = len(observations) - n_W\n    proportions = np.linspace(0, 1, resolution)\n        \n    probs = beta_posterior(n_W, n_L, proportions)\n    plt.plot(proportions, probs, **plot_kwargs)\n    plt.yticks([])\n    plt.title(observations)\n    \n\n# Tossing the globe\nobservations = \"WLWWWLWLW\"\nfig, axs = plt.subplots(3, 3, figsize=(8, 8))\nfor ii in range(9):\n    ax = axs[ii // 3][ii % 3]\n    plt.sca(ax)\n    # Plot previous\n    if ii &gt; 0:\n        plot_beta_posterior_from_observations(observations[:ii], color='k', linestyle='--')\n    else:\n        # First observation, no previous data\n        plot_beta_posterior_from_observations('', color='k', linestyle='--')\n        \n    color = 'C1' if observations[ii] == 'W' else 'C0'\n    plot_beta_posterior_from_observations(observations[:ii+1], color=color, linewidth=4, alpha=.5)\n    \n    if not ii % 3:\n        plt.ylabel(\"posterior probability\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne component of interviews is that we should have some intuition on counting. Like not 1,2,3 etc but how many possible combinations of things can there be aka permutations and combinations. It has been awhile since you have had to do this so it is worth going over. Permutations care about about the unique order that things can be paired in. While combinations are order agnostic. Lets say that we are trying to figure out the number of possible ways that we can seat guests at various tables. That would be a combination. For your own sake lets number all the seats. A permutation would care about the number of unique ways we can arrange people while taking the seat numbers available. What is underriding this whole thing is factorials and orders. So the formulas look something like this. Where n is the number of items and k is the number of items to arrange\n\\[\\begin{align}\n\\text{permutation} = \\frac{n!}{(n-k)!} \\\\\n\n\\text{combination} = \\frac{n!}{k!(n-k)!}\n\n\\end{align}\n\n\\]\nfactorials are just a simplfying way of writing out something like this. Where we are progresively multiplying 1 by 1 to 2 * 1 then so on and so forth.\n\ndef factorial(n):\n\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\nprint(factorial(n = 5))\n\n120\n\n\nLets say we wanted to figure out the number of possible table combinations. Lets say we have 20 people and 5 tables. So we would do something like this.\n\nimport math \n\nmath.factorial(20)//(math.factorial(5) * math.factorial(20 - 5))\n\n15504\n\n\nHowever this is not really how wedding seating works. Order matters and who sits next to who matters. So we need to take that into account. In instead of multiplying by the factorial of k we do something like this with permutations\n\nmath.factorial(20)//(factorial(20-5))\n\n1860480\n\n\n\n\n\nShe is genuinely one of the most coolest concepts in statistics. Bascially no matter the distribution of the variablee as we take more and more samples of the data as N increases we are going to converge to a normal distribution. This is really powerful concept in frequentist statistics because we are making inferences about the population using samples.\nEffectively we can take any distribution that we want and take an infinite number of samples from it and it will be a normal distribution. Lets take our trusty dusty old uniform distribution\n\npop = np.random.uniform(0,100, size = 5000)\n\n\n\nfig, ax = plt.subplots(figsize = (12,8))\n\nax.hist(pop, edgecolor = 'white')\n\nplt.show()\n\n\n\n\n\n\n\n\nThen lets compare what happens when we take increasingly larger sample sizes.\n\nsamp_sizes = [10, 100, 500, 1000]\n\nsamps = {}\n\nfor i in samp_sizes:\n    sample_means = [\n        np.random.choice(pop, size=samp, replace=True) for samp in samp_sizes\n    ]\n    samps[f\"sample_size{i}\"] = sample_means\n\n\ndf = (\n    pl.DataFrame(samps)\n    .unpivot(value_name=\"vals\", variable_name=\"sample_size\")\n    .explode(\"vals\")\n)\n\n(ggplot(df, aes(x=\"vals\"))\n  + geom_histogram(bins = 30) \n  + facet_wrap(\"sample_size\"))\n\n\n\n\n\n\n\n\n\n\n\nProbability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses.\nProbability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5.\nThe likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low."
  },
  {
    "objectID": "probability.html#probability",
    "href": "probability.html#probability",
    "title": "Baby Stats",
    "section": "",
    "text": "We have our general sense of probabilities where we are just the number of times things occur. This is kind of helpful.\n\nimport numpy as np\nfrom scipy import stats as stats\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nimport polars as pl\n\nnumbs = np.array([1, 3, 4, 3])\n\n1 / numbs.sum()\n\nnp.float64(0.09090909090909091)\n\n\nHowever, for the most part in the real world we don’t neccessarily care about the probability of a single event happening unconditionally. Conditioanl probability is genereallly a little weird but not totally different than just counting."
  },
  {
    "objectID": "probability.html#conditional-probability",
    "href": "probability.html#conditional-probability",
    "title": "Baby Stats",
    "section": "",
    "text": "There are two ways we generally think of basic condiitonal probability Frequentistly and Bayesianly. For the most part these are fairly similar. The key difference is how we incorporate what we know about the world.\n\n\nIn Frequentism people often say that we don’t impose any priors on the data. This is not true in any real sense because we impose a flat prior. So the prior is kind of incorporated for us. Typically we see conditional probabilities expressed in two ways\n\\[\n\\text{P(A|B)} = \\frac{\\text{Probability of A and B Hapenning}}{\\text{Probability of B happening}} \\\\\\\n\\text{P(A|B)} = \\frac{\\text{P(A)} \\cap \\text{P(B)}}{\\text{P(B)}}\n\\]\nThis is a bit hard to visualize so lets take an example\n\nsenators.glimpse()\n\nRows: 6\nColumns: 3\n$ party  &lt;str&gt; 'Democrats', 'Republican', 'Independents', 'Democrats', 'Republican', 'Independents'\n$ gender &lt;str&gt; 'Men', 'Men', 'Men', 'Woman', 'Woman', 'Woman'\n$ total  &lt;i64&gt; 33, 40, 2, 15, 9, 1\n\n\n\nSo if we wanted to calculate the conditional probability of drawing a female democratic senator we would really just do this.\n\nwomand_democrat = senators.filter((pl.col('gender') == 'Woman') & (pl.col('party') == 'Democrats'))['total']\n\ndemocrat = senators.filter(pl.col('party') == 'Democrats').with_columns(total = pl.col('total').sum())['total'][0]\n\ndenom = 100\n\n(womand_democrat/100)/(democrat/100)\n\n\nshape: (1,)\n\n\n\ntotal\n\n\nf64\n\n\n\n\n0.3125\n\n\n\n\n\n\nSo this will roughly get us that the odds of drawing a female democratic senator is 0.31.\n\n\n\nBayesians view conditional probability in a slightly different way. But as Richard McCelreath argues that isn’t entirely true in this canned example. Typically we define Bayes Rule as something along these lines.\n\\[\\begin{align}\n\n\\text{P(A|B)} = \\frac{\\text{P(A and B)} \\times \\text{P(B)}}{P(A)}\n\n\\end{align}\\]\nHowever as Rich points out there is nothing uniquely Bayesian about this example. We could theoretically rewrite this to just plug in the numbers into the classic frequentist conditional probability statement. The canoncial example is some sort of testing framework. In statistical rethinking they use “Vampirism”\n\nprob_vampire_positive = 0.95 \n\nprob_positive_mortal = 0.01\n\npr_vamp = 0.001 \n\npr_positive = prob_vampire_positive * pr_vamp + prob_positive_mortal * (1-pr_vamp)\n\nprob_vampire_positive * pr_vamp /  pr_positive\n\n0.08683729433272395\n\n\n\n\n\nThe richness of Bayesian statistics come from using our expertise and updating them in light of data. Informally we can think of a prior as something we know about the data or data generating process before actually seeing. If we know that height in American males is about 5’9 we can write our prior as\n\\[\n\\begin{align}\n\\text{Male Height} \\sim N(\\mu, \\sigma) \\\\\n                        N(70.8, 1)\n\\end{align}\n\\]\nwhere we assume height is normally distributed around 70.8 and a variance of an inch. We can make this prior more restrictive by changing the standard deviation to a smaller value or greedier by increasing it. This works for all kinds of data. Say we want to formalize our prior on the Chiefs getting a questionable call. We could write the prior as a binomial distribution\n\\[\n\\begin{align}\n\\text{Chiefs Get Questionable call} \\sim Binomial(n, \\pi) \\\\\\\n                                         Binomial(n, .6)\n\\end{align}\n\\]\nA posterior is really just when our prior meets our data. Our posterior distribution contains every unique combination of data, likelihood, parameters, and our prior.\nWhat makes Bayesianism interesting is when we start updating the posterior distribution. Under the hood we need integral calculus to do this but hand deriving that either on paper or from scratch. Really what we are kind of doing when we run this through a MCMC is that we are sampling from our posterior distribution and counting the frequencies that something occurs. I am going to use the counting water example\n\n\nCode\ndef calculate_n_ways_possible(observations: str, n_water: int, resolution: int = 4):\n    \"\"\"\n    Calculate the number of ways to observing water ('W') given the toss of a globe\n    with `resolution` number of sides and `n_water` faces.\n    \n    Note: this method results in numerical precision issues (due to the product) when the\n    resolution of 16 or so, depending on your system.\n    \"\"\"\n    assert n_water &lt;= resolution\n    \n    # Convert observation string to an array\n    observations = np.array(list(observations.upper()))\n    \n    # Create n-sided globe with possible outcomes\n    possible = np.array(list(\"L\" * (resolution - n_water)) + list(\"W\" * n_water))\n    \n    # Tally up ways to obtain each observation given the possible outcomes\n    # Here we use brute-force, but we could also use the analytical solution below\n    ways = []\n    for obs in observations:\n        ways.append((possible == obs).sum())\n    \n    p_water = n_water / resolution\n    # perform product in log space for numerical precision\n    n_ways = np.round(np.exp(np.sum(np.log(ways)))).astype(int)\n    return n_ways, p_water\n\n\ndef run_globe_tossing_simulation(observations, resolution, current_n_possible_ways=None):\n    \"\"\"Simulate the number of ways you can observe water ('W') for a globe of `resolution`\n    sides, varying the proportion of the globe that is covered by water.\n    \"\"\"\n    # For Bayesian updates\n    current_n_possible_ways = current_n_possible_ways if current_n_possible_ways is not None else np.array([])\n    \n    print(f\"Observations: '{observations}'\")\n    p_water = np.array([])\n    for n_W in range(0, resolution + 1):\n        n_L = resolution - n_W\n        globe_sides = \"W\" * n_W + \"L\" * n_L\n        n_possible_ways, p_water_ = calculate_n_ways_possible(observations, n_water=n_W, resolution=resolution)\n        print(f\"({n_W+1}) {globe_sides} p(W) = {p_water_:1.2}\\t\\t{n_possible_ways} Ways to Produce\")\n\n        p_water = np.append(p_water, p_water_)\n        current_n_possible_ways = np.append(current_n_possible_ways, n_possible_ways)\n\n    return current_n_possible_ways, p_water\n\nfrom pprint import pprint\nnp.random.seed(1)\ndef simulate_globe_toss(p: float = 0.7, N: int = 9) -&gt; list[str]:\n    \"\"\"Simulate N globe tosses with a specific/known proportion\n    p: float\n        The propotion of water\n    N: int\n        Number of globe tosses\n    \"\"\"\n    return np.random.choice(list(\"WL\"),  size=N, p=np.array([p, 1-p]), replace=True)\n\n\n\nRESOLUTION = 4\nobservations = \"WLW\"\nn_possible_ways, p_water = run_globe_tossing_simulation(observations, resolution=RESOLUTION)\n\nObservations: 'WLW'\n(1) LLLL p(W) = 0.0     0 Ways to Produce\n(2) WLLL p(W) = 0.25        3 Ways to Produce\n(3) WWLL p(W) = 0.5     8 Ways to Produce\n(4) WWWL p(W) = 0.75        9 Ways to Produce\n(5) WWWW p(W) = 1.0     0 Ways to Produce\n\n\nThis will spit out the number of ways we can produce various draws. Now lets simulate the number of globe tosses wher we are just adding tosses.\n\n\nCode\nfrom scipy.special import factorial\n\ndef beta_posterior(n_W: int, n_L: int, p: float) -&gt; float:\n    \"\"\"Calculates the beta posterior over proportions `p` given a set of\n    `N_W` water and `N_L` land observations\n    \"\"\"\n    return factorial(n_W + n_L + 1) / (factorial(n_W) * factorial(n_L)) * p ** n_W * (1-p) ** n_L\n\n\ndef plot_beta_posterior_from_observations(observations: str, resolution: int = 50, **plot_kwargs) -&gt; None:\n    \"\"\"Calculates and plots the beta posterior for a string of observations\"\"\"\n    n_W = len(observations.replace(\"L\", \"\"))\n    n_L = len(observations) - n_W\n    proportions = np.linspace(0, 1, resolution)\n        \n    probs = beta_posterior(n_W, n_L, proportions)\n    plt.plot(proportions, probs, **plot_kwargs)\n    plt.yticks([])\n    plt.title(observations)\n    \n\n# Tossing the globe\nobservations = \"WLWWWLWLW\"\nfig, axs = plt.subplots(3, 3, figsize=(8, 8))\nfor ii in range(9):\n    ax = axs[ii // 3][ii % 3]\n    plt.sca(ax)\n    # Plot previous\n    if ii &gt; 0:\n        plot_beta_posterior_from_observations(observations[:ii], color='k', linestyle='--')\n    else:\n        # First observation, no previous data\n        plot_beta_posterior_from_observations('', color='k', linestyle='--')\n        \n    color = 'C1' if observations[ii] == 'W' else 'C0'\n    plot_beta_posterior_from_observations(observations[:ii+1], color=color, linewidth=4, alpha=.5)\n    \n    if not ii % 3:\n        plt.ylabel(\"posterior probability\")"
  },
  {
    "objectID": "probability.html#counting",
    "href": "probability.html#counting",
    "title": "Baby Stats",
    "section": "",
    "text": "One component of interviews is that we should have some intuition on counting. Like not 1,2,3 etc but how many possible combinations of things can there be aka permutations and combinations. It has been awhile since you have had to do this so it is worth going over. Permutations care about about the unique order that things can be paired in. While combinations are order agnostic. Lets say that we are trying to figure out the number of possible ways that we can seat guests at various tables. That would be a combination. For your own sake lets number all the seats. A permutation would care about the number of unique ways we can arrange people while taking the seat numbers available. What is underriding this whole thing is factorials and orders. So the formulas look something like this. Where n is the number of items and k is the number of items to arrange\n\\[\\begin{align}\n\\text{permutation} = \\frac{n!}{(n-k)!} \\\\\n\n\\text{combination} = \\frac{n!}{k!(n-k)!}\n\n\\end{align}\n\n\\]\nfactorials are just a simplfying way of writing out something like this. Where we are progresively multiplying 1 by 1 to 2 * 1 then so on and so forth.\n\ndef factorial(n):\n\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\nprint(factorial(n = 5))\n\n120\n\n\nLets say we wanted to figure out the number of possible table combinations. Lets say we have 20 people and 5 tables. So we would do something like this.\n\nimport math \n\nmath.factorial(20)//(math.factorial(5) * math.factorial(20 - 5))\n\n15504\n\n\nHowever this is not really how wedding seating works. Order matters and who sits next to who matters. So we need to take that into account. In instead of multiplying by the factorial of k we do something like this with permutations\n\nmath.factorial(20)//(factorial(20-5))\n\n1860480"
  },
  {
    "objectID": "probability.html#the-centeral-limit-theorum",
    "href": "probability.html#the-centeral-limit-theorum",
    "title": "Baby Stats",
    "section": "",
    "text": "She is genuinely one of the most coolest concepts in statistics. Bascially no matter the distribution of the variablee as we take more and more samples of the data as N increases we are going to converge to a normal distribution. This is really powerful concept in frequentist statistics because we are making inferences about the population using samples.\nEffectively we can take any distribution that we want and take an infinite number of samples from it and it will be a normal distribution. Lets take our trusty dusty old uniform distribution\n\npop = np.random.uniform(0,100, size = 5000)\n\n\n\nfig, ax = plt.subplots(figsize = (12,8))\n\nax.hist(pop, edgecolor = 'white')\n\nplt.show()\n\n\n\n\n\n\n\n\nThen lets compare what happens when we take increasingly larger sample sizes.\n\nsamp_sizes = [10, 100, 500, 1000]\n\nsamps = {}\n\nfor i in samp_sizes:\n    sample_means = [\n        np.random.choice(pop, size=samp, replace=True) for samp in samp_sizes\n    ]\n    samps[f\"sample_size{i}\"] = sample_means\n\n\ndf = (\n    pl.DataFrame(samps)\n    .unpivot(value_name=\"vals\", variable_name=\"sample_size\")\n    .explode(\"vals\")\n)\n\n(ggplot(df, aes(x=\"vals\"))\n  + geom_histogram(bins = 30) \n  + facet_wrap(\"sample_size\"))"
  },
  {
    "objectID": "probability.html#explain-the-difference-between-probability-and-likelihood",
    "href": "probability.html#explain-the-difference-between-probability-and-likelihood",
    "title": "Baby Stats",
    "section": "",
    "text": "Probability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses.\nProbability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5.\nThe likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low."
  },
  {
    "objectID": "tree-based-methods.html",
    "href": "tree-based-methods.html",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "For a variety of regression and classification tasks we can use what was once the state of the art and that is random forrests\n\n\nCode\nimport polars as pl \nimport polars.selectors as cs\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.tree import (DecisionTreeClassifier as DTC,\n                          DecisionTreeRegressor as DTR,\n                          plot_tree,\n                          export_text)\nfrom sklearn.metrics import (accuracy_score,\n                             log_loss)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay                            \nfrom sklearn.ensemble import \\\n     (RandomForestRegressor as RF,\n      GradientBoostingRegressor as GBR)\nimport arviz as az\nfrom great_tables import GT\n\ncarseats = pl.read_csv('data/Carseats.csv').with_columns(\n    pl.when(pl.col('Sales') &gt; 0).then(pl.lit('Yes')).otherwise(pl.lit('No')).alias('high')\n)\n\n\n\n\nUnderriding the entire idea of Bagging and Boosting are regression trees.. Regression trees have a lot of math behind an inherently kind of simple and beautifully dumb idea.\nWhen we fit a single tree there are a set of rules that we give it to make decisions about the data if a variable hits that rule then depending on what side of the line it is then it will split the data off into the left or right side. In the example above when income is not greater than or equal to 24.5 we split it into the false category an then it stops. We have satisfied the criteria. In the true column we then repeat the proces however when we hit the false column again we have a new criterion and that is how much there is on advertising spending.\nFormally we are splitting the predictor space into a number of regions or leave or the little boxes. The branches, lines are the branches. Somewhat unituitevely at first the leaves appear at the bottom and the stem appears at the top. Intuitively each side gets labelled region one on the graph with the associated points and so on and so forth. In practice this creates a lot of weird boundaries that map onto interactions or different functional forms. Making them good at finding thse relationships without directly specifying them.\n\nlibrary(rpart)\nlibrary(parttree)\n\nout = rpart(Kyphosis ~ Start + Age, data = kyphosis)\n\n# Grab the partitions and plot\nfit_pt = parttree(out)\n\nplot(fit_pt)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nOLS is really nice in the social sciences where we have a better understanding of the DGP and not alot of variables and we don’t neccessarily have that or have the luxury of a lot of data. One thing that OLS does a poor job of is that we have to enter in the relationships ourselves and every possible combination of interactions or functional form of a small set of variables is going to overfit the model/we are going to have a lot of multicollinearity. OLS has a closed form solution where we are making these predictions over the entire data. Which is going to get messy in a way we may not quite understand.\nRegression trees divide the this mess into smaller regions. We are effectively dividing up the space into small mutually exclusive regions. We do this by starting off greedy we find some value that does well with our splitting rule. Then it will make a split into a mutually exclusive regions then redo this process. Instead of looking ahead to see if there is some partiontion that will make its predictions better in the future it will choose what is convenient.\n\n\n\nAs you may imagine growing a tree can maximize in sample fit but thats not really what we are after. However a tree with two branches maybe really interpretable and less sensitive to new data but is going to be somewhat biased. In the regression part we did a little cross validation but didn’t really go over it so it is worth talking about.\nCross-validation is critical to our workflow. We use training and test sets to evaluate our models. This is good but effectively we are only evaluating our model once with the validation set. Instead of a spray and pray approach we can hold out “more” data to evaluate our model and tweak it. Generally we want a model that minimizes our test set error. But if tweak the model based on the test set we are going to end up overfitting. Effectively as I understand it is that we are getting leakage into the training phase without cross-validation.\nCross-validation helps with this because we are using parts of our training set that the model hasn’t seen before to tune our models. We shuffle the data a bit and then evaluate our model on the folds. There are lots of methods most people use is K-fold cross validation. We effectively create lots of datasets from our training set where some of the data is used in the evaluation phase and some of it is used in the training phase. This is really beneficial because you can’t just wait around for more data.\n\n\n\nAs you may imagine striking a delicate balance in growing a single tree is tricky. Basiscally trees are a little bit like adaptive nearest neighbors. This gets a little more complicated but we will not get into that to much. Once we start partition to things into finer and finer neighborhoods we may be able to tune it pretty well but it is going to be extremely sensitive. Enter bagging and boosting. These rely on a similar idea but go about it in a different way. Basically what if we just made a bunch of dumb models and found a way to make them not dumb?\n\n\n\nBagging is just a fancy way of saying voting or averaging. Instead of one smart tree we make a bunch of them! We fit the trees on random parts of our training data and penalize them for getting them to smart. Once each tree makes its predictions we ask them to vote or average their predictions. Whatever gets spit out is the answer to our problem. Conditional on us doing it correctly. So in classification problem if we are predicting whether or not a transaction is fraudulent we would fit the model asking it to classify a bunch of transactions as fraud or not. Whatever number of trees we ask it to make we are just going to take a simple vote on whether that transaction is fraud or not. It is a little beautifully democratic. This works because we are using the predictions from sevearl weak learners where we have high variance but low bias. Meaning that we may not always get our darts in the same area but the difference between predicted and real values is low. Then once we average over these weak learners we reduce our variance.\nRandom forests are based off of this idea but we add a small tweak. We still fit a large number of bad learners but we decorrelate the trees to improve performance. We sample the data with replacement where some trees see the same information a few times while others never see the same inforamtion. We also never let any one tree see a majority of the predictors. This will bias downward really good predictors and give other predictors a chance. Than we take a vote/average of what the trees spit out.\n\n\n\nOne thing that is important is what goes into the linear algebra machine. For things like XGBoost and Random Forests these are kind of the epitome of the “Two Culture” mentioned by Breiman (2001) where the result of a bunch bad learners end up doing really well predicting the relationships between independent and dependent variables. This happens because well we don’t impose many restriction on what the functional form of the relationship between the independent variables between the dependent variable. So if the actual regression equation is something to the effect of\n\\[\n\\text{Wreckless Driving} = \\alpha + \\beta_{1} \\log{\\text{risk taking}} + \\beta_{2} age^{2} + \\beta_{3} \\log(\\text{risk taking}) \\times age^{2} + \\varepsilon\n\\]\nThese models are going to have a fairly good chance of picking this weird relationship up because it is dividing up the space into smaller pieces to either classify or generate predicitions. For a visual demstration see Figure 1\nSubstantively what this means is that these methods are uber flexible but if we were are going to use this as a feature selection machine for models with more parametric assumptions than that could be problem. Random forest are a form of self-regularizing adaptive smoothers. As Curth, Jeffares, and van der Schaar (2024) argue\n\nWe showed that forests improve upon trees through multiple distinct mechanisms that are usually implicitly entangled: they reduce the effect of noise in outcomes, reduce the variability in realized predictors and reduce potential bias by enriching the class of functions that can be represented.\n\nAs you may have guessed a logit doesn’t have these self regularizing properties. So if we cram a bunch of predictors on the right hand side of the equation the likelihood that there is some multicollinearity is pretty high. While this doesn’t effect our coefficient estimates it does effect our standard errors in many cases this can cause them to be inflated resulting in us failing to reject the null. However, it is also possible that this effect is reversed which is highly problematic. Lets say we are interested in describing the effect of some new strategy on some KPI. We can come up with a plausible causal mechanism for why that is. We then go and do our feature selection and our main variable of interest is statistically significant. WOOHOOO great! Not so fast in many political science papers by adding correlated variables we can actually deflate the standard errors causing over rejection of the null (Lenz and Sahn 2021).\nThe other key property that we should focus on is that random forests search over a more diverse array of functions making their predictions less sensitive to us inputting the features in there wrong. Lets say we have a really good idea that age is an important predictor of reckless driving. We would expect that not only is the effect non-linear but has an interaction with gender or risk taking behavior. We may not be able to directly observe risk taking behavior but we may have some proxies that we could get ahold of like traffic tickets. If this interaction is a significant predictor of wreckless driving then our random forest models are going to pick them up. Unless we explicitly enter these into our model we are not going to pick this relationship up. So if we enter age in without transforming the variable we are likely not going to find its effect or the effect of a one unit increase in age is likely to be incredibly small.\n\n\n\nOur other option is boosting. Boosting has different mechanics but the same idea. Instead of making a really good tree we start with a tree that slightly better than random guessing but not that much better on some sample of our training data. However, instead of modeling a lot of trees at once we sequentially fit a series of tree and then sum over them to grab our predictions or classifications\n\n\nADAboost focuses on the misclassification part. Where we take the residuals of the weak learners then re-weight the samples to give more weight to mistakes. Then at the end we take the contribution of each learner as the weighted average of the predictions/majority vote. This was popular for awhille but tends not to perform that well compared to gradient boosting.\n\n\n\nMore recent innovations of boosting use gradient boosting. What is annoying about machine learning is that they are always coming up with catchier terms for old concepts. By gradient we are really just talking about a derivative of our loss function. It depends on what framework you are working in but this could be first order derivatives or second order derivatives. All gradient descent is doing is using the derivative (slope) to move in the direction of the steepest descent (negative gradient).\nIn OLS we can think of this as finding the slope and finding the value that gets us closer to minimizing the sum of the squared. So we would get predicted values from one tree and see how much this reduces this distance then we take the residuals from the model to try and reduce the distance between the residuals and real values in the next tree. We repeat this process by taking the slope and seeing what direction the values would take. Then we would repeat this process until until we no longer get any improvement minimizing the loss function. In this model the size of the step that we take right or left is determined by our learning rate. So if we the set a high learning rate we may take 2 or three steps in the positive direction rather than a half step in the positive direction. This isn’t a perfect analogy since OLS has a closed form solution but this is the general idea.\n\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\ngbr = GradientBoostingRegressor(n_estimators=5, learning_rate=0.1, max_depth=2, random_state=42)\ngbr.fit(X, y)\n\nGradientBoostingRegressor(max_depth=2, n_estimators=5, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressor?Documentation for GradientBoostingRegressoriFittedGradientBoostingRegressor(max_depth=2, n_estimators=5, random_state=42) \n\nstage_predictions = np.array(list(gbr.staged_predict(X)))\n\ncolors = ['red', 'green', 'blue', 'orange', 'purple']\n\n# Plot predictions from each tree\nplt.figure(figsize=(10, 6))\nfor i, preds in enumerate(stage_predictions):\n    plt.scatter(X, y, label='True Data', color='blue', alpha=0.3)  # True data is blue\n    plt.scatter(X, preds, label=f'Tree {i+1} Predictions', color=colors[i], alpha=0.6)  # Different colors for each tree\n    plt.title(f\"After Tree {i+1}\")\n    plt.xlabel(\"Feature\")\n    plt.ylabel(\"Target\")\n    \n    # Calculate residuals after this tree's prediction\n    residuals = y - preds\n    print(f\"Residuals after Tree {i+1}: {residuals[:5]}\")\n\n# Move legend outside the plot\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Legend\")\n\nplt.tight_layout()  # Adjust layout to fit the legend outside\nplt.show()\n\n\n\n\n\n\n\n\nWithin the gradient boosting framework we have two popular approaches: XGboost and LightGBM. Each of these approaches focus on impurity meaning how well a tree can distinguish between two classes using something like a GINI coeficient or Entropy that tries to measure class separtion or using mean squared error.\nThey both work to reduce these impurities however they differ in how they grow the tree. LightGbm focuses on the leaf wise growth whereas XGBoost will do branch or level wise growth. Functionally all this means is that for a level wise strategy we are growing both sides of the flow chart which kind of imposes some regularization. Whereas leafwise growth adds more and more leaves making better predictions but is prone to overfitting.\n\n\n\nIt is worth going over since there are a ton of them. Some we may not actually end up touching but still. For referecnce I am going to just use the XGBoost function from sci-kit learn.\nloss: Just the loss function to be optimized - So if we were to use a OLS loss function than we would just be using the sum of the squared residuals. Meaning XGBoost is going to minimize the distance between the set of the points using the sum of the squared residuals.\nlearning rate: The learning rate effectively regularizes the contribution of each tree. So if we have 3 trees the learning rate is going to add a penalty to minimize the contribution of the tree to slow down how much information an individual tree contributes. Typically we have to balance this penalty with the number of trees. In effect the learning rate is a weighting factor for each new model that minimizes how quickly a new tree responds to errors from an old tree.\ntrees/N_estimators: really just the number of trees our model is going to have.\ntree depth: So this is really just a way of saying how tall the tree is. so a tree depth of 5 is going to have 5 levels\ncriterion: So this governs the ambigous cases. We set a metric to measure the quality of the split\nnumber of iterations no change: How many times does it get to keep trying to improve on the validation score before we stop trainging it.\nsubsamples: controls the fraction of training data that the trees use for the boosting round. I.e if we set it to .5 it will use half the training data.\nmin samples split: Effectively how many data points does a node have to have to make a split. Higher values effectively penalize the tree for splitting off points below this threshold. This helps the tree from learning to much about points that could just be weird for any number of recisions\nStop iterations/number of iterations no change: This is effectively the number of times we let the model try to improve its validation score. We only set this if we plan on implementing early stopping. Early stopping is one way to try to prevent overfitting. However, it is a bit of a double edged sword. We could stop it to early and it may not get to learn some of the quirks of the data generating process. We could let it retry 100 times but on the 101st try it actually improves on the validation score. We are also making the assumption that the validation set is representative of the test data. This may be true but we are making a bit of an assumption that random sampling isn’t biting.\nmtry/ max features: specifies the number of features that each tree can randomly select from when considering a split at each node. This prevents the tree from overfitting the data since one tree does not see all the features. Since it is still an ensemble method we want the tree to have more shots are a variety of functional forms and interactions.\n\n\n\n\nThe answer is that it kind of depends. Gradient boosting of what ever flavor you choose is going to be more prone to overfitting that random forests. Gradient boosting is also a little bit better in the face of class imbalances. Since the prior tree is used to inform the previous tree if you don’t select good features in the beginning you could end of fitting the noise. The other problem is compute time. Since gradient boosting relies on the previous tree it has to wait till that tree is done being fit.\n\n\n\nOne thing about random forests and Gradient boosting machines is that they can handle missing values or in the case of random forests can be used as missing values imputers. For the most part missing data is a little bit of a fact of life. Data goes missing for a variety of reasons. Respondents could forget or refuse to identify their race and ethnicity on a survey some people may be embarassed to report their educational attainment. The issue is not that the data is missing the issue is that we don’t actually know the how these data are missing. Classifically we don’t really about missingness in the DV. We will kind of see why when we cover the difference in mechanisms.\n\nmissingess_mechanisms = pl.DataFrame(\n    {'Missingess Mechanism':[\n        'Missing Completly at Random (MCAR)',\n        'Missing at Random (MAR)',\n        'Missing Not at Random (MNAR)'\n    ],\n    'Assumption': [\n        'Data is missing completly at random. We update a database and the power goes out. In the full database this data is kind of missing completly at random. Nothing about the DGP dicatates that these missing values',\n        'A weaker form of missngness. Missingness in our DV is not determined by our DV. What this means is that we can learn about the missingness by knowing our independent variables',\n        'Means that a missing DV is not determined by any of the IVs instead it is determined by the DV iteself'\n    ]\n\n    }\n)\n\nGT(missingess_mechanisms)\n\n\n\n\n\n\n\nMissingess Mechanism\nAssumption\n\n\n\n\nMissing Completly at Random (MCAR)\nData is missing completly at random. We update a database and the power goes out. In the full database this data is kind of missing completly at random. Nothing about the DGP dicatates that these missing values\n\n\nMissing at Random (MAR)\nA weaker form of missngness. Missingness in our DV is not determined by our DV. What this means is that we can learn about the missingness by knowing our independent variables\n\n\nMissing Not at Random (MNAR)\nMeans that a missing DV is not determined by any of the IVs instead it is determined by the DV iteself\n\n\n\n\n\n\n\n\nIn practice we can never really tell the difference between MNAR or MAR. We can start to investigate the missingness by seeing if we can build a model to classify missing values. If our models start to do a good job of classfiying missing values than we at least have a lead on the possible mechanism. This doesn’t fully get us there because we can only really tell the difference between them with data we never observe. Missing at random is only applicable in very rare instances.\n\n\nFor missing completly at random we can simply delete the missingness observations. Whether this is through listwise deletion or just modifying our query to ignore missing values. If we want to save time than we can let the model handle it for us. Certainly for simple models like linear regression the default is to drop it so it can actual solve the optimization problem.\nHowever, if we assume that we can’t simply ignore the values than we have a lot of options. One of the most common forms of handling missingess is that we can perform interpolation. Effectively we are using information in that variable to predict the missing value. Typically this is done with the next value and the previous value to generate a predicition. We then have to use some kind of distance metric to get that value. We can use the mean and make the guess, if we are worried about extreme values we can use the median, or if we want to get fancy then we can use nearest neighbors where it will try and find a typically value in a cluster. So if a 4 is generally surrounded by 3’s and 5’s and we have a missing value sandwhiched between a 3 and a 5 the nearest neighbor interpolation will probably guess 4 because those are the values that are typically within that neighborhood. We can either to this column wise or rowwise. Column wise is going to be a lot cheaper but doesn’t integrate another information that could be helpful in estimating the missing values.\n\n\n\nInterpolation is fine but we are only guessing once and not integrating all the information that we could into the estimating the missingess or treat the missingness as something as observable. Multiple imputation or imputation methods generally allow us to estimate those missing values but preseve the uncertianty associated with the estimation proceduce. Effectively what we are going to do is treat the missing values as a statistical problem where create some number of copies of our dataset. We make the assumption that the missingness is multivariate normal and the data is MAR or MCAR. We can then start to think of the probability of missingness as a function of the parameter estimates. In the copies of our dataset we then estimate a plausible value. We then bootstrap the data and use a model to get an estimate on the value of the mising data then we draw a single point from the posterior distribution. Once all m datasets are imputed, we fit our model to each dataset separately. The final estimates are obtained by averaging the parameter estimates across all datasets, while the standard errors incorporate both within-dataset variance and between-dataset variance (following Rubin’s rules)\n\n\n\nWell it turns out that random forest are really great at learning hidden relationships between variables. Unless we place priors on certain variables we are imposing a flat prior or making other assumptions about the data. Random Forests for missing data imputation proposes a more flexible model for missing value imputation then interpolation. For things like KNN imputation or Lasso imputation tuning hyperparameters can be difficult and often effect the imputations in ways that are hard to asssess. One alternative to MICE or AMELIA is missForest which is doesn’t require some of the same overhead for large datasets. In multiple imputation we are making m copies of our dataset then running a model to predict the missing values in the dataset and then running m models on the various datasets and then combining them at the end. These multiple imputation methods have been kept up to date to incorporate machine learning models. As you may imagine this is a lot of heavy lifting computationally for a moderatly sized dataset much less a large dataset.\nA compromise, in some respects, is using a specially tailored random forest model to predict the missing values. These random forest approaches do really well with large datasets and incorporate the benefits of bagging into the missing value replacement. By that we mean we can start to model more complex relationships that could explain the missingness in our data",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#a-tree",
    "href": "tree-based-methods.html#a-tree",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "Underriding the entire idea of Bagging and Boosting are regression trees.. Regression trees have a lot of math behind an inherently kind of simple and beautifully dumb idea.\nWhen we fit a single tree there are a set of rules that we give it to make decisions about the data if a variable hits that rule then depending on what side of the line it is then it will split the data off into the left or right side. In the example above when income is not greater than or equal to 24.5 we split it into the false category an then it stops. We have satisfied the criteria. In the true column we then repeat the proces however when we hit the false column again we have a new criterion and that is how much there is on advertising spending.\nFormally we are splitting the predictor space into a number of regions or leave or the little boxes. The branches, lines are the branches. Somewhat unituitevely at first the leaves appear at the bottom and the stem appears at the top. Intuitively each side gets labelled region one on the graph with the associated points and so on and so forth. In practice this creates a lot of weird boundaries that map onto interactions or different functional forms. Making them good at finding thse relationships without directly specifying them.\n\nlibrary(rpart)\nlibrary(parttree)\n\nout = rpart(Kyphosis ~ Start + Age, data = kyphosis)\n\n# Grab the partitions and plot\nfit_pt = parttree(out)\n\nplot(fit_pt)\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#why-do-this",
    "href": "tree-based-methods.html#why-do-this",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "OLS is really nice in the social sciences where we have a better understanding of the DGP and not alot of variables and we don’t neccessarily have that or have the luxury of a lot of data. One thing that OLS does a poor job of is that we have to enter in the relationships ourselves and every possible combination of interactions or functional form of a small set of variables is going to overfit the model/we are going to have a lot of multicollinearity. OLS has a closed form solution where we are making these predictions over the entire data. Which is going to get messy in a way we may not quite understand.\nRegression trees divide the this mess into smaller regions. We are effectively dividing up the space into small mutually exclusive regions. We do this by starting off greedy we find some value that does well with our splitting rule. Then it will make a split into a mutually exclusive regions then redo this process. Instead of looking ahead to see if there is some partiontion that will make its predictions better in the future it will choose what is convenient.",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#gardening",
    "href": "tree-based-methods.html#gardening",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "As you may imagine growing a tree can maximize in sample fit but thats not really what we are after. However a tree with two branches maybe really interpretable and less sensitive to new data but is going to be somewhat biased. In the regression part we did a little cross validation but didn’t really go over it so it is worth talking about.\nCross-validation is critical to our workflow. We use training and test sets to evaluate our models. This is good but effectively we are only evaluating our model once with the validation set. Instead of a spray and pray approach we can hold out “more” data to evaluate our model and tweak it. Generally we want a model that minimizes our test set error. But if tweak the model based on the test set we are going to end up overfitting. Effectively as I understand it is that we are getting leakage into the training phase without cross-validation.\nCross-validation helps with this because we are using parts of our training set that the model hasn’t seen before to tune our models. We shuffle the data a bit and then evaluate our model on the folds. There are lots of methods most people use is K-fold cross validation. We effectively create lots of datasets from our training set where some of the data is used in the evaluation phase and some of it is used in the training phase. This is really beneficial because you can’t just wait around for more data.",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#creating-a-forest-or-boosting-a-tree",
    "href": "tree-based-methods.html#creating-a-forest-or-boosting-a-tree",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "As you may imagine striking a delicate balance in growing a single tree is tricky. Basiscally trees are a little bit like adaptive nearest neighbors. This gets a little more complicated but we will not get into that to much. Once we start partition to things into finer and finer neighborhoods we may be able to tune it pretty well but it is going to be extremely sensitive. Enter bagging and boosting. These rely on a similar idea but go about it in a different way. Basically what if we just made a bunch of dumb models and found a way to make them not dumb?",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#bagging",
    "href": "tree-based-methods.html#bagging",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "Bagging is just a fancy way of saying voting or averaging. Instead of one smart tree we make a bunch of them! We fit the trees on random parts of our training data and penalize them for getting them to smart. Once each tree makes its predictions we ask them to vote or average their predictions. Whatever gets spit out is the answer to our problem. Conditional on us doing it correctly. So in classification problem if we are predicting whether or not a transaction is fraudulent we would fit the model asking it to classify a bunch of transactions as fraud or not. Whatever number of trees we ask it to make we are just going to take a simple vote on whether that transaction is fraud or not. It is a little beautifully democratic. This works because we are using the predictions from sevearl weak learners where we have high variance but low bias. Meaning that we may not always get our darts in the same area but the difference between predicted and real values is low. Then once we average over these weak learners we reduce our variance.\nRandom forests are based off of this idea but we add a small tweak. We still fit a large number of bad learners but we decorrelate the trees to improve performance. We sample the data with replacement where some trees see the same information a few times while others never see the same inforamtion. We also never let any one tree see a majority of the predictors. This will bias downward really good predictors and give other predictors a chance. Than we take a vote/average of what the trees spit out.",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#the-intuition-behind-why-random-forests-work-well",
    "href": "tree-based-methods.html#the-intuition-behind-why-random-forests-work-well",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "One thing that is important is what goes into the linear algebra machine. For things like XGBoost and Random Forests these are kind of the epitome of the “Two Culture” mentioned by Breiman (2001) where the result of a bunch bad learners end up doing really well predicting the relationships between independent and dependent variables. This happens because well we don’t impose many restriction on what the functional form of the relationship between the independent variables between the dependent variable. So if the actual regression equation is something to the effect of\n\\[\n\\text{Wreckless Driving} = \\alpha + \\beta_{1} \\log{\\text{risk taking}} + \\beta_{2} age^{2} + \\beta_{3} \\log(\\text{risk taking}) \\times age^{2} + \\varepsilon\n\\]\nThese models are going to have a fairly good chance of picking this weird relationship up because it is dividing up the space into smaller pieces to either classify or generate predicitions. For a visual demstration see Figure 1\nSubstantively what this means is that these methods are uber flexible but if we were are going to use this as a feature selection machine for models with more parametric assumptions than that could be problem. Random forest are a form of self-regularizing adaptive smoothers. As Curth, Jeffares, and van der Schaar (2024) argue\n\nWe showed that forests improve upon trees through multiple distinct mechanisms that are usually implicitly entangled: they reduce the effect of noise in outcomes, reduce the variability in realized predictors and reduce potential bias by enriching the class of functions that can be represented.\n\nAs you may have guessed a logit doesn’t have these self regularizing properties. So if we cram a bunch of predictors on the right hand side of the equation the likelihood that there is some multicollinearity is pretty high. While this doesn’t effect our coefficient estimates it does effect our standard errors in many cases this can cause them to be inflated resulting in us failing to reject the null. However, it is also possible that this effect is reversed which is highly problematic. Lets say we are interested in describing the effect of some new strategy on some KPI. We can come up with a plausible causal mechanism for why that is. We then go and do our feature selection and our main variable of interest is statistically significant. WOOHOOO great! Not so fast in many political science papers by adding correlated variables we can actually deflate the standard errors causing over rejection of the null (Lenz and Sahn 2021).\nThe other key property that we should focus on is that random forests search over a more diverse array of functions making their predictions less sensitive to us inputting the features in there wrong. Lets say we have a really good idea that age is an important predictor of reckless driving. We would expect that not only is the effect non-linear but has an interaction with gender or risk taking behavior. We may not be able to directly observe risk taking behavior but we may have some proxies that we could get ahold of like traffic tickets. If this interaction is a significant predictor of wreckless driving then our random forest models are going to pick them up. Unless we explicitly enter these into our model we are not going to pick this relationship up. So if we enter age in without transforming the variable we are likely not going to find its effect or the effect of a one unit increase in age is likely to be incredibly small.",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#boosting",
    "href": "tree-based-methods.html#boosting",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "Our other option is boosting. Boosting has different mechanics but the same idea. Instead of making a really good tree we start with a tree that slightly better than random guessing but not that much better on some sample of our training data. However, instead of modeling a lot of trees at once we sequentially fit a series of tree and then sum over them to grab our predictions or classifications\n\n\nADAboost focuses on the misclassification part. Where we take the residuals of the weak learners then re-weight the samples to give more weight to mistakes. Then at the end we take the contribution of each learner as the weighted average of the predictions/majority vote. This was popular for awhille but tends not to perform that well compared to gradient boosting.\n\n\n\nMore recent innovations of boosting use gradient boosting. What is annoying about machine learning is that they are always coming up with catchier terms for old concepts. By gradient we are really just talking about a derivative of our loss function. It depends on what framework you are working in but this could be first order derivatives or second order derivatives. All gradient descent is doing is using the derivative (slope) to move in the direction of the steepest descent (negative gradient).\nIn OLS we can think of this as finding the slope and finding the value that gets us closer to minimizing the sum of the squared. So we would get predicted values from one tree and see how much this reduces this distance then we take the residuals from the model to try and reduce the distance between the residuals and real values in the next tree. We repeat this process by taking the slope and seeing what direction the values would take. Then we would repeat this process until until we no longer get any improvement minimizing the loss function. In this model the size of the step that we take right or left is determined by our learning rate. So if we the set a high learning rate we may take 2 or three steps in the positive direction rather than a half step in the positive direction. This isn’t a perfect analogy since OLS has a closed form solution but this is the general idea.\n\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\ngbr = GradientBoostingRegressor(n_estimators=5, learning_rate=0.1, max_depth=2, random_state=42)\ngbr.fit(X, y)\n\nGradientBoostingRegressor(max_depth=2, n_estimators=5, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressor?Documentation for GradientBoostingRegressoriFittedGradientBoostingRegressor(max_depth=2, n_estimators=5, random_state=42) \n\nstage_predictions = np.array(list(gbr.staged_predict(X)))\n\ncolors = ['red', 'green', 'blue', 'orange', 'purple']\n\n# Plot predictions from each tree\nplt.figure(figsize=(10, 6))\nfor i, preds in enumerate(stage_predictions):\n    plt.scatter(X, y, label='True Data', color='blue', alpha=0.3)  # True data is blue\n    plt.scatter(X, preds, label=f'Tree {i+1} Predictions', color=colors[i], alpha=0.6)  # Different colors for each tree\n    plt.title(f\"After Tree {i+1}\")\n    plt.xlabel(\"Feature\")\n    plt.ylabel(\"Target\")\n    \n    # Calculate residuals after this tree's prediction\n    residuals = y - preds\n    print(f\"Residuals after Tree {i+1}: {residuals[:5]}\")\n\n# Move legend outside the plot\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Legend\")\n\nplt.tight_layout()  # Adjust layout to fit the legend outside\nplt.show()\n\n\n\n\n\n\n\n\nWithin the gradient boosting framework we have two popular approaches: XGboost and LightGBM. Each of these approaches focus on impurity meaning how well a tree can distinguish between two classes using something like a GINI coeficient or Entropy that tries to measure class separtion or using mean squared error.\nThey both work to reduce these impurities however they differ in how they grow the tree. LightGbm focuses on the leaf wise growth whereas XGBoost will do branch or level wise growth. Functionally all this means is that for a level wise strategy we are growing both sides of the flow chart which kind of imposes some regularization. Whereas leafwise growth adds more and more leaves making better predictions but is prone to overfitting.\n\n\n\nIt is worth going over since there are a ton of them. Some we may not actually end up touching but still. For referecnce I am going to just use the XGBoost function from sci-kit learn.\nloss: Just the loss function to be optimized - So if we were to use a OLS loss function than we would just be using the sum of the squared residuals. Meaning XGBoost is going to minimize the distance between the set of the points using the sum of the squared residuals.\nlearning rate: The learning rate effectively regularizes the contribution of each tree. So if we have 3 trees the learning rate is going to add a penalty to minimize the contribution of the tree to slow down how much information an individual tree contributes. Typically we have to balance this penalty with the number of trees. In effect the learning rate is a weighting factor for each new model that minimizes how quickly a new tree responds to errors from an old tree.\ntrees/N_estimators: really just the number of trees our model is going to have.\ntree depth: So this is really just a way of saying how tall the tree is. so a tree depth of 5 is going to have 5 levels\ncriterion: So this governs the ambigous cases. We set a metric to measure the quality of the split\nnumber of iterations no change: How many times does it get to keep trying to improve on the validation score before we stop trainging it.\nsubsamples: controls the fraction of training data that the trees use for the boosting round. I.e if we set it to .5 it will use half the training data.\nmin samples split: Effectively how many data points does a node have to have to make a split. Higher values effectively penalize the tree for splitting off points below this threshold. This helps the tree from learning to much about points that could just be weird for any number of recisions\nStop iterations/number of iterations no change: This is effectively the number of times we let the model try to improve its validation score. We only set this if we plan on implementing early stopping. Early stopping is one way to try to prevent overfitting. However, it is a bit of a double edged sword. We could stop it to early and it may not get to learn some of the quirks of the data generating process. We could let it retry 100 times but on the 101st try it actually improves on the validation score. We are also making the assumption that the validation set is representative of the test data. This may be true but we are making a bit of an assumption that random sampling isn’t biting.\nmtry/ max features: specifies the number of features that each tree can randomly select from when considering a split at each node. This prevents the tree from overfitting the data since one tree does not see all the features. Since it is still an ensemble method we want the tree to have more shots are a variety of functional forms and interactions.",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#gradient-boosting-or-random-trees",
    "href": "tree-based-methods.html#gradient-boosting-or-random-trees",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "The answer is that it kind of depends. Gradient boosting of what ever flavor you choose is going to be more prone to overfitting that random forests. Gradient boosting is also a little bit better in the face of class imbalances. Since the prior tree is used to inform the previous tree if you don’t select good features in the beginning you could end of fitting the noise. The other problem is compute time. Since gradient boosting relies on the previous tree it has to wait till that tree is done being fit.",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  },
  {
    "objectID": "tree-based-methods.html#imputation-methods",
    "href": "tree-based-methods.html#imputation-methods",
    "title": "Tree Based Methods and an Aside on Validation",
    "section": "",
    "text": "One thing about random forests and Gradient boosting machines is that they can handle missing values or in the case of random forests can be used as missing values imputers. For the most part missing data is a little bit of a fact of life. Data goes missing for a variety of reasons. Respondents could forget or refuse to identify their race and ethnicity on a survey some people may be embarassed to report their educational attainment. The issue is not that the data is missing the issue is that we don’t actually know the how these data are missing. Classifically we don’t really about missingness in the DV. We will kind of see why when we cover the difference in mechanisms.\n\nmissingess_mechanisms = pl.DataFrame(\n    {'Missingess Mechanism':[\n        'Missing Completly at Random (MCAR)',\n        'Missing at Random (MAR)',\n        'Missing Not at Random (MNAR)'\n    ],\n    'Assumption': [\n        'Data is missing completly at random. We update a database and the power goes out. In the full database this data is kind of missing completly at random. Nothing about the DGP dicatates that these missing values',\n        'A weaker form of missngness. Missingness in our DV is not determined by our DV. What this means is that we can learn about the missingness by knowing our independent variables',\n        'Means that a missing DV is not determined by any of the IVs instead it is determined by the DV iteself'\n    ]\n\n    }\n)\n\nGT(missingess_mechanisms)\n\n\n\n\n\n\n\nMissingess Mechanism\nAssumption\n\n\n\n\nMissing Completly at Random (MCAR)\nData is missing completly at random. We update a database and the power goes out. In the full database this data is kind of missing completly at random. Nothing about the DGP dicatates that these missing values\n\n\nMissing at Random (MAR)\nA weaker form of missngness. Missingness in our DV is not determined by our DV. What this means is that we can learn about the missingness by knowing our independent variables\n\n\nMissing Not at Random (MNAR)\nMeans that a missing DV is not determined by any of the IVs instead it is determined by the DV iteself\n\n\n\n\n\n\n\n\nIn practice we can never really tell the difference between MNAR or MAR. We can start to investigate the missingness by seeing if we can build a model to classify missing values. If our models start to do a good job of classfiying missing values than we at least have a lead on the possible mechanism. This doesn’t fully get us there because we can only really tell the difference between them with data we never observe. Missing at random is only applicable in very rare instances.\n\n\nFor missing completly at random we can simply delete the missingness observations. Whether this is through listwise deletion or just modifying our query to ignore missing values. If we want to save time than we can let the model handle it for us. Certainly for simple models like linear regression the default is to drop it so it can actual solve the optimization problem.\nHowever, if we assume that we can’t simply ignore the values than we have a lot of options. One of the most common forms of handling missingess is that we can perform interpolation. Effectively we are using information in that variable to predict the missing value. Typically this is done with the next value and the previous value to generate a predicition. We then have to use some kind of distance metric to get that value. We can use the mean and make the guess, if we are worried about extreme values we can use the median, or if we want to get fancy then we can use nearest neighbors where it will try and find a typically value in a cluster. So if a 4 is generally surrounded by 3’s and 5’s and we have a missing value sandwhiched between a 3 and a 5 the nearest neighbor interpolation will probably guess 4 because those are the values that are typically within that neighborhood. We can either to this column wise or rowwise. Column wise is going to be a lot cheaper but doesn’t integrate another information that could be helpful in estimating the missing values.\n\n\n\nInterpolation is fine but we are only guessing once and not integrating all the information that we could into the estimating the missingess or treat the missingness as something as observable. Multiple imputation or imputation methods generally allow us to estimate those missing values but preseve the uncertianty associated with the estimation proceduce. Effectively what we are going to do is treat the missing values as a statistical problem where create some number of copies of our dataset. We make the assumption that the missingness is multivariate normal and the data is MAR or MCAR. We can then start to think of the probability of missingness as a function of the parameter estimates. In the copies of our dataset we then estimate a plausible value. We then bootstrap the data and use a model to get an estimate on the value of the mising data then we draw a single point from the posterior distribution. Once all m datasets are imputed, we fit our model to each dataset separately. The final estimates are obtained by averaging the parameter estimates across all datasets, while the standard errors incorporate both within-dataset variance and between-dataset variance (following Rubin’s rules)\n\n\n\nWell it turns out that random forest are really great at learning hidden relationships between variables. Unless we place priors on certain variables we are imposing a flat prior or making other assumptions about the data. Random Forests for missing data imputation proposes a more flexible model for missing value imputation then interpolation. For things like KNN imputation or Lasso imputation tuning hyperparameters can be difficult and often effect the imputations in ways that are hard to asssess. One alternative to MICE or AMELIA is missForest which is doesn’t require some of the same overhead for large datasets. In multiple imputation we are making m copies of our dataset then running a model to predict the missing values in the dataset and then running m models on the various datasets and then combining them at the end. These multiple imputation methods have been kept up to date to incorporate machine learning models. As you may imagine this is a lot of heavy lifting computationally for a moderatly sized dataset much less a large dataset.\nA compromise, in some respects, is using a specially tailored random forest model to predict the missing values. These random forest approaches do really well with large datasets and incorporate the benefits of bagging into the missing value replacement. By that we mean we can start to model more complex relationships that could explain the missingness in our data",
    "crumbs": [
      "Tree Based Methods and an Aside on Validation"
    ]
  }
]