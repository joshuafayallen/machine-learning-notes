{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baby Stats\n",
        "\n",
        "## Probability\n",
        "\n",
        "We have our general sense of probabilities where we are just the number of times things occur. This is kind of helpful.\n"
      ],
      "id": "12af19a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy import stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "from plotnine import *\n",
        "import polars as pl\n",
        "\n",
        "numbs = np.array([1, 3, 4, 3])\n",
        "\n",
        "1 / numbs.sum()\n"
      ],
      "id": "0266a4f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, for the most part in the real world we don't neccessarily care about the probability of a single event happening unconditionally. Conditioanl probability is genereallly a little weird but not totally different than just counting. \n",
        "\n",
        "## Conditional Probability\n",
        "\n",
        "There are two ways we generally think of basic condiitonal probability Frequentistly and Bayesianly. For the most part these are fairly similar. The key difference is how we incorporate what we know about the world. \n",
        "\n",
        "\n",
        "### Freqeuentist\n",
        "\n",
        "In Frequentism people often say that we don't impose any priors on the data. This is not true in any real sense because we impose a flat prior. So the prior is kind of incorporated for us. Typically we see conditional probabilities expressed in two ways \n",
        "\n",
        "\n",
        "$$\n",
        "\\text{P(A|B)} = \\frac{\\text{Probability of A and B Hapenning}}{\\text{Probability of B happening}} \\\\\\\n",
        "\\text{P(A|B)} = \\frac{\\text{P(A)} \\cap \\text{P(B)}}{\\text{P(B)}} \n",
        "$$\n",
        "\n",
        "This is a bit hard to visualize so lets take an example \n"
      ],
      "id": "54d91c3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "senators = pl.DataFrame({\n",
        "    'Men': [33, 40, 2],\n",
        "    'Woman': [15,9,1],\n",
        "    'party': ['Democrats', 'Republican', 'Independents']\n",
        "}).unpivot(index = 'party',variable_name='gender', value_name='total')\n"
      ],
      "id": "e33e1e64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "senators.glimpse()\n"
      ],
      "id": "40e5a841",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So if we wanted to calculate the conditional probability of drawing a female democratic senator we would really just do this. \n"
      ],
      "id": "4f45dcee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "womand_democrat = senators.filter((pl.col('gender') == 'Woman') & (pl.col('party') == 'Democrats'))['total']\n",
        "\n",
        "democrat = senators.filter(pl.col('party') == 'Democrats').with_columns(total = pl.col('total').sum())['total'][0]\n",
        "\n",
        "denom = 100\n",
        "\n",
        "(womand_democrat/100)/(democrat/100)\n"
      ],
      "id": "56584682",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So this will roughly get us that the odds of drawing a female democratic senator is 0.31. \n",
        "\n",
        "\n",
        "###  Bayesianism \n",
        "\n",
        "Bayesians view conditional probability in a slightly different way. But as Richard McCelreath argues that isn't entirely true in this canned example. Typically we define Bayes Rule as something along these lines. \n",
        "\n",
        "\n",
        "\n",
        "```{=latex}\n",
        "\\begin{align}\n",
        "\n",
        "\\text{P(A|B)} = \\frac{\\text{P(A and B)} \\times \\text{P(B)}}{P(A)}\n",
        "\n",
        "\\end{align}\n",
        "```\n",
        "\n",
        "\n",
        "However as Rich points out there is nothing uniquely Bayesian about this example. We could theoretically rewrite this to just plug in the numbers into the classic frequentist conditional probability statement. The canoncial example is some sort of testing framework. In statistical rethinking they use \"Vampirism\" \n"
      ],
      "id": "884165ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prob_vampire_positive = 0.95 \n",
        "\n",
        "prob_positive_mortal = 0.01\n",
        "\n",
        "pr_vamp = 0.001 \n",
        "\n",
        "pr_positive = prob_vampire_positive * pr_vamp + prob_positive_mortal * (1-pr_vamp)\n",
        "\n",
        "prob_vampire_positive * pr_vamp /  pr_positive\n"
      ],
      "id": "c72f232b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Integrating what we know and what we learn \n",
        "\n",
        "The richness of Bayesian statistics come from using our expertise and updating them in light of data. Informally we can think of a prior as something we know about the data or data generating process before actually seeing. If we know that height in American males is about 5'9 we can write our prior as \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{Male Height} \\sim N(\\mu, \\sigma) \\\\\n",
        "                        N(70.8, 1)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "where we assume height is normally distributed around 70.8 and a variance of an inch. We can make this prior more restrictive by changing the standard deviation to a smaller value or greedier by increasing it. This works for all kinds of data. Say we want to formalize our prior on the Chiefs getting a questionable call. We could write the prior as a binomial distribution \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{Chiefs Get Questionable call} \\sim Binomial(n, \\pi) \\\\\\\n",
        "                                         Binomial(n, .6)\n",
        "\\end{align}\n",
        "$$ \n",
        "\n",
        "A posterior is really just when our prior meets our data. Our posterior distribution contains every unique combination of data, likelihood, parameters, and our prior. \n",
        "\n",
        "What makes Bayesianism interesting is when we start updating the posterior distribution. Under the hood we need integral calculus to do this but hand deriving that either on paper or from scratch. Really what we are kind of doing when we run this through a MCMC is that we are sampling from our posterior distribution and counting the frequencies that something occurs. I am going to use the counting water example  \n"
      ],
      "id": "767bd3b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "def calculate_n_ways_possible(observations: str, n_water: int, resolution: int = 4):\n",
        "    \"\"\"\n",
        "    Calculate the number of ways to observing water ('W') given the toss of a globe\n",
        "    with `resolution` number of sides and `n_water` faces.\n",
        "    \n",
        "    Note: this method results in numerical precision issues (due to the product) when the\n",
        "    resolution of 16 or so, depending on your system.\n",
        "    \"\"\"\n",
        "    assert n_water <= resolution\n",
        "    \n",
        "    # Convert observation string to an array\n",
        "    observations = np.array(list(observations.upper()))\n",
        "    \n",
        "    # Create n-sided globe with possible outcomes\n",
        "    possible = np.array(list(\"L\" * (resolution - n_water)) + list(\"W\" * n_water))\n",
        "    \n",
        "    # Tally up ways to obtain each observation given the possible outcomes\n",
        "    # Here we use brute-force, but we could also use the analytical solution below\n",
        "    ways = []\n",
        "    for obs in observations:\n",
        "        ways.append((possible == obs).sum())\n",
        "    \n",
        "    p_water = n_water / resolution\n",
        "    # perform product in log space for numerical precision\n",
        "    n_ways = np.round(np.exp(np.sum(np.log(ways)))).astype(int)\n",
        "    return n_ways, p_water\n",
        "\n",
        "\n",
        "def run_globe_tossing_simulation(observations, resolution, current_n_possible_ways=None):\n",
        "    \"\"\"Simulate the number of ways you can observe water ('W') for a globe of `resolution`\n",
        "    sides, varying the proportion of the globe that is covered by water.\n",
        "    \"\"\"\n",
        "    # For Bayesian updates\n",
        "    current_n_possible_ways = current_n_possible_ways if current_n_possible_ways is not None else np.array([])\n",
        "    \n",
        "    print(f\"Observations: '{observations}'\")\n",
        "    p_water = np.array([])\n",
        "    for n_W in range(0, resolution + 1):\n",
        "        n_L = resolution - n_W\n",
        "        globe_sides = \"W\" * n_W + \"L\" * n_L\n",
        "        n_possible_ways, p_water_ = calculate_n_ways_possible(observations, n_water=n_W, resolution=resolution)\n",
        "        print(f\"({n_W+1}) {globe_sides} p(W) = {p_water_:1.2}\\t\\t{n_possible_ways} Ways to Produce\")\n",
        "\n",
        "        p_water = np.append(p_water, p_water_)\n",
        "        current_n_possible_ways = np.append(current_n_possible_ways, n_possible_ways)\n",
        "\n",
        "    return current_n_possible_ways, p_water\n",
        "\n",
        "from pprint import pprint\n",
        "np.random.seed(1)\n",
        "def simulate_globe_toss(p: float = 0.7, N: int = 9) -> list[str]:\n",
        "    \"\"\"Simulate N globe tosses with a specific/known proportion\n",
        "    p: float\n",
        "        The propotion of water\n",
        "    N: int\n",
        "        Number of globe tosses\n",
        "    \"\"\"\n",
        "    return np.random.choice(list(\"WL\"),  size=N, p=np.array([p, 1-p]), replace=True)"
      ],
      "id": "2cbbace5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RESOLUTION = 4\n",
        "observations = \"WLW\"\n",
        "n_possible_ways, p_water = run_globe_tossing_simulation(observations, resolution=RESOLUTION)"
      ],
      "id": "53dee363",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will spit out the number of ways we can produce various draws. Now lets simulate the number of globe tosses wher we are just adding tosses. \n"
      ],
      "id": "3fb5576e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "from scipy.special import factorial\n",
        "\n",
        "def beta_posterior(n_W: int, n_L: int, p: float) -> float:\n",
        "    \"\"\"Calculates the beta posterior over proportions `p` given a set of\n",
        "    `N_W` water and `N_L` land observations\n",
        "    \"\"\"\n",
        "    return factorial(n_W + n_L + 1) / (factorial(n_W) * factorial(n_L)) * p ** n_W * (1-p) ** n_L\n",
        "\n",
        "\n",
        "def plot_beta_posterior_from_observations(observations: str, resolution: int = 50, **plot_kwargs) -> None:\n",
        "    \"\"\"Calculates and plots the beta posterior for a string of observations\"\"\"\n",
        "    n_W = len(observations.replace(\"L\", \"\"))\n",
        "    n_L = len(observations) - n_W\n",
        "    proportions = np.linspace(0, 1, resolution)\n",
        "        \n",
        "    probs = beta_posterior(n_W, n_L, proportions)\n",
        "    plt.plot(proportions, probs, **plot_kwargs)\n",
        "    plt.yticks([])\n",
        "    plt.title(observations)\n",
        "    \n",
        "\n",
        "# Tossing the globe\n",
        "observations = \"WLWWWLWLW\"\n",
        "fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
        "for ii in range(9):\n",
        "    ax = axs[ii // 3][ii % 3]\n",
        "    plt.sca(ax)\n",
        "    # Plot previous\n",
        "    if ii > 0:\n",
        "        plot_beta_posterior_from_observations(observations[:ii], color='k', linestyle='--')\n",
        "    else:\n",
        "        # First observation, no previous data\n",
        "        plot_beta_posterior_from_observations('', color='k', linestyle='--')\n",
        "        \n",
        "    color = 'C1' if observations[ii] == 'W' else 'C0'\n",
        "    plot_beta_posterior_from_observations(observations[:ii+1], color=color, linewidth=4, alpha=.5)\n",
        "    \n",
        "    if not ii % 3:\n",
        "        plt.ylabel(\"posterior probability\")\n"
      ],
      "id": "704dd8dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counting \n",
        "\n",
        "One component of interviews is that we should have some intuition on counting. Like not 1,2,3 etc but how many possible combinations of things can there be aka permutations and combinations. It has been awhile since you have had to do this so it is worth going over. Permutations care about about the unique order that things can be paired in. While combinations are order agnostic. Lets say that we are trying to figure out the number of possible ways that we can seat guests at various tables. That would be a combination. For your own sake lets number all the seats. A permutation would care about the number of unique ways we can arrange people while taking the seat numbers available. What is underriding this whole thing is factorials and orders. So the formulas look something like this. Where n is the number of items and k is the number of items to arrange\n",
        "\n",
        "\n",
        "```{=latex}\n",
        "\\begin{align}\n",
        "\\text{permutation} = \\frac{n!}{(n-k)!} \\\\\n",
        "\n",
        "\\text{combination} = \\frac{n!}{k!(n-k)!}\n",
        "\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "factorials are just a simplfying way of writing out something like this. Where we are progresively multiplying 1 by 1 to 2 * 1 then so on and so forth. \n"
      ],
      "id": "058570a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def factorial(n):\n",
        "\n",
        "    result = 1\n",
        "    for i in range(1, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "print(factorial(n = 5))"
      ],
      "id": "4a754942",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets say we wanted to figure out the number of possible table combinations. Lets say we have 20 people and 5 tables. So we would do something like this. \n"
      ],
      "id": "235db085"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math \n",
        "\n",
        "math.factorial(20)//(math.factorial(5) * math.factorial(20 - 5))\n"
      ],
      "id": "1d893c60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However this is not really how wedding seating works. Order matters and who sits next to who matters. So we need to take that into account. In instead of multiplying by the factorial of k we do something like this with permutations \n"
      ],
      "id": "e3a59cd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "math.factorial(20)//(factorial(20-5))\n"
      ],
      "id": "b566d22a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Centeral Limit Theorum\n",
        "\n",
        "She is genuinely one of the most coolest concepts in statistics. Bascially no matter the distribution of the variablee as we take more and more samples of the data as N increases we are going to converge to a normal distribution. This is really powerful concept in frequentist statistics because we are making inferences about the population using samples. \n",
        "\n",
        "Effectively we can take any distribution that we want and take an infinite number of samples from it and it will be a normal distribution. Lets take our trusty dusty old uniform distribution\n"
      ],
      "id": "042f22b3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pop = np.random.uniform(0,100, size = 5000)\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,8))\n",
        "\n",
        "ax.hist(pop, edgecolor = 'white')\n",
        "\n",
        "plt.show()"
      ],
      "id": "51467cfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then lets compare what happens when we take increasingly larger sample sizes.\n"
      ],
      "id": "20796369"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samp_sizes = [10, 100, 500, 1000]\n",
        "\n",
        "samps = {}\n",
        "\n",
        "for i in samp_sizes:\n",
        "    sample_means = [\n",
        "        np.random.choice(pop, size=samp, replace=True) for samp in samp_sizes\n",
        "    ]\n",
        "    samps[f\"sample_size{i}\"] = sample_means\n",
        "\n",
        "\n",
        "df = (\n",
        "    pl.DataFrame(samps)\n",
        "    .unpivot(value_name=\"vals\", variable_name=\"sample_size\")\n",
        "    .explode(\"vals\")\n",
        ")\n",
        "\n",
        "(ggplot(df, aes(x=\"vals\"))\n",
        "  + geom_histogram(bins = 30) \n",
        "  + facet_wrap(\"sample_size\"))"
      ],
      "id": "93f0b3aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain the Difference Between Probability and Likelihood\n",
        "Probability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses.\n",
        "\n",
        "Probability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5.\n",
        "\n",
        "The likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low.\n"
      ],
      "id": "ed4ab01b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/josh/Library/Python/3.11/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}