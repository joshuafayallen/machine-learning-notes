{
  "hash": "c7f7528dd1b5bdf682febb4ec258d35f",
  "result": {
    "engine": "jupyter",
    "markdown": "# Baby Stats\n\n## Probability\n\nWe have our general sense of probabilities where we are just the number of times things occur. This is kind of helpful.\n\n::: {#e9d72cfd .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy import stats as stats\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nimport polars as pl\n\nnumbs = np.array([1, 3, 4, 3])\n\n1 / numbs.sum()\n\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nnp.float64(0.09090909090909091)\n```\n:::\n:::\n\n\nHowever, for the most part in the real world we don't neccessarily care about the probability of a single event happening unconditionally. Conditioanl probability is genereallly a little weird but not totally different than just counting. \n\n## Conditional Probability\n\nThere are two ways we generally think of basic condiitonal probability Frequentistly and Bayesianly. For the most part these are fairly similar. The key difference is how we incorporate what we know about the world. \n\n\n### Freqeuentist\n\nIn Frequentism people often say that we don't impose any priors on the data. This is not true in any real sense because we impose a flat prior. So the prior is kind of incorporated for us. Typically we see conditional probabilities expressed in two ways \n\n\n$$\n\\text{P(A|B)} = \\frac{\\text{Probability of A and B Hapenning}}{\\text{Probability of B happening}} \\\\\\\n\\text{P(A|B)} = \\frac{\\text{P(A)} \\cap \\text{P(B)}}{\\text{P(B)}} \n$$\n\nThis is a bit hard to visualize so lets take an example \n\n\n\n::: {#81a15b1c .cell execution_count=3}\n``` {.python .cell-code}\nsenators.glimpse()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 6\nColumns: 3\n$ party  <str> 'Democrats', 'Republican', 'Independents', 'Democrats', 'Republican', 'Independents'\n$ gender <str> 'Men', 'Men', 'Men', 'Woman', 'Woman', 'Woman'\n$ total  <i64> 33, 40, 2, 15, 9, 1\n\n```\n:::\n:::\n\n\nSo if we wanted to calculate the conditional probability of drawing a female democratic senator we would really just do this. \n\n::: {#ecd829d4 .cell execution_count=4}\n``` {.python .cell-code}\nwomand_democrat = senators.filter((pl.col('gender') == 'Woman') & (pl.col('party') == 'Democrats'))['total']\n\ndemocrat = senators.filter(pl.col('party') == 'Democrats').with_columns(total = pl.col('total').sum())['total'][0]\n\ndenom = 100\n\n(womand_democrat/100)/(democrat/100)\n\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (1,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>total</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.3125</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nSo this will roughly get us that the odds of drawing a female democratic senator is 0.31. \n\n\n###  Bayesianism \n\nBayesians view conditional probability in a slightly different way. But as Richard McCelreath argues that isn't entirely true in this canned example. Typically we define Bayes Rule as something along these lines. \n\n\n\n\n\n\n```{=latex}\n\\begin{align}\n\n\\text{P(A|B)} = \\frac{\\text{P(A and B)} \\times \\text{P(B)}}{P(A)}\n\n\\end{align}\n```\n\n\n\n\n\nHowever as Rich points out there is nothing uniquely Bayesian about this example. We could theoretically rewrite this to just plug in the numbers into the classic frequentist conditional probability statement. The canoncial example is some sort of testing framework. In statistical rethinking they use \"Vampirism\" \n\n::: {#ceda0675 .cell execution_count=5}\n``` {.python .cell-code}\nprob_vampire_positive = 0.95 \n\nprob_positive_mortal = 0.01\n\npr_vamp = 0.001 \n\npr_positive = prob_vampire_positive * pr_vamp + prob_positive_mortal * (1-pr_vamp)\n\nprob_vampire_positive * pr_vamp /  pr_positive\n\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.08683729433272395\n```\n:::\n:::\n\n\n### Integrating what we know and what we learn \n\nThe richness of Bayesian statistics come from using our expertise and updating them in light of data. Informally we can think of a prior as something we know about the data or data generating process before actually seeing. If we know that height in American males is about 5'9 we can write our prior as \n\n$$\n\\begin{align}\n\\text{Male Height} \\sim N(\\mu, \\sigma) \\\\\n                        N(70.8, 1)\n\\end{align}\n$$\n\n\nwhere we assume height is normally distributed around 70.8 and a variance of an inch. We can make this prior more restrictive by changing the standard deviation to a smaller value or greedier by increasing it. This works for all kinds of data. Say we want to formalize our prior on the Chiefs getting a questionable call. We could write the prior as a binomial distribution \n\n$$\n\\begin{align}\n\\text{Chiefs Get Questionable call} \\sim Binomial(n, \\pi) \\\\\\\n                                         Binomial(n, .6)\n\\end{align}\n$$ \n\nA posterior is really just when our prior meets our data. Our posterior distribution contains every unique combination of data, likelihood, parameters, and our prior. \n\nWhat makes Bayesianism interesting is when we start updating the posterior distribution. Under the hood we need integral calculus to do this but hand deriving that either on paper or from scratch. Really what we are kind of doing when we run this through a MCMC is that we are sampling from our posterior distribution and counting the frequencies that something occurs. I am going to use the counting water example  \n\n::: {#55d63273 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ndef calculate_n_ways_possible(observations: str, n_water: int, resolution: int = 4):\n    \"\"\"\n    Calculate the number of ways to observing water ('W') given the toss of a globe\n    with `resolution` number of sides and `n_water` faces.\n    \n    Note: this method results in numerical precision issues (due to the product) when the\n    resolution of 16 or so, depending on your system.\n    \"\"\"\n    assert n_water <= resolution\n    \n    # Convert observation string to an array\n    observations = np.array(list(observations.upper()))\n    \n    # Create n-sided globe with possible outcomes\n    possible = np.array(list(\"L\" * (resolution - n_water)) + list(\"W\" * n_water))\n    \n    # Tally up ways to obtain each observation given the possible outcomes\n    # Here we use brute-force, but we could also use the analytical solution below\n    ways = []\n    for obs in observations:\n        ways.append((possible == obs).sum())\n    \n    p_water = n_water / resolution\n    # perform product in log space for numerical precision\n    n_ways = np.round(np.exp(np.sum(np.log(ways)))).astype(int)\n    return n_ways, p_water\n\n\ndef run_globe_tossing_simulation(observations, resolution, current_n_possible_ways=None):\n    \"\"\"Simulate the number of ways you can observe water ('W') for a globe of `resolution`\n    sides, varying the proportion of the globe that is covered by water.\n    \"\"\"\n    # For Bayesian updates\n    current_n_possible_ways = current_n_possible_ways if current_n_possible_ways is not None else np.array([])\n    \n    print(f\"Observations: '{observations}'\")\n    p_water = np.array([])\n    for n_W in range(0, resolution + 1):\n        n_L = resolution - n_W\n        globe_sides = \"W\" * n_W + \"L\" * n_L\n        n_possible_ways, p_water_ = calculate_n_ways_possible(observations, n_water=n_W, resolution=resolution)\n        print(f\"({n_W+1}) {globe_sides} p(W) = {p_water_:1.2}\\t\\t{n_possible_ways} Ways to Produce\")\n\n        p_water = np.append(p_water, p_water_)\n        current_n_possible_ways = np.append(current_n_possible_ways, n_possible_ways)\n\n    return current_n_possible_ways, p_water\n\nfrom pprint import pprint\nnp.random.seed(1)\ndef simulate_globe_toss(p: float = 0.7, N: int = 9) -> list[str]:\n    \"\"\"Simulate N globe tosses with a specific/known proportion\n    p: float\n        The propotion of water\n    N: int\n        Number of globe tosses\n    \"\"\"\n    return np.random.choice(list(\"WL\"),  size=N, p=np.array([p, 1-p]), replace=True)\n```\n:::\n\n\n::: {#95c27636 .cell execution_count=7}\n``` {.python .cell-code}\nRESOLUTION = 4\nobservations = \"WLW\"\nn_possible_ways, p_water = run_globe_tossing_simulation(observations, resolution=RESOLUTION)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nObservations: 'WLW'\n(1) LLLL p(W) = 0.0\t\t0 Ways to Produce\n(2) WLLL p(W) = 0.25\t\t3 Ways to Produce\n(3) WWLL p(W) = 0.5\t\t8 Ways to Produce\n(4) WWWL p(W) = 0.75\t\t9 Ways to Produce\n(5) WWWW p(W) = 1.0\t\t0 Ways to Produce\n```\n:::\n:::\n\n\nThis will spit out the number of ways we can produce various draws. Now lets simulate the number of globe tosses wher we are just adding tosses. \n\n::: {#e2057c3a .cell execution_count=8}\n`````` {.python .cell-code code-fold=\"true\"}\nfrom scipy.special import factorial\n\ndef beta_posterior(n_W: int, n_L: int, p: float) -> float:\n    \"\"\"Calculates the beta posterior over proportions `p` given a set of\n    `N_W` water and `N_L` land observations\n    \"\"\"\n    return factorial(n_W + n_L + 1) / (factorial(n_W) * factorial(n_L)) * p ** n_W * (1-p) ** n_L\n\n\ndef plot_beta_posterior_from_observations(observations: str, resolution: int = 50, **plot_kwargs) -> None:\n    \"\"\"Calculates and plots the beta posterior for a string of observations\"\"\"\n    n_W = len(observations.replace(\"L\", \"\"))\n    n_L = len(observations) - n_W\n    proportions = np.linspace(0, 1, resolution)\n        \n    probs = beta_posterior(n_W, n_L, proportions)\n    plt.plot(proportions, probs, **plot_kwargs)\n    plt.yticks([])\n    plt.title(observations)\n    \n\n# Tossing the globe\nobservations = \"WLWWWLWLW\"\nfig, axs = plt.subplots(3, 3, figsize=(8, 8))\nfor ii in range(9):\n    ax = axs[ii // 3][ii % 3]\n    plt.sca(ax)\n    # Plot previous\n    if ii > 0:\n        plot_beta_posterior_from_observations(observations[:ii], color='k', linestyle='--')\n    else:\n        # First observation, no previous data\n        plot_beta_posterior_from_observations('', color='k', linestyle='--')\n        \n    color = 'C1' if observations[ii] == 'W' else 'C0'\n    plot_beta_posterior_from_observations(observations[:ii+1], color=color, linewidth=4, alpha=.5)\n    \n    if not ii % 3:\n        plt.ylabel(\"posterior probability\")\n\n``````\n\n::: {.cell-output .cell-output-display}\n![](probability_files/figure-html/cell-9-output-1.png){width=635 height=653}\n:::\n:::\n\n\n## Counting \n\nOne component of interviews is that we should have some intuition on counting. Like not 1,2,3 etc but how many possible combinations of things can there be aka permutations and combinations. It has been awhile since you have had to do this so it is worth going over. Permutations care about about the unique order that things can be paired in. While combinations are order agnostic. Lets say that we are trying to figure out the number of possible ways that we can seat guests at various tables. That would be a combination. For your own sake lets number all the seats. A permutation would care about the number of unique ways we can arrange people while taking the seat numbers available. What is underriding this whole thing is factorials and orders. So the formulas look something like this. Where n is the number of items and k is the number of items to arrange\n\n\n\n\n\n```{=latex}\n\\begin{align}\n\\text{permutation} = \\frac{n!}{(n-k)!} \\\\\n\n\\text{combination} = \\frac{n!}{k!(n-k)!}\n\n\\end{align}\n\n\n```\n\n\n\n\nfactorials are just a simplfying way of writing out something like this. Where we are progresively multiplying 1 by 1 to 2 * 1 then so on and so forth. \n\n::: {#06ccd30e .cell execution_count=9}\n``` {.python .cell-code}\ndef factorial(n):\n\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\nprint(factorial(n = 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n120\n```\n:::\n:::\n\n\nLets say we wanted to figure out the number of possible table combinations. Lets say we have 20 people and 5 tables. So we would do something like this. \n\n::: {#16ada2ba .cell execution_count=10}\n``` {.python .cell-code}\nimport math \n\nmath.factorial(20)//(math.factorial(5) * math.factorial(20 - 5))\n\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n15504\n```\n:::\n:::\n\n\nHowever this is not really how wedding seating works. Order matters and who sits next to who matters. So we need to take that into account. In instead of multiplying by the factorial of k we do something like this with permutations \n\n::: {#50ebb721 .cell execution_count=11}\n``` {.python .cell-code}\nmath.factorial(20)//(factorial(20-5))\n\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n1860480\n```\n:::\n:::\n\n\n## The Centeral Limit Theorum\n\nShe is genuinely one of the most coolest concepts in statistics. Bascially no matter the distribution of the variablee as we take more and more samples of the data as N increases we are going to converge to a normal distribution. This is really powerful concept in frequentist statistics because we are making inferences about the population using samples. \n\nEffectively we can take any distribution that we want and take an infinite number of samples from it and it will be a normal distribution. Lets take our trusty dusty old uniform distribution\n\n::: {#59f64833 .cell execution_count=12}\n``` {.python .cell-code}\npop = np.random.uniform(0,100, size = 5000)\n\n\n\nfig, ax = plt.subplots(figsize = (12,8))\n\nax.hist(pop, edgecolor = 'white')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](probability_files/figure-html/cell-13-output-1.png){width=947 height=633}\n:::\n:::\n\n\nThen lets compare what happens when we take increasingly larger sample sizes.\n\n::: {#45939f8d .cell execution_count=13}\n``` {.python .cell-code}\nsamp_sizes = [10, 100, 500, 1000]\n\nsamps = {}\n\nfor i in samp_sizes:\n    sample_means = [\n        np.random.choice(pop, size=samp, replace=True) for samp in samp_sizes\n    ]\n    samps[f\"sample_size{i}\"] = sample_means\n\n\ndf = (\n    pl.DataFrame(samps)\n    .unpivot(value_name=\"vals\", variable_name=\"sample_size\")\n    .explode(\"vals\")\n)\n\n(ggplot(df, aes(x=\"vals\"))\n  + geom_histogram(bins = 30) \n  + facet_wrap(\"sample_size\"))\n```\n\n::: {.cell-output .cell-output-display}\n![](probability_files/figure-html/cell-14-output-1.png){width=672 height=480}\n:::\n:::\n\n\n## Explain the Difference Between Probability and Likelihood\nProbability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses.\n\nProbability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5.\n\nThe likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low.\n\n",
    "supporting": [
      "probability_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}