{
  "hash": "b093b3234b34b746a708465302dffe49",
  "result": {
    "engine": "knitr",
    "markdown": "# Classification \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\"}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport polars as pl \nimport polars.selectors as cs \nfrom sklearn.datasets import make_classification\n\nstocks = pl.read_csv('data/Smarket.csv')\n\n```\n:::\n\n\n\n\n\nLets work through a somewhat contrived example. Lets say we wanted to predict the whether the stock market is going up or down. This is not neccessarily all that interesting but will be good practice. Lets visualize the data\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncorrs = stocks.select(~cs.string()).corr()\n\ncorrs \n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (8, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Year</th><th>Lag1</th><th>Lag2</th><th>Lag3</th><th>Lag4</th><th>Lag5</th><th>Volume</th><th>Today</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0</td><td>0.0297</td><td>0.030596</td><td>0.033195</td><td>0.035689</td><td>0.029788</td><td>0.539006</td><td>0.030095</td></tr><tr><td>0.0297</td><td>1.0</td><td>-0.026294</td><td>-0.010803</td><td>-0.002986</td><td>-0.005675</td><td>0.04091</td><td>-0.026155</td></tr><tr><td>0.030596</td><td>-0.026294</td><td>1.0</td><td>-0.025897</td><td>-0.010854</td><td>-0.003558</td><td>-0.043383</td><td>-0.01025</td></tr><tr><td>0.033195</td><td>-0.010803</td><td>-0.025897</td><td>1.0</td><td>-0.024051</td><td>-0.018808</td><td>-0.041824</td><td>-0.002448</td></tr><tr><td>0.035689</td><td>-0.002986</td><td>-0.010854</td><td>-0.024051</td><td>1.0</td><td>-0.027084</td><td>-0.048414</td><td>-0.0069</td></tr><tr><td>0.029788</td><td>-0.005675</td><td>-0.003558</td><td>-0.018808</td><td>-0.027084</td><td>1.0</td><td>-0.022002</td><td>-0.03486</td></tr><tr><td>0.539006</td><td>0.04091</td><td>-0.043383</td><td>-0.041824</td><td>-0.048414</td><td>-0.022002</td><td>1.0</td><td>0.014592</td></tr><tr><td>0.030095</td><td>-0.026155</td><td>-0.01025</td><td>-0.002448</td><td>-0.0069</td><td>-0.03486</td><td>0.014592</td><td>1.0</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\n\n\nWe may also want to see some descriptives. A line plot would be nice but it would kind of hide alot so we are going to make a beeswarm plot \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nyears_unique = stocks.unique(subset=\"Year\")[\"Year\"].to_list()\n\nyears_num = {year: i for i, year in enumerate(years_unique)}\n\nfor years in years_unique:\n    year_data = stocks.filter(pl.col(\"Year\") == years)\n    y = year_data[\"Volume\"].to_numpy()\n    x = np.random.normal(years_num[years], 0.1, len(y))\n\n    plt.scatter(x, y)\n\n\nplt.xticks(range(len(years_unique)), sorted(years_unique))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n([<matplotlib.axis.XTick object at 0x149cf1b10>, <matplotlib.axis.XTick object at 0x14a3c3690>, <matplotlib.axis.XTick object at 0x15cb56a10>, <matplotlib.axis.XTick object at 0x15cb677d0>, <matplotlib.axis.XTick object at 0x15cb71250>], [Text(0, 0, '2001'), Text(1, 0, '2002'), Text(2, 0, '2003'), Text(3, 0, '2004'), Text(4, 0, '2005')])\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.ylabel(\"Trading Volume\")\nplt.xlabel(\"Year\")\n```\n\n::: {.cell-output-display}\n![](classification_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nWe are slowly but surely seeing an upwardsish trend in trading volume. There are definitely more graphs that we can make but for now we are going to move to the study portion. \n\n\n## Logistic Regression\n\n\nLogistic regression is probably your first classifier and is an incredibly important. They are a subfamily of Maximum Likelihood estimation. In a canned example lets say we have the a coin flip and we do it like a ton of times. We record each outcome where the probability is defined something like this. \n\n\n\n\n\n```{=latex}\n\\begin{align}\nP(Heads) = \\frac{number of heads}{number of tosses} \\\\\nP(Tails) = 1 - P(Heads)\n \n\n\\end{align}\n\n\n```\n\n\n\n\nWe have some data and now we want to model it. We take the joint probablility as the function of some parameters aka the likelihood function. The value of theta that maxmimizes the likelihood function is called the maximum likelihood estimator. \n\nLogit is just a sub estimator of these estimators where we make the assumption that the DGP follows a binomial distribution and that the most appropriate link function is the logistic link which is just a technical sounding way of saying we are taking the log of $\\frac{p}{1-p}$. This is really usefull because we bound the odds of an event happening between 0-1. The problem with MLE is that when underneath the hood we are logging the likelihood function because well prior to the invention of computer with this much power you would never be able to hand derive these things, but we still had data to model. \n\n\n### Modeling it in Python \n\nLets take our stock market data and try to classify whether the direction is up or down using contemporaneous volumne and what appears to be lags of volume. Using the Stats model api we can do it like this\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstocks_pd = stocks.with_columns(\n    direction = pl.when(pl.col('Direction') == 'Up').then(1).otherwise(0)\n)\n\nform_version = smf.logit('direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume', data = stocks_pd.to_pandas()).fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOptimization terminated successfully.\n         Current function value: 0.691034\n         Iterations 4\n```\n\n\n:::\n\n```{.python .cell-code}\nx = stocks_pd.drop(['Direction', 'direction', 'Year', 'Today']).to_numpy()\n\nx = sm.add_constant(x)\n\ny = stocks_pd['direction'].to_numpy()\n\nstats_version = sm.GLM(y, x, family = sm.families.Binomial()).fit()\n\nstats_version.summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>  1250</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  1243</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     6</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -863.79</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Wed, 26 Feb 2025</td> <th>  Deviance:          </th> <td>  1727.6</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>16:23:38</td>     <th>  Pearson chi2:      </th> <td>1.25e+03</td>\n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>4</td>        <th>  Pseudo R-squ. (CS):</th> <td>0.002868</td>\n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>   -0.1260</td> <td>    0.241</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.598</td> <td>    0.346</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>   -0.0731</td> <td>    0.050</td> <td>   -1.457</td> <td> 0.145</td> <td>   -0.171</td> <td>    0.025</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>   -0.0423</td> <td>    0.050</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.140</td> <td>    0.056</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.0111</td> <td>    0.050</td> <td>    0.222</td> <td> 0.824</td> <td>   -0.087</td> <td>    0.109</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>    0.0094</td> <td>    0.050</td> <td>    0.187</td> <td> 0.851</td> <td>   -0.089</td> <td>    0.107</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>    0.0103</td> <td>    0.050</td> <td>    0.208</td> <td> 0.835</td> <td>   -0.087</td> <td>    0.107</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>    0.1354</td> <td>    0.158</td> <td>    0.855</td> <td> 0.392</td> <td>   -0.175</td> <td>    0.446</td>\n</tr>\n</table>\n```\n\n:::\n\n```{.python .cell-code}\nform_version.summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>Logit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>       <td>direction</td>    <th>  No. Observations:  </th>  <td>  1250</td> \n</tr>\n<tr>\n  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1243</td> \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     6</td> \n</tr>\n<tr>\n  <th>Date:</th>            <td>Wed, 26 Feb 2025</td> <th>  Pseudo R-squ.:     </th> <td>0.002074</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>16:23:38</td>     <th>  Log-Likelihood:    </th> <td> -863.79</td>\n</tr>\n<tr>\n  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -865.59</td>\n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.7319</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   -0.1260</td> <td>    0.241</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.598</td> <td>    0.346</td>\n</tr>\n<tr>\n  <th>Lag1</th>      <td>   -0.0731</td> <td>    0.050</td> <td>   -1.457</td> <td> 0.145</td> <td>   -0.171</td> <td>    0.025</td>\n</tr>\n<tr>\n  <th>Lag2</th>      <td>   -0.0423</td> <td>    0.050</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.140</td> <td>    0.056</td>\n</tr>\n<tr>\n  <th>Lag3</th>      <td>    0.0111</td> <td>    0.050</td> <td>    0.222</td> <td> 0.824</td> <td>   -0.087</td> <td>    0.109</td>\n</tr>\n<tr>\n  <th>Lag4</th>      <td>    0.0094</td> <td>    0.050</td> <td>    0.187</td> <td> 0.851</td> <td>   -0.089</td> <td>    0.107</td>\n</tr>\n<tr>\n  <th>Lag5</th>      <td>    0.0103</td> <td>    0.050</td> <td>    0.208</td> <td> 0.835</td> <td>   -0.087</td> <td>    0.107</td>\n</tr>\n<tr>\n  <th>Volume</th>    <td>    0.1354</td> <td>    0.158</td> <td>    0.855</td> <td> 0.392</td> <td>   -0.175</td> <td>    0.446</td>\n</tr>\n</table>\n```\n\n:::\n:::\n\n\n\n\nTo assess our classifier we should go through several steps first we can look at the confusion matrix. Basically we can get as very general look at how well our predictions lineup with the actual data. First we need to get our predictions and then bin them into 1's and zero\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nform_preds = form_version.predict(stocks_pd.to_pandas())\n\ny_pred = (form_preds >= 0.5).astype(int)\n\ny_actual = stocks_pd['direction'].to_pandas()\n\nconf_mat = confusion_matrix(y_actual, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n\ndisp.plot()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x15cb4f090>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](classification_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n:::\n\n\n\n\nThis is a nice intuitive display. We are seeing what the model predicted versus what actually happens. When it comes to classification we have a variety of metrics. \n\n\n\n\n\n\n```{=latex}\n\\begin{align}\n\\text{Accuracy} = \\frac{\\text{True Postive} + \\text{True Negative}}{TP + TN + FN + FP} \\\\\n\\text{Precision} = \\frac{TP}{TP + FP} \\\\\n\\text{Recall} = \\frac{TP}{TP + FN} \\\\\n\\text{F1 Score} = 2 \\times \\frac{precision \\times recall}{precision + recall}\n\n\\end{align}\n```\n\n\n\n\n\nEach of these metrics have a variety of benefits and tradeoffs.\n\n- Accuracy \n\nAccuracy is nice and intuitive what proportion of correct predictions are we making? In a perfect world this is the only thing that we would use when evaluating models. However, if we have a lot of positives and very few negatives or vice versa our model is going to get good at predicting positives. But not that great at predicting negatives. When the class balance is bad enough we are going to get high accuracy because it is good at predicting the dominant class. \n\n\n- Precision and Recall \n\nPrecision is useful if we decompose what is in the denominator. If false positives are costly meaning that if flagging something as a positive would lead to not great outcomes we may need to maximize precision. However, we should always consider how it does with recall. \nRecall is the compliment to precision. In this case we are looking at the proportion of true positives and false negatives. \n\nIf we take the case of fraud and think about it like along these lines we want to strike a balance between the two maybe slightly favoring recall. While false positives are something we want to mininmize because it can cause frustrations and eat up company resources. Which isn't good but significantly more costly. We don't want to miss actual cases of fraud. \n\n\n- F1 Score\n\nF1 score tries to strike this balance because maximizing precision or recall will lead the model to overcorrect. F1 score does a bit better with imbalanced datasets than accuracy the big drawback is we lose the interpretibility of whether the model is doing better minimizing false negatives or false positives. \n\n\n### Metrics \n- Brier Score:\nThe Brier Score is closer in spirit to the mean squared error. So in effect we are talking about the average squared distance in probability from the true class label. This may seem kind of hard to get your head around since we are just classifying something as yes or no in a binary context or yes, no, or maybe in a multiclass context. Instead of taking the predicted class which will take on a value of 1 or 0 we are instead going to use the predicted probablity of the class. So the equation looks something like \n\n$$\n\\text{Brier Score} = \\frac{1}{n} \\sigma^{n}_{t = 1} (\\text{Predicted Probablilty of class} - \\text{True Class Label})^2\n$$\n\nSo much like the MSE a lower Brier score would indicate that the squared probability distance from the actual class on average. What this also means is that in the face of class imbalances it is going to bias the dominant class. If we think about the predicted probablity as simplistically as possible then the numerator is just the predicted frequency of events. If one class dominants the set then it is likely that they are also going to dominate the the predicted frequency. \n\n- Matthew's Correlation Coefficient\n\nThe MCC measures the correlation of predicted class labels with true class labels \n\n$$\nMCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FN) \\times (TN + FN) }}\n$$\n\nThe nice thing about the MCC is that it has the same interpretation as the correlation coefficient. So values closer to one indicate that there is a strong positive correlation between the predicted class and the observed class. A value closer to negative one indicates that their is a strong negative correlation. As holistic measure of how well our classifier is doing it is less sensitive to class imbalance. However, it does not neccessarily tell us how well it is doing with respect to telling us the proportion of false positives like precision does or false negatives like recall does.\n\n\n## Test Questions \n\nSome times employers will test you on things that aren't just going through these. One real example you ran into is that you needed to hand calculate the True postives and the True negatives which you failed miserably and wasn't able to even get to the rest. \n\n\n\nYou are working on a classification problem with two classes: Class 1 and Class 2. There are a total of 2000 observations in the dataset, with 1200 observations in Class 1 and 800 observations in Class 2. Your classifier produces the following predictions:\n\nIt assigns 1000 observations to Class 1.\nIt assigns 800 observations to Class 2.\nAdditionally, your classifier correctly classifies 1000 observations in total.\n\nUsing this information:\n\nHow many observations are true positives for Class 1 and Class 2?\nHow many observations are false positives for each class?\n\nFor the True Postives we would do some \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the known values\ncorrectly_classified <- 1000\npredicted_class1 <- 1000\npredicted_class2 <- 800\nactual_class1 <- 1200\nactual_class2 <- 800\n\n\nTP2 <- (predicted_class2 - actual_class1 + correctly_classified) / 2\n\n# Solve for TP1 using TP1 + TP2 = correctly_classified\nTP1 <- correctly_classified - TP2\n\n# Solve for FP1 and FP2\nFP1 <- predicted_class1 - TP1\nFP2 <- predicted_class2 - TP2\n\nTP1 + TP2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print the results\ncat(\"True Positives for Class 1 (TP1):\", TP1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue Positives for Class 1 (TP1): 700 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True Positives for Class 2 (TP2):\", TP2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue Positives for Class 2 (TP2): 300 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"False Positives for Class 1 (FP1):\", FP1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse Positives for Class 1 (FP1): 300 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"False Positives for Class 2 (FP2):\", FP2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse Positives for Class 2 (FP2): 500 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## The workflow \n\nIn the stock market example we can do something like this. \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nx = stocks[['Lag1', 'Lag2']]\n\ny = stocks['Direction']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1994)\n\n```\n:::\n\n\n\n\nThen we fit our models like this we initiate a logit object same as we would in tidy models same goes for various other. To pair this down we are going to just use two features. One thing to note is that scikit learn regularizs the logit by default with an l2 norm aka the ridge penalty. \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n\nlogit = LogisticRegression()\n\n```\n:::\n\n\n\n\n\nSo lets go ahead and fit the logit and see how it does \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlogit_mod = logit.fit(x_train, y_train)\n\nlogit_preds = logit_mod.predict(x_test)\n\n\naccuracy_score(y_test, logit_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.508\n```\n\n\n:::\n\n```{.python .cell-code}\nconfusion_matrix(y_test, logit_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[33, 83],\n       [40, 94]])\n```\n\n\n:::\n\n```{.python .cell-code}\nclassification_report(y_test, logit_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'              precision    recall  f1-score   support\\n\\n        Down       0.45      0.28      0.35       116\\n          Up       0.53      0.70      0.60       134\\n\\n    accuracy                           0.51       250\\n   macro avg       0.49      0.49      0.48       250\\nweighted avg       0.49      0.51      0.49       250\\n'\n```\n\n\n:::\n:::\n\n\n\n\nSo it has about a coin toss chance on being right. If we wanted to give it specific values we can give it some values \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nnew_dat = pl.DataFrame({\n    'Lag1': [0.5, 0.8, 1.1],\n    'Lag2': [1.2, -0.5, 0.3],\n})\n\npredicted_probs = logit_mod.predict(new_dat)\n\n```\n:::\n\n\n\n\n\n\n\n\n## LDA \n\nLDA is like a lot of these a dimensionality reduction machine. We make some assumptions one being that the classes are linearly separable hence the L in LDA. We also assume equal variance covariance matrices that follow a multivariate normal distribution. To check this we plot the matrices and they should look like an \nellipsis. \n\n- When classes are pretty close to perfectly seperable. This is because the MLE starts to break down. Even a firth correction may not be optimal. \n\n- If we have small sample size and the distribution of the predictors is approx normal. \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlda = LinearDiscriminantAnalysis(store_covariance=True)\n\nlda_mod = lda.fit(x_train, y_train)\n\nlda_preds = lda_mod.predict(x_test)\n\nconfusion_matrix(y_test, lda_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[33, 83],\n       [40, 94]])\n```\n\n\n:::\n:::\n\n\n\n\n\nI will comeback to this but for the most part we are still doing a bad job of predicting the down direction. We are also seeing some bad things in the diagnostics. \n\n\n\n\n\n## QDA \n\nQDA is pretty similar to LDA in a lot of respects howevr it  assumes that each class has its own mean and covariance rather than enforcing and equality assumptions\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\n\nqda_mod = qda.fit(x_train, y_train)\n\n\nqda_preds = qda_mod.predict(x_test)\n\n\nconfusion_matrix(y_test, qda_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[ 27,  89],\n       [ 33, 101]])\n```\n\n\n:::\n:::\n\n\n\n\n\n## Naive Bayes \n\nNaive Bayes is a classic we make the assumption that our predictors are drawn from a gaussian distribution, that each of the features is conditionally independent, and we make the assumption that the classes are linearly seperable. What is interesting about Naive Bayes is that it works pretty well \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnb = GaussianNB()\n\nnb_mod = nb.fit(x_train, y_train)\n\n\nnb_preds = nb.predict(x_test)\n\nconfusion_matrix(y_test, nb_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[32, 84],\n       [36, 98]])\n```\n\n\n:::\n:::\n\n\n\n\n\n## K-Nearest Neighbors \n\nFinally the most \"machine-learny\" of the models of these classifiers that we have covered so far is K-Nearest neightbors. KNN is fairly intuitive things that are close to each other are more likely to be related to each other. We don't make any assumptions of the functional form of the decision boundary. For the most part each of the classifiers so far we make linearity assumptions or that the classification boundary follows a Bernoulli distribution. We also make no assumptions about the distribution of the data. This is kind of cool but as we make less and less assumptions about the data we start needing more of it. However we need to ensure that each of our features are on the same scale or the algorithm is not going to do well. If we have the difference in years versus 1,000 or millions of dollars. A jump of 100 yeasrs is substantively larger than a jump in a 100 dollars but K-nearest neighbors is going to let the larger numbers dominate. So we need to rescale everything.\n\n\nThe other thing is we don't have any a priori knowledge of the optimal number of neighbors. We have have some idea but for machine learning models we use something called a hyperparameter to improve our model. There are mechancical parts of our models that we don't have control over. In this setting we are not going to change how we calculate Euclidean distance. However, the number of neighbors to set that determines the classification boundaries are. Nothing in dataset or model can tell us what is the correct number of neighbors. We basically iterate over these to find the optimal *k* aka the optimal number of neighbors\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ncaravan = pl.read_csv('data/Caravan.csv')\n\nx_df = caravan.select(pl.exclude('Purchase'))\n\nscaler.fit(x_df)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\nx_std = scaler.transform(x_df)\n\nfeature_sd = pl.DataFrame(x_std, schema=x_df.columns)\n\n\nx_train, x_test, y_train, y_test = train_test_split(np.asarray(feature_sd),\n                                                    caravan['Purchase'],\n                                                    test_size = 1000)\n\n\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn_preds = knn.fit(x_train, y_train).predict(x_test)\n\nknn.fit(x_train, y_train).predict_proba\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<bound method KNeighborsClassifier.predict_proba of KNeighborsClassifier(n_neighbors=1)>\n```\n\n\n:::\n\n```{.python .cell-code}\nconfusion_matrix(y_test, knn_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[879,  61],\n       [ 56,   4]])\n```\n\n\n:::\n:::\n\n\n\n\n\nSo one neighbor does pretty well but what if we could do better? We can perform a grid search over the number of neighors. 10 Neighbors is probably unreasonable. Since this is not actually all that intensive we could theoretically just use a for loop to tune this parameter. However, thats not really the best way since we have built in tools. \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors' : list(range(1,10))}\n\ngrid_search = GridSearchCV(knn, param_grid, cv = 5)\n\ngrid_search.fit(x_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-2 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={&#x27;n_neighbors&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={&#x27;n_neighbors&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: KNeighborsClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(n_neighbors=8)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KNeighborsClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(n_neighbors=8)</pre></div> </div></div></div></div></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ngrid_search.best_score_\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnp.float64(0.9400666480338831)\n```\n\n\n:::\n\n```{.python .cell-code}\ngrid_search.score(x_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.939\n```\n\n\n:::\n\n```{.python .cell-code}\nbest_grid = grid_search.best_estimator_\n\n\nbest_grid_preds = best_grid.predict(x_test)\n\nconfusion_matrix(y_test, best_grid_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[939,   1],\n       [ 60,   0]])\n```\n\n\n:::\n\n```{.python .cell-code}\nbest_grid.n_neighbors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8\n```\n\n\n:::\n:::\n\n\n\n\nThis is kind of nice. So lets breakdown what we did. We k-fold cross-validation meaning we created 5 evenly sized folds where the model will be trained on k-1 fold. Meaning we trained the model on 4 folds. Then repeat this process. In a grid search we are kind of just going through each individual combination of hyperparameters. So we are doing k =1 distance = manhattan, k =1 distance = euclidean etc. So this maybe fine if we don't have a ton of things to do but if we have a ton of hyperparameters than that is not all that efficient. \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport itertools\n\np_grid_2 = {'n_neighbors': list(range(1,5)), 'metric': list(['euclidean', 'manhattan', 'minkowski'])}\n\ncombos = itertools.product(p_grid_2['n_neighbors'], p_grid_2['metric'])\n\nfor comb in combos:\n    print(comb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(1, 'euclidean')\n(1, 'manhattan')\n(1, 'minkowski')\n(2, 'euclidean')\n(2, 'manhattan')\n(2, 'minkowski')\n(3, 'euclidean')\n(3, 'manhattan')\n(3, 'minkowski')\n(4, 'euclidean')\n(4, 'manhattan')\n(4, 'minkowski')\n```\n\n\n:::\n:::\n\n\n\n\nWhereas random search is will take a random sample of these combos\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nparam_dist = {'n_neighbors': np.arange(1,5), 'metric': ['euclidean', 'manhattan', 'minkowski']}\n\nrandom_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter = 4)\n\nrandom_search.fit(x_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-3 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-3 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-3 pre {\n  padding: 0;\n}\n\n#sk-container-id-3 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-3 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-3 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-3 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-3 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-3 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-3 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-3 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-3 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-3 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-3 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-3 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-3 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-3 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-3 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-3 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-3 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n#sk-container-id-3 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-3 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-3 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-3 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-3 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-3 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-3 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-3 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-3 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-3 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={&#x27;metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;,\n                                                   &#x27;minkowski&#x27;],\n                                        &#x27;n_neighbors&#x27;: array([1, 2, 3, 4])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomizedSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(estimator=KNeighborsClassifier(), n_iter=4,\n                   param_distributions={&#x27;metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;,\n                                                   &#x27;minkowski&#x27;],\n                                        &#x27;n_neighbors&#x27;: array([1, 2, 3, 4])})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: KNeighborsClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(metric=&#x27;euclidean&#x27;, n_neighbors=np.int64(4))</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KNeighborsClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(metric=&#x27;euclidean&#x27;, n_neighbors=np.int64(4))</pre></div> </div></div></div></div></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\nbest_random = random_search.best_estimator_\n\nbest_preds_random = best_random.predict(x_test)\n\nbest_random.score(x_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.935\n```\n\n\n:::\n\n```{.python .cell-code}\nconfusion_matrix(y_test, best_preds_random)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[935,   5],\n       [ 60,   0]])\n```\n\n\n:::\n:::\n\n\n\n\n\n## Selecting the best classifier\n\n\nOften times we do something akin to this where we train a bunch of models and then have to compare which one is the best. This would be a huge pain to do manually. However this is why computers are nice \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_curve, auc\n\nscaler = StandardScaler()\n\n\nx = stocks.select(pl.col('Lag1', 'Lag2', 'Volume')).to_numpy()\n\ny =  stocks.select(pl.col('Direction')).to_numpy().flatten()\n\nlabel_encoder = LabelEncoder()\n\ny_encoded = label_encoder.fit_transform(y)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y_encoded,\n                                                    test_size = 0.2)\n\n\nx_train_scaled = scaler.fit_transform(x_train)\n\nx_test_scaled = scaler.fit_transform(x_test)\n\nclassifiers = [\n    {'label':'QDA',  'model': QuadraticDiscriminantAnalysis()},\n    {'label':\"Logit\",'model': LogisticRegression()},\n    {'label':\"LDA\",  'model': LinearDiscriminantAnalysis()}, \n    {'label':\"KNN\",  'model': KNeighborsClassifier(n_neighbors=5)}\n]\n\n\nfor m in classifiers:\n    model = m['model']\n    model.fit(x_train_scaled, y_train)\n    pred = model.predict(x_test_scaled)\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test_scaled)[:,0])# direction down \n    plt.plot(fpr, tpr, label=f'{m[\"label\"]} ROC')\n\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(0.0, 1.0)\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.ylim([0.0, 1.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(0.0, 1.05)\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()   \n```\n\n::: {.cell-output-display}\n![](classification_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\nSo we plotting ROC-AUC curves. Generally we want it to look a lot better than this. But this will give us a chance to talk about these. When it comes to evaluating models ROC-AUC curves are a favorite. These complement each other in a lot of respects. \n\nIn our accuracy, precision, recall, and F1 scores they all try to evaluate the proportion of true positives in comparision to either total classifications or something else. However, we don't really ever have a good intuition at what level we should be cutting off these judgements. ROC-AUC lets us plot the performance of our models at various thresholds and against the random chance. In this case there are lots of instances whwer our model is not even as good as the random classifier. We probably need more features in order to improve our sensitivity. \n\nIn general we want it to look more curvy where the ROC-AUC is a lot closer to the 1 on the Y axis. A flat curve indicates that our model performs as well as flipping a coin. In any classification task we are going to have some mistakes in classification no matter the threshold. A AUC of 0.8 would indicate that our classifier is going to classify that point correctly close to 80% of the time.  \n\n## What should we do if we have some class imbalance? \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tinytable)\n\ndf = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv\") |>\n    janitor::clean_names() \n\n df |>\n    group_by(success, died) |>\n    summarise(total = n()) |>\n    mutate(prop = total/sum(total)) |>\n                tt()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_fwx36oskfd52hgq0mizg(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_fwx36oskfd52hgq0mizg\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_fwx36oskfd52hgq0mizg');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_fwx36oskfd52hgq0mizg(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_fwx36oskfd52hgq0mizg\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 0, j: 0 }, { i: 0, j: 1 }, { i: 0, j: 2 }, { i: 0, j: 3 },  ], css_id: 'tinytable_css_w2jyju8i5gjpb6avv4xl',}, \n          { positions: [ { i: 4, j: 0 }, { i: 4, j: 1 }, { i: 4, j: 2 }, { i: 4, j: 3 },  ], css_id: 'tinytable_css_mdgaymbmgewkwq6jlv7m',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_fwx36oskfd52hgq0mizg(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_w2jyju8i5gjpb6avv4xl, .table th.tinytable_css_w2jyju8i5gjpb6avv4xl { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_mdgaymbmgewkwq6jlv7m, .table th.tinytable_css_mdgaymbmgewkwq6jlv7m { border-bottom: solid #d3d8dc 0.1em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_fwx36oskfd52hgq0mizg\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\">success</th>\n                <th scope=\"col\">died</th>\n                <th scope=\"col\">total</th>\n                <th scope=\"col\">prop</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>FALSE</td>\n                  <td>FALSE</td>\n                  <td>46452</td>\n                  <td>0.981656805</td>\n                </tr>\n                <tr>\n                  <td>FALSE</td>\n                  <td> TRUE</td>\n                  <td>  868</td>\n                  <td>0.018343195</td>\n                </tr>\n                <tr>\n                  <td> TRUE</td>\n                  <td>FALSE</td>\n                  <td>28961</td>\n                  <td>0.991849036</td>\n                </tr>\n                <tr>\n                  <td> TRUE</td>\n                  <td> TRUE</td>\n                  <td>  238</td>\n                  <td>0.008150964</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\nSo for the most part nicely balanced class data is pretty rare! Lets take a fairly simple from Julia Silge's [excellent blog post](https://juliasilge.com/blog/himalayan-climbing/). For the most part mountaneering trips don't tend to have a lot of deaths which is good! However, when we want to start predicting what makes a succesful climb this can be a problem if we aren't careful.\n\n### Preprocessing: A variety of sampling techniques\n\n\nSo we can think about fraud and war along similar lines in the class imbalance space. For the most part we have a ton of not war or not fraud cases in these kinds of dataset. I am sure if we start to skew the class imbalances a little bit more or even just added a third class like 'maybe fraud' or 'mid' to the COW database we are going to start pushing the limits of what the model could handle. \n\nOne of the real important things is choosing evaluation metrics that can handle imbalanced classes. As we saw in the last section accuracy tends to be a poor metric with big class imbalances becuase its the total number of correct predictions divided by the total number of predicitions. The thing about these models is that they tend to do pretty well in predicting things after they are trained on some data. So the model gets really good at predicting the dominant class but gets really bad at predicting the less dominant class. \n\nWe can pray that we get more data and just by pure chance we got a bad draw with lots of class imbalance. But, for the most part those prayers will never be answered. Fundamentally if the DGP of war or fraud change we have kind of a big problem on our hands. Instead what we can do is use as variety of resampling methods to artificially create balance between the two classes. In @tbl-resampling-methods I outline the broad strokes of each resampling method. \n\n\n\n\n\n::: {#tbl-resampling-methods .cell}\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_jcze4653rfq9be0lclan(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_jcze4653rfq9be0lclan\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_jcze4653rfq9be0lclan');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_jcze4653rfq9be0lclan(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_jcze4653rfq9be0lclan\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 0, j: 0 }, { i: 0, j: 1 },  ], css_id: 'tinytable_css_k89xh2319g1zlc8nqsc0',}, \n          { positions: [ { i: 5, j: 0 }, { i: 5, j: 1 },  ], css_id: 'tinytable_css_6qm3t82wycdgezk4rxe3',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_jcze4653rfq9be0lclan(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_k89xh2319g1zlc8nqsc0, .table th.tinytable_css_k89xh2319g1zlc8nqsc0 { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_6qm3t82wycdgezk4rxe3, .table th.tinytable_css_6qm3t82wycdgezk4rxe3 { border-bottom: solid #d3d8dc 0.1em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_jcze4653rfq9be0lclan\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\">Method</th>\n                <th scope=\"col\">What it Does</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>Oversampling                                     </td>\n                  <td>Bias the classifier towards the minority class by duplicating the minority class                                                      </td>\n                </tr>\n                <tr>\n                  <td>Undersampling                                    </td>\n                  <td>Bias the classifier towards the minority class by removing examples of the dominant class                                             </td>\n                </tr>\n                <tr>\n                  <td>Random Oversampling Examples (ROSE)              </td>\n                  <td>Generate new synthetic points with some noise to the minority class                                                                   </td>\n                </tr>\n                <tr>\n                  <td>Synthetic Minority Oversampling Technique (SMOTE)</td>\n                  <td>Generates new synthetic points by interpolating between existing points                                                               </td>\n                </tr>\n                <tr>\n                  <td>Adaptive Synthetic Sampling (ADASYN)             </td>\n                  <td>Identify hard to classify examples meaning they do not have a ton of neighbors. Generate some synthetic points via K-nearest neighbors</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\nEffectively what each of these are doing from the perspective of the classifier because it imposes non-uniform missclassfication costs. The simplest approach is to simply randomly add data to the minority class or randomly delete data from the dominant class. However, these approaches have some obvious drawbacks. One thing that you will notice is that we have to manually set a ratio. What ratio do we set 🤷. We may want to shoot for equal balance but we have kind of fundamentaly changed the DGP which does not come at zero cost. By oversampling we are increasing our computational cost because we are copying our data and we risk over fitting. For undersampling we are chucking useful information away a lot of useful information that may improve predictive power down the line. \n\nTo combat this there are various procedures to use more data driven approaches to make up data or sample data. SMOTE generates synthetic examples by selecting a minority class instance, finding one of its k-nearest neighbors (from the same class), and creating a new point along the line segment between them. The synthetic point is placed at a random position between the two, ensuring that the newly generated data follows the distribution of the minority class via K-NN. SMOTE is viable strategy if the skew isn't really bad. One of the problems is that while k-NN is kind of clever we have a hyperparameter to tune which can be computationally expensive. Since SMOTE is an oversampling technique some of the same draw backs of oversampling still apply.\n\nTo handle some of the weakpoints of SMOTE we have a family of synthetic data generators try to combat overfitting. A slightly modified version of the SMOTE framework is ADASYN which generates more and more synthetic data points near points closer to the decision boundary. Effectively what is happening is that we end up generating more and more points closer to the decision boundary with the goal of class balance. Another technique,\nROSE which is based on bootstrap re-sampling techniques. Effectively what that means is that we are going to randomly draw a row from our dataset, setting the probability of the drawing the minority and the majority class to be the same. Then we are going to generate a synthetic example in the same neighborhood with a small amount of noise estimated via a kernel density estimate. We are going to keep doing this over and over again till we get a balanced dataset.  \n\n\n\n\n\n\n",
    "supporting": [
      "classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}