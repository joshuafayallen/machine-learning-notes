{
  "hash": "09423a7200885dc7544f33c6af9588ca",
  "result": {
    "engine": "jupyter",
    "markdown": "# Unsupervised Learning\n\nIn the prior \"chapters\" we covered a variety of supervised learning techniques. By supervised we have some direct control over hyperparameter tuning and how we enter things into the model to predict a response. Unsupervised learning doesn't neccessarily have a dependent variable that we are trying to predict. Instead the unsupervised techniques covered in ISLR and ESLR look at unsupervised learning as an approach to help us uncover hidden groups in our data. They focus on PCA and various clustering methods. There are no hard and fast rules with this and we often think of it as an EDA excercise or a way to reduce the dimensionality of our data. In effect supervised learning is learning the relationship of data without labels. So say we need to build a classifier. We label the data as penguin not a penguin without then we train a model on these things. For an unsupervised model we may not care so much but may want to throw everything in a model and then see if the model can uncover groupings of penguin not penguin. \n\n\n## The Curse \n\nThe curse of dimensionality frequently comes up in machine learning because we often have a ton of predictors. The curse of dimensionality can be a little weird if you don't deal with math a whole lot. The general idea of the curse of dimensionality is that as the number of dimensions increase our models are going to start to break down because as the number of dimensions increase our model is going to find data points standing all alone. We may not know that because our model starts to do well finding things that are alone in a neighborhood. A good model will be able to find these points hiding in a crowd a bad model wouldn't! Both good and bad models will find points by themselves which is not neccessarily helpful as we start to introduce new data points the model's performance will start to detoriate. \n\n\nThe thing with the curse of dimensionality is that we never escape it because we are going to run into when modeling variables. For OLS the curse of dimensionality enters as we start to add more and more variables because we are stratiying our data by more and more information. Even when we are doing something like inverse propensity score weighting and then using these weights in a bivariate OLS you are running into the curse in the propensity score equation. \n\n\n## What can we do about the curse?\n\nWe can use methods like PCA to reduce the number of dimensions. The general idea is that we are trying to map high dimensional relationships into lower dimesional proxies. \n\n::: {#f24ad350 .cell execution_count=1}\n``` {.python .cell-code}\nimport polars as pl\nimport polars.selectors as cs \nimport matplotlib.pyplot as plt\nfrom palmerpenguins import load_penguins\n\npengs = pl.from_pandas(load_penguins())\n\npengs.select(pl.exclude('year', 'species', 'sex', 'island')).drop_nans().corr()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>bill_length_mm</th><th>bill_depth_mm</th><th>flipper_length_mm</th><th>body_mass_g</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0</td><td>-0.235053</td><td>0.656181</td><td>0.59511</td></tr><tr><td>-0.235053</td><td>1.0</td><td>-0.583851</td><td>-0.471916</td></tr><tr><td>0.656181</td><td>-0.583851</td><td>1.0</td><td>0.871202</td></tr><tr><td>0.59511</td><td>-0.471916</td><td>0.871202</td><td>1.0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nEach of these variables map on to some measure of penguin bigness some of them are more correlated with each other but if we had flipper width or bill width or some other measure of penguins bigness there is going to significant overlap. Once we enter these into a regression these models are going to pretty good job at descrbing penguin weirdness. A way to help us is to summarize these variables into what we call principal components. \n\nPrincipal components are a way to summarize the variation in these overlapping variables in a minimal set of variables. \n\n::: {#91b3e91c .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\nnormalized = pengs.select(cs.numeric(), pl.col('species')).select(pl.exclude('year')).with_columns(\n        (cs.numeric() - cs.numeric().mean() / cs.numeric().std())\n    ).drop_nulls()\n\npca = PCA()\nnumeric_data = normalized.select(cs.numeric())\npca.fit(numeric_data)\n\n# Transform the data and include species labels\ntr_df = pca.transform(numeric_data)\npca_components = pl.DataFrame(\n    tr_df,\n    schema=[f'PC{i+1}' for i in range(tr_df.shape[1])]\n).with_columns(normalized['species']) \n\npca_df = pca_components.to_pandas()\n\n# Define a color map for the species\ncolor_map = {\n    \"Adelie\": \"red\",\n    \"Gentoo\": \"blue\",\n    \"Chinstrap\": \"green\"\n}\n\n# Create the scatter plot\nplt.figure(figsize=(8, 6))\nfor species, color in color_map.items():\n    subset = pca_df[pca_df['species'] == species]\n    plt.scatter(\n        subset['PC1'], \n        subset['PC2'], \n        label=species, \n        color=color, \n        alpha=0.7\n    )\n\n# Add plot details\nplt.title('PCA: First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title='Species')\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](unsupervised-learning_files/figure-html/cell-3-output-1.png){width=670 height=523}\n:::\n:::\n\n\nWe can see that these two variables do a reasonable enough job of distinguishing the species from each other. What is going on underneath the hood is that we are creating some linear combinations of these variables. We can relax this constraint using gams if we want but in practice we do this linearly alot. \n\n\n## Clustering \n\nIn a sense we are finding subgroups with this PCA example but we can also use clustering algorithms. Basically clustering algorithms will try to ifer subgroups from the data either through minimizing a distance measure if we are using k-means. We could also use mixture clustering where we make some assumptions of the distributions but can let more moment conditions enter into the clustering algorithm.\n\n",
    "supporting": [
      "unsupervised-learning_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}