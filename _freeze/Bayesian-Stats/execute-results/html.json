{
  "hash": "06bcb74e1ec164b3c4c5e1b8fa5bc591",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayesian Statistics \n\nBayesianism has been on the to do list for awhile since well they make the prettiest plots. More importantly we get more coherent measures of uncertainty and are closer to how we actually think of the world. There are lots of broad applications for Bayesianism in the private industry. The betting market for sports and how we think about teams is effectively Bayesian. We incorporate a lot of information about the observable parts of a team like schedule, roster talent, coaching, and guesses about the probablity that the roster will stay healthy. These are all priors we have about things that relate to the success of the team going into the season. As the season progresses and we get data our prior will update. In the case of the Saints this year our prior updated to make them a good team after one game moving our guess about the number of games they would win this year to probably more than their projected win total. With the second win this may have not moved our prior much at all. However, as attrition set in then our prior shifted a little bit back toward the pre-season total so on and so forth.\n\n## What is Baye's Rule? \n\nFormally Bayes rule is usually denoted by this equation.  \n\n$$\nP(A|B) = \\frac{P(A) P(B|A)}{P(B)}\n$$\n\nWhile uber important in respect to Bayesian statistics this not always been the easiest way for me to think of Bayes rules with respect to logic of how we do Bayesian statistics. I think its hard to ground it for me because we can just rewrite it a bit and get the law of total probability which doesn't make it inherently Bayesian. For me I think it is easier to clarify it as \n\n$$\nPosterior = \\frac{Prior \\times \\text{Probability of Hypothesis being true | Data}}{\\text{Mean probability of Data}}\n$$\n\nOr we are just counting the number of instances that we observed this data and then we average over the prior which get us our average. Practically this is really just so everything adds to 1. To be a Bayesian is to incorporate our beliefs and then update them as the data meets our hypothesis meets the data or as Rich McCelreth argues \n\n   > The rules of probability tell us that the logical way to compute the plausibilities, after\naccounting for the data, is to use Bayesâ€™ theorem.\n\nWe then sample from the posterior to communicate something interesting or important about our model. Like where is the mass of the probability distribution, where is the center of this distribution, etc\n\n\nBayesianism, in a way, is a formalization of this process by encoding our beliefs in probability distributions. Our prior is really just our belief about the distribution and plausible range of values for that variable before ever seeing the data. So if we were to set a prior for a coin we would set the prior as somthing kind of loosy goosy as this \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_heads = rbinom(n = 2, size = 1000, prob = 0.5)\n\nprior_heads[1]/sum(c(prior_heads[1], prior_heads[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5045317\n```\n\n\n:::\n:::\n\n\n\n\n\n\nSo in this case we are just stating that the probability of heads is about 50 percent over a thousand trials assuming a fair coin. However, lets say that we know the coin is biased in a known way that makes it come up heads 61% of the time. We could then set our prior that it willl come up heads as this \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_heads_biased = rbinom(n = 2, size = 1000, prob = .61)\n```\n:::\n\n\n\n\n\n\nThis is not neccessarily all that unique from a standard null hypothesis which we covered in the opening \"chapter\" of this book. It is the combination of the prior and the posterior which makes the Bayesian machine go brrr. \n\nThe posterior can we operationalized along these lines \n\n$$\nPosterior = \\frac{\\text{Probability of Data} \\times Prior}{\\text{Mean probability of Data}}\n$$\n\nWhere we are averaging the probability of the prior. If we derive this out more formally we will see that the posterior is actually proportional to the prodct of the prior and the probability of the data. The posterior distribution contains the relative plausibility of different parameter values, conditional on the data and model.\nWe can overcome a bad prior given an infinite amount of data, but this process will be inefficient and critically we will never reach infinite data. Priors in Bayesian inference are important computationally as well as substantively. \n\n### What is a prior really? \n\nSetting a prior is one of the hardest things in Bayesian statistics and the subject a large and rich part of the literature. I think one really important thing to adjudicate is what a prior really is. That way we can reinforce the importance of setting a good one, what to check, and how to check it. \n\nAccording to @gelman2013bayesian we can conceptualize the prior distribution along two different lines. There is the *population* interpretation. This is a little frequentisty but is definitely helpful. Lets say that we have some pseudo population of parameters that the candidate parameter $\\beta$ is drawn from. For simplicity sake lets draw $\\beta$ from as standard normal where we overlay the overall distribution \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nconflicted::conflict_prefer('map', winner = 'purrr')\nconflicted::conflict_prefer_all(winner = 'dplyr')\n\nsim_data = \\(beta_val = 0, beta_variance = 1, n = 100){\n  \n  sim_dat = tibble(beta_values = rnorm(n, mean = beta_val, sd = beta_variance),\n                   beta_mean = rep(beta_val, length.out = n),\n                   beta_variance = rep(beta_variance, length.out = n))\n\n  return(sim_dat)\n    \n}\n\npop_sims = map_dfr(c(5, 10, 15), \\(x) sim_data(beta_val = x)) \n\nwide_version = pop_sims |> \n    mutate(id = row_number()) |>\n    pivot_wider(names_from = beta_mean, values_from = beta_values, names_glue = 'mean_{beta_mean}') \n\nplot_dat = wide_version |>\n    mutate(pop_total = pop_sims$beta_values) |>\n    pivot_longer(cols = mean_5:mean_15) |>\n    mutate(nice_labs = as.factor(str_to_title(str_replace(name, '_', \" \"))), \n           nice_labs  = fct_relevel(nice_labs, 'Mean 5', 'Mean 10', 'Mean 15'))\n\n\nggplot(plot_dat) +\ngeom_density(mapping = aes(x = value, y = after_stat(ncount),   fill = nice_labs), \n             stat = \"bin\",  size = 0.5,\n             alpha = 0.7) + \ngeom_density(mapping = aes(x = pop_total, y = after_stat(ncount)), \n                 alpha = 0.9,\n                 color = \"gray30\", size = 0.6, \n             stat = \"bin\",\n             direction = \"mid\") +\n   facet_wrap(vars(nice_labs)) +\n   MetBrewer::scale_fill_met_d(name = 'Lakota') +\n   theme_minimal() +\n   labs(fill = NULL, y = 'Scaled Count') +\n   theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/check2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nSo in the population interpretation of priors we have kind of weird population by design. Depending on what 'draw' we get our prior could be *N(5,1)* or it could be *N(10,1)* where plausible values center around 5 or 10 with a standard deviation of 1. In this setting we are thinking about given the data what is a reasonable set of values that we would expect to see. In this setting we are kind of explicitly using a bit of frequentist logic to bridge the gap. However, instead of assuming that all values between 5-15 are equally plausible we are expliciltly stating that the mass of the distribution will be around some value and the amount of variation that we will have. This could be useful if we have a huge amount of experiments or results banked and we can imagine our plausible beta values as drawn from some distribution of the experiments.\n\nIn the *state of knowledge* interpretation of priors we are still using our subject matter expertise of the phenomena, but there may not be a good population to ground our priors on. Say we are launching a completly new product or expanding our product into a new market. We may have some idea about what we are likely to see but we don't have the same reference population that we can draw from, but we have at least some idea of what our expectation should be. \n\nAn important thing to note about Bayes Rule is that we can think of Bayes rules along these lines \n\n$$\nPosterior = \\frac{\\overbrace{P(A)}^{\\text{Prior}} \\times \\overbrace{\\text{Probability of Data}}^{Likelihood}}{\\text{Average Probability of Data}}\n$$\n\nWhat this means is that effectively the posterior is a bit of a compromise between our data and our beliefs as the size of the data get bigger our prior will have less of an influence on our posterior distribution, but it will never completely evaporate. \n\n### How to set priors\n\nMost of the time people say just use your substantive knowledge. But that's generally not helpful if you haven't done that in a statistical setting. Lets take it step by step. First we generally outline what we link the distribution of our dependent variable would be. For our outcome variable we may think that its a normal distribution so we would write it like this\n\n$$\nOutcome \\sim Normal(\\mu, \\sigma)\n$$\n\n\nIf it is a binary outcome we would write it like this \n\n\n$$\nOutcome \\sim Bernouli(\\text{Number of Trials},\\text{Probability true})\n$$\n\nThe next step is is we have to think about our generative model. So to ground our analysis it helps to start from a DAG. We generally have beliefs about an intervention in the world. That way we can start thinking about the potential relationships in our data. We then have to think of the plausible ranges for these values and what the uncertainty around them. \n\n\n$$\n\\begin{align}\n\\textbf{Outcome} \\sim N(\\mu, \\sigma) \\\\[8pt]\n\\textbf{Predictor One} \\sim N (3, 1)\n\\end{align}\n$$\n\nWhere we are shifting the mean a bit and putting a somewhat conservative prior. This is generally fine but there are lots of priors we can set. \n\nLets say that out in the real world by some miracle of god we have a true normal distribution where the mean is zero and the standard deviation is 1. We can set informative, weakly informative, flat/non-informative prior, and a conjugate prior. Conjugate priors are a little bit harder to visualize with the below schema because a conjugate prior is something that also relies on the posterior. Meaning that if it turns out the posterior of our mean comes from the same family then it turns out that our prior is conjugate.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = 1000\n\nprior_df = tibble(\n    pop = rnorm(n),\n    `Kinda Flat` = rnorm(n, sd = 5), \n    informative = rnorm(n, mean = 0.5, sd = 1)\n) |>\n    pivot_longer(everything(),\n        names_to = 'prior',\n        values_to = 'value'\n    )\n\n\nggplot(prior_df, aes(x = value, fill = prior)) +\n    geom_density(alpha = 0.5) +\n    scale_fill_manual(values = c('Kinda Flat' = '#a40000', 'informative' = '#00b7a7', 'pop' = '#ffcd12')) +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThis could be better but you kind of get the idea. A flat/uninformative prior is at best a bit like saying \"the parameter of interest is somewhere between $-\\infty$ and $\\infty$.\" My flat prior in the plot is more akin to a weakly informative prior where we are ruling out impossible values, but not really ruling out extreme values. A weakly informative prior is more akin to like we wouldn't expect the treatment effect to be all that big but it is not outside of the realm of possibility that due to a weird draw of the experimental population people really take to the treatment. Priors can get pretty crazy because well the real world is messy and making simplyfying assumptions is hard. This is why (most) Bayesians will simulate the world first before even touching the data. \n\n\n\n\n\n## Simulating Worlds \n\nSimulations are superpowerful because we get to play god in a way that we don't normally get to do as social scientists. We can really simply simulate that you have roughly a 50/50 chance of getting heads or tails. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoin_flips = replicate(10000, sample(c('heads', 'tails'), 1))\n\ncoin_flips |>\n    as_tibble() |>\n    group_by(value) |>\n    summarise(counts = n()) |>\n    ungroup() |>\n    mutate(probs = counts/sum(counts))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 3\n  value counts probs\n  <chr>  <int> <dbl>\n1 heads   5048 0.505\n2 tails   4952 0.495\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nWhile this is intuitive we can see how this varies by number of flips. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_flips = 100\n\nflips = sample(c('heads', 'tails'), size = num_flips, replace = TRUE)\n\ncoin_flips = tibble(\n    heads_frequency = cumsum(flips == 'heads')/1:num_flips,\n    flip_number = 1:num_flips\n\n)\n\n\nggplot(coin_flips, aes(x = flip_number, y = heads_frequency)) +\n    geom_line() +\n    geom_hline(yintercept = 0.5) +\n    scale_y_continuous(limits = c(0,1)) +\n    labs(x = 'Flip Number', y = 'Proportion of Heads') +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nIn expectation we start getting closer and closer to 50% heads. Neat, but why should we care? Well we can test how well our prior does on various situations with a known truth. \n\nLets take a randomized control trial with unobserved confounding using the really excellent `DeclareDesign` package. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(scipen = 999)\nlibrary(DeclareDesign)\n\nrct <-\n  declare_model(N = 100,\n                U = rnorm(N),\n                potential_outcomes(Y ~ 0.2 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\nfake_data = draw_data(rct)\n```\n:::\n\n\n\n\n\n\n\nSo now we have some fake data where we can display the 'truth' or in this case 0.2 \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagnose_design(rct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResearch design diagnosis based on 500 simulations. Diagnosis completed in 2 secs. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates).\n\n Design Inquiry Estimator Outcome Term N Sims Mean Estimand Mean Estimate\n    rct     ATE estimator       Y    Z    500          0.20          0.21\n                                                     (0.00)        (0.01)\n   Bias SD Estimate   RMSE  Power Coverage\n   0.01        0.20   0.20   0.17     0.96\n (0.01)      (0.01) (0.01) (0.02)   (0.01)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nWhen we go and estimate it on some fake data we can see how keeping or omitting the unobserved confounding. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrol_for_confounding = lm(Y ~ Z + U, data = fake_data)\n\nno_controls = lm(Y ~ Z, data = fake_data)\n\n\nmodelsummary::modelsummary(list('Controls Added' = control_for_confounding,\n                                 'No Controls' = no_controls),\n                          gof_map = 'nobs',\n                          stars = TRUE)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_7sielo1mv3p9nmvzfr86(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_7sielo1mv3p9nmvzfr86\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_7sielo1mv3p9nmvzfr86');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_7sielo1mv3p9nmvzfr86(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_7sielo1mv3p9nmvzfr86\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 },  ], css_id: 'tinytable_css_zddzc7myaoa6ruev90le',}, \n          { positions: [ { i: 7, j: 1 }, { i: 7, j: 2 },  ], css_id: 'tinytable_css_x8lmp2aqrcztyca926n1',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_obnctmjkqze5jdz6p21z',}, \n          { positions: [ { i: 7, j: 0 },  ], css_id: 'tinytable_css_igwhcokm2q7gmzxrbx55',}, \n          { positions: [ { i: 0, j: 1 }, { i: 0, j: 2 },  ], css_id: 'tinytable_css_i890z0m8rvzd95qbyomo',}, \n          { positions: [ { i: 1, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 5, j: 1 }, { i: 2, j: 2 }, { i: 3, j: 2 }, { i: 4, j: 1 }, { i: 1, j: 2 }, { i: 4, j: 2 }, { i: 5, j: 2 },  ], css_id: 'tinytable_css_3t341n8ntkw5iwqozs27',}, \n          { positions: [ { i: 6, j: 1 }, { i: 6, j: 2 },  ], css_id: 'tinytable_css_2pb913fze4v68jyocwnj',}, \n          { positions: [ { i: 6, j: 0 },  ], css_id: 'tinytable_css_1s7xvgiqv7kclmewhqvy',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_7sielo1mv3p9nmvzfr86(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_zddzc7myaoa6ruev90le, .table th.tinytable_css_zddzc7myaoa6ruev90le { text-align: left; }\n      .table td.tinytable_css_x8lmp2aqrcztyca926n1, .table th.tinytable_css_x8lmp2aqrcztyca926n1 { text-align: center; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_obnctmjkqze5jdz6p21z, .table th.tinytable_css_obnctmjkqze5jdz6p21z { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_igwhcokm2q7gmzxrbx55, .table th.tinytable_css_igwhcokm2q7gmzxrbx55 { text-align: left; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_i890z0m8rvzd95qbyomo, .table th.tinytable_css_i890z0m8rvzd95qbyomo { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_3t341n8ntkw5iwqozs27, .table th.tinytable_css_3t341n8ntkw5iwqozs27 { text-align: center; }\n      .table td.tinytable_css_2pb913fze4v68jyocwnj, .table th.tinytable_css_2pb913fze4v68jyocwnj { text-align: center; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_1s7xvgiqv7kclmewhqvy, .table th.tinytable_css_1s7xvgiqv7kclmewhqvy { text-align: left; border-bottom: solid black 0.05em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_7sielo1mv3p9nmvzfr86\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">Controls Added</th>\n                <th scope=\"col\">No Controls</th>\n              </tr>\n        </thead>\n        <tfoot><tr><td colspan='3'>+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001</td></tr></tfoot>\n        <tbody>\n                <tr>\n                  <td>(Intercept)</td>\n                  <td>0.000**</td>\n                  <td>-0.153</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.000)</td>\n                  <td>(0.130)</td>\n                </tr>\n                <tr>\n                  <td>Z</td>\n                  <td>0.200***</td>\n                  <td>0.391*</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.000)</td>\n                  <td>(0.184)</td>\n                </tr>\n                <tr>\n                  <td>U</td>\n                  <td>1.000***</td>\n                  <td></td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.000)</td>\n                  <td></td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.</td>\n                  <td>100</td>\n                  <td>100</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\n\n\n\nThis is pretty cool to see how this can go if we don't account for things that should be accounted for. The same general principle applies in Bayesian analysis. The reason we simulate out a RCT is that they are super expensive! We want to diagnose what could go wrong before we tell our partners what to do. The same idea applies for Bayes because we have two separate problems that can make it hard to tell what is going on. We have computational problems that can arise due to how Bayesian models are fit and we have modeling problems which are really just scientific problems. Conceptually these are somewhat distinct but practically these two run into each other all the time. We can isolate some of the computational mechanics of fitting a bad model on data we know is 'good'. This makes fitting lots of models easier. \n\n\nHow should we simulat the data? Typically we will define a parameter that seems reasonable! So if we are trying measure. So if we were trying to model the impact of a treatment on conversion rate aka how often do we move from a free user to a subscriber setting simulating a uniform distribution across treatment and control.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(brms)\nlibrary(modelsummary)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\nconversions_tibble = tibble(\n    Control = runif(100, min = 0, max = 100),\n    Treatment = runif(100, min = 0, max = 100),\n) |>\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none') \n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nWe wouldn't expect that by doing nothing that conversion rate is uniformly distributed between 0 and 100 percent. Conversion rate for Netflix or established streaming services is probably closer to something that looks like this. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nconversions_tibble_reasonable = tibble(\n    # most in the control group don't convert \n    Control = rbeta(n = 100, shape1 = 3, shape2 = 7) * 10,\n    # people in the control group are slightly more likely to convert\n    Treatment = rbeta(n = 100, shape1 = 6, shape2 = 4) * 10,\n) |>\n    pivot_longer(everything(),\n    names_to = 'condition',\n    values_to = 'conversion_rate') \n\nggplot(conversions_tibble_reasonable, aes(x = conversion_rate, fill = condition)) +\n    geom_density() +\n    facet_wrap(vars(condition), ncol = 1) +\n    theme_minimal() + \n    labs(x = 'Conversion Rate', y = NULL) +\n    theme(legend.position = 'none') \n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Checking our Models\n\nThere are lots of ways to check our model! One way that is fairly common because Gelman reccomends it is prior predictive checks. The first is a prior predictive check which is just a way for us to check on what influence the prior is going to have on the posterior distribution. As our data grows the influence of our prior is going to decrease and the likelihood function is going to start to have more influence on the posterior distribution. Or in other words how probable the observed data is given different value of the model. However, they are not totally irrelevant the prior can impose a form of regularization shrinking the posterior predictions back towards the 'true value.' This is to say that even in large-n settings a well calibrated prior is still important in simple models. In much more complex models the prior is going to do a lot more.\n\nIn a sense we did something similar but what we are doing is modeling the problem only using the priors. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\ntitanic <- subset(titanic, PClass != \"*\")\n\nf <- Survived ~ SexCode + Age + PClass\n\n\n\nmod_prior <- brm(PClass ~ SexCode + Age,\n    data = titanic,\n     prior = c(\n        prior(normal(0, 3), class = b, dpar = \"mu2nd\"),\n        prior(normal(0, 3), class = b, dpar = \"mu3rd\")),\n    family = categorical(link = logit),\n    sample_prior = \"only\")\n\n\npp_check(mod_prior)   \n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nIn this case we the prior does okay matching the observed data. It could definitely be better\n\n:::callout\n\nLets break down some `brms` syntax first. \n\n- `prior` - somewhat self explantory\n\n- `normal` - somewhat self explantory\n\n- `class = b` - Set the prior at the population level. \n\n- `dpar = 'class'` - Here we are telling it that we are making an assumption about the probability of being in second or 3rd class relative to 1st class \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_prior |>\n    modelsummary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_d8u43cwv4flc3w05sb62(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_d8u43cwv4flc3w05sb62\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_d8u43cwv4flc3w05sb62');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_d8u43cwv4flc3w05sb62(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_d8u43cwv4flc3w05sb62\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 0, j: 1 },  ], css_id: 'tinytable_css_v3h4k6002vj2xy091lly',}, \n          { positions: [ { i: 12, j: 1 },  ], css_id: 'tinytable_css_snnrp3mrq2ylsjgvqv07',}, \n          { positions: [ { i: 6, j: 1 },  ], css_id: 'tinytable_css_q2lhcaasysl0805g1yk1',}, \n          { positions: [ { i: 1, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 4, j: 1 }, { i: 5, j: 1 }, { i: 10, j: 1 }, { i: 7, j: 1 }, { i: 8, j: 1 }, { i: 9, j: 1 }, { i: 11, j: 1 },  ], css_id: 'tinytable_css_oemjh66iobzlzh62agha',}, \n          { positions: [ { i: 12, j: 0 },  ], css_id: 'tinytable_css_k4ovx4mo4o6fkjkio0pa',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_eypzcrkpclfqbaax9xro',}, \n          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 }, { i: 10, j: 0 }, { i: 7, j: 0 }, { i: 8, j: 0 }, { i: 9, j: 0 }, { i: 11, j: 0 },  ], css_id: 'tinytable_css_b6cnpy7gdiihcov8lgga',}, \n          { positions: [ { i: 6, j: 0 },  ], css_id: 'tinytable_css_44u0cdhovzh2syld0nrd',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_d8u43cwv4flc3w05sb62(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_v3h4k6002vj2xy091lly, .table th.tinytable_css_v3h4k6002vj2xy091lly { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_snnrp3mrq2ylsjgvqv07, .table th.tinytable_css_snnrp3mrq2ylsjgvqv07 { text-align: center; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_q2lhcaasysl0805g1yk1, .table th.tinytable_css_q2lhcaasysl0805g1yk1 { text-align: center; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_oemjh66iobzlzh62agha, .table th.tinytable_css_oemjh66iobzlzh62agha { text-align: center; }\n      .table td.tinytable_css_k4ovx4mo4o6fkjkio0pa, .table th.tinytable_css_k4ovx4mo4o6fkjkio0pa { text-align: left; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_eypzcrkpclfqbaax9xro, .table th.tinytable_css_eypzcrkpclfqbaax9xro { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_b6cnpy7gdiihcov8lgga, .table th.tinytable_css_b6cnpy7gdiihcov8lgga { text-align: left; }\n      .table td.tinytable_css_44u0cdhovzh2syld0nrd, .table th.tinytable_css_44u0cdhovzh2syld0nrd { text-align: left; border-bottom: solid black 0.05em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_d8u43cwv4flc3w05sb62\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">(1)</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>b_mu2nd_Intercept</td>\n                  <td>-0.817</td>\n                </tr>\n                <tr>\n                  <td>b_mu3rd_Intercept</td>\n                  <td>1.219</td>\n                </tr>\n                <tr>\n                  <td>b_mu2nd_SexCode</td>\n                  <td>-0.019</td>\n                </tr>\n                <tr>\n                  <td>b_mu2nd_Age</td>\n                  <td>0.021</td>\n                </tr>\n                <tr>\n                  <td>b_mu3rd_SexCode</td>\n                  <td>0.018</td>\n                </tr>\n                <tr>\n                  <td>b_mu3rd_Age</td>\n                  <td>-0.046</td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.</td>\n                  <td>756</td>\n                </tr>\n                <tr>\n                  <td>ELPD</td>\n                  <td>-116646.3</td>\n                </tr>\n                <tr>\n                  <td>ELPD s.e.</td>\n                  <td>3063.0</td>\n                </tr>\n                <tr>\n                  <td>LOOIC</td>\n                  <td>233292.6</td>\n                </tr>\n                <tr>\n                  <td>LOOIC s.e.</td>\n                  <td>6126.0</td>\n                </tr>\n                <tr>\n                  <td>WAIC</td>\n                  <td>1645870.6</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\n\n\nOne important thing to note is that `brms` is based off of `lme4` which is one of the premier multilevel modeling packages for frequentist. As a political scientist we aren't really multilevel model people we are more throw OLS at everything people so some of the terminology around is different. \n\nIn OLS political science land we would say we include 'class fixed effects' to refer to a model that does something like this. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this is a bad model don't judge \nlogit_fe = glm(Survived ~ Age + SexCode + factor(PClass), data = titanic, family = binomial(link = 'logit'))\n\nmodelsummary(logit_fe)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_xlm9kjq4p2514sxl3kq1(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_xlm9kjq4p2514sxl3kq1\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_xlm9kjq4p2514sxl3kq1');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_xlm9kjq4p2514sxl3kq1(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_xlm9kjq4p2514sxl3kq1\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 16, j: 0 },  ], css_id: 'tinytable_css_z0zcpqjtl3ce6aj4ikrl',}, \n          { positions: [ { i: 10, j: 1 },  ], css_id: 'tinytable_css_xi3ulj47qbia77uvdqpj',}, \n          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 }, { i: 6, j: 0 }, { i: 7, j: 0 }, { i: 8, j: 0 }, { i: 9, j: 0 }, { i: 14, j: 0 }, { i: 11, j: 0 }, { i: 12, j: 0 }, { i: 13, j: 0 }, { i: 15, j: 0 },  ], css_id: 'tinytable_css_nw5v20tznbdzzf271ox5',}, \n          { positions: [ { i: 1, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 4, j: 1 }, { i: 5, j: 1 }, { i: 6, j: 1 }, { i: 7, j: 1 }, { i: 8, j: 1 }, { i: 9, j: 1 }, { i: 14, j: 1 }, { i: 11, j: 1 }, { i: 12, j: 1 }, { i: 13, j: 1 }, { i: 15, j: 1 },  ], css_id: 'tinytable_css_mv6rj7lp0xkd9mirl209',}, \n          { positions: [ { i: 0, j: 1 },  ], css_id: 'tinytable_css_jz2v82ee1f5k1bbgjlck',}, \n          { positions: [ { i: 10, j: 0 },  ], css_id: 'tinytable_css_c1wb0ci5dspdq70726dv',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_bxhmq4li99btelnykf8h',}, \n          { positions: [ { i: 16, j: 1 },  ], css_id: 'tinytable_css_28716hq5wvzz1yetsdi1',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_xlm9kjq4p2514sxl3kq1(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_z0zcpqjtl3ce6aj4ikrl, .table th.tinytable_css_z0zcpqjtl3ce6aj4ikrl { text-align: left; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_xi3ulj47qbia77uvdqpj, .table th.tinytable_css_xi3ulj47qbia77uvdqpj { text-align: center; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_nw5v20tznbdzzf271ox5, .table th.tinytable_css_nw5v20tznbdzzf271ox5 { text-align: left; }\n      .table td.tinytable_css_mv6rj7lp0xkd9mirl209, .table th.tinytable_css_mv6rj7lp0xkd9mirl209 { text-align: center; }\n      .table td.tinytable_css_jz2v82ee1f5k1bbgjlck, .table th.tinytable_css_jz2v82ee1f5k1bbgjlck { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_c1wb0ci5dspdq70726dv, .table th.tinytable_css_c1wb0ci5dspdq70726dv { text-align: left; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_bxhmq4li99btelnykf8h, .table th.tinytable_css_bxhmq4li99btelnykf8h { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_28716hq5wvzz1yetsdi1, .table th.tinytable_css_28716hq5wvzz1yetsdi1 { text-align: center; border-bottom: solid #d3d8dc 0.1em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_xlm9kjq4p2514sxl3kq1\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">(1)</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>(Intercept)</td>\n                  <td>1.128</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.351)</td>\n                </tr>\n                <tr>\n                  <td>Age</td>\n                  <td>-0.039</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.008)</td>\n                </tr>\n                <tr>\n                  <td>SexCode</td>\n                  <td>2.631</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.202)</td>\n                </tr>\n                <tr>\n                  <td>factor(PClass)2nd</td>\n                  <td>-1.292</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.260)</td>\n                </tr>\n                <tr>\n                  <td>factor(PClass)3rd</td>\n                  <td>-2.521</td>\n                </tr>\n                <tr>\n                  <td></td>\n                  <td>(0.277)</td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.</td>\n                  <td>756</td>\n                </tr>\n                <tr>\n                  <td>AIC</td>\n                  <td>705.1</td>\n                </tr>\n                <tr>\n                  <td>BIC</td>\n                  <td>728.3</td>\n                </tr>\n                <tr>\n                  <td>Log.Lik.</td>\n                  <td>-347.570</td>\n                </tr>\n                <tr>\n                  <td>F</td>\n                  <td>50.339</td>\n                </tr>\n                <tr>\n                  <td>RMSE</td>\n                  <td>0.38</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\n\n\nWhen we say class fixed effects we really just mean including an intercept per classs. In multilevel land what we mean by a fixed effect is really just saying that these effects are constant across groups. So think of the effect of school on classroom performance or the effect of product popularity on ROI on adverstising channels. The way we would replicate the this is something to the effect of \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm(Survived ~ Age + (1|PClass), data = titanic) |>\n   modelsummary()\n```\n:::\n\n\n\n\n\n\nAdmittedly also a really bad model but you get the idea. \n\n:::\n\nWe can then check our priors. We can also set priors on the intercept in our Titanic example if we stripped away age, class, and sex what would our baseline expectation of survival be? In an RCT this is really just what is a reasonable expectation for our control group. For other more complex models we could put priors on the effect of time, space, interactions between groupings and so much more. \n\n\n\n## Using Brms \n\nAs an R Bayesian we use brm mostly. We are going to walk through a variety of models that you have fit before using frequentist methods using brms. As the excercise progressed it reallly just became redoing the Statistical Rethinking \n\n\n### Linear Models \n\nSo as a good Bayesian we are going to set some priors. First we are going to set a prior for the height in cm and then we are going to set a scale parameter for the weight in kg's and stretches the distribution. The sigma parameter must be positive. The alpha parameter we are going to think about well what if all the covariates were zero? What would a reasonable set of values be? In this case what would be the weight of an individual be if we never took into account height? Well a somewhat reasonable prior is just taking the average of the weight and spreading it out. \n\nStan really likes when you scale things because well it makes everything easier to compute. Rescaling in this case gives us the benefit of making the intercept interpretable. Inside the regression the intercept parameter now just becomes the expected weight of an individual is when they are of average height. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata('Howell1', package = 'rethinking')\n\njust_adults = Howell1 |>\n    filter(age >= 18) |>\n    mutate(height_z = scale(height))\n\nall.equal(just_adults$height_z, (just_adults$height - mean(just_adults$height))/sd(just_adults$height))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Attributes: < Modes: list, NULL >\"                   \n[2] \"Attributes: < Lengths: 3, 0 >\"                       \n[3] \"Attributes: < names for target but not for current >\"\n[4] \"Attributes: < current is not list-like >\"            \n[5] \"target is matrix, current is numeric\"                \n```\n\n\n:::\n\n```{.r .cell-code}\npriors = c(# this is just weight in kilos so approx 132 lbs \n           prior(normal(60, 10), class = Intercept),\n           # this is just centering on the new variable. Now we are also \n           # this is kilos/cm with a spread of 10^2 \n           prior(normal(0, 10), class = b, coef = 'height_z'),\n           prior(uniform(0,50), class = sigma, ub = 50))\n\nfirst_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors,\n    data = just_adults\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 1:                0.012 seconds (Sampling)\nChain 1:                0.026 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.011 seconds (Sampling)\nChain 2:                0.024 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.011 seconds (Sampling)\nChain 3:                0.024 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 4:                0.013 seconds (Sampling)\nChain 4:                0.027 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(first_cut)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nSo far the chains look really nice there is great mixing. Everything you would want! However, if we simulate our prior we see  an interesting phenomena \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_prior = first_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors,\n    data = just_adults,\n    sample_prior = 'only'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.48 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.006 seconds (Warm-up)\nChain 1:                0.006 seconds (Sampling)\nChain 1:                0.012 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.006 seconds (Warm-up)\nChain 2:                0.006 seconds (Sampling)\nChain 2:                0.012 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.007 seconds (Warm-up)\nChain 3:                0.006 seconds (Sampling)\nChain 3:                0.013 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.006 seconds (Warm-up)\nChain 4:                0.005 seconds (Sampling)\nChain 4:                0.011 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nheight_scale <- attributes(just_adults$height_z) %>%\n  set_names(janitor::make_clean_names(names(.)))\n\ndraws_prior <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 1000)) |>\n                add_epred_draws(sample_prior, ndraws = 1000) |>\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot(draws_prior, aes(x = height_unscaled, y = .epred, group = .draw)) +\n    geom_line(alpha = 0.2) +\n    coord_cartesian(xlim = c(130, 170), ylim = c(10, 100)) +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nThis plot looks super wonky we wouldn't expect that as height increases weight would decrease which seems to be the case for a fair number of cases. What we are seeing is that not out maliciousness we have set up a weird world for our model to calibrate to. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(sample_prior)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nIf we look a the posterior predictions they look a little weird. We have some draws that go negative implying there are negative weights. To recalibrate our model a bit we need to simply choose a prior that constrains our world a bit more. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors2 = c(prior(normal(60, 10), class = Intercept),\n            # here we are actually just constraining \n            # beta to be positive \n           prior(lognormal(0, 1), class = b, lb = 0 ),\n            prior(uniform(0,10), class = sigma, ub = 10))\n\nsample_prior2 =  brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors2,\n    data = just_adults,\n    sample_prior = 'only'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 1:                0.01 seconds (Sampling)\nChain 1:                0.021 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 2:                0.009 seconds (Sampling)\nChain 2:                0.02 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 3:                0.01 seconds (Sampling)\nChain 3:                0.021 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 4:                0.009 seconds (Sampling)\nChain 4:                0.019 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\ndraws_prior2 <- tibble(height_z = seq((130 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     (170 - height_scale$scaled_center) / height_scale$scaled_scale, \n                                     length.out = 1000)) |>\n                add_epred_draws(sample_prior2, ndraws = 1000) |>\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\nggplot(draws_prior2 ,aes(x = height_unscaled,  y = .epred, group = .draw)) +\n    geom_line(alpha = 0.2) +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThis is definitely a lot better! Out of 1000 draws we get only two weird samples, but that happens. For the most part we can live with this. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(sample_prior2)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThis is definitely at least encouraging! We are not getting really really weird predictions. There are definitely some extreme ones in there, but I wouldn't be like to shocked. \n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model = first_cut = brm(\n    weight ~ 1 + height_z,\n    family = gaussian,\n    prior = priors2,\n    data = just_adults\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.012 seconds (Sampling)\nChain 1:                0.024 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 2:                0.011 seconds (Sampling)\nChain 2:                0.023 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.011 seconds (Sampling)\nChain 3:                0.024 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.01 seconds (Sampling)\nChain 4:                0.022 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nfinal_draws = tibble(height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 500)) |> \n  add_predicted_draws(final_model, ndraws = 100) |>\n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\n\nggplot(just_adults, aes(x = height, y  = weight)) +\n    geom_point(alpha = 0.5) +\n    stat_lineribbon(data = final_draws,\n    aes(x = height_unscaled, y = .prediction), .width = 0.95, alpha = 0.2, inherit.aes = FALSE) +\n    theme_minimal() +\n    theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nShe looks great! What about the posterior predictions? \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(final_model)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nShe is beautiful. However if we look at the data \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(just_adults, aes(x = height, y = weight, color = as.factor(male))) +\n    geom_point() +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nAs you would expect we see that there group differences. So if our dag looks like this. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdag)\nlibrary(dagitty)\n\ncoords = list(x = c(Height = 0, Sex = 1, Weight = 2),\n              y = c(Height = 0, Sex  = 1, Weight = 0))\n\nlabs = c(Height = 'Height', Sex = 'Sex', Weight = 'Weight')\n\ndagify(Weight ~ Height + Sex,\n        Height ~ Sex,\n        outcome = 'Weight',\n        exposure = 'Height',\n        labels = labs,\n        coords = coords) |>\n    ggdag_status(use_labels = 'label', text = FALSE) +\n    guides(fill = 'none', color = 'none') +\n    theme_dag()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nRich advises using index variables for things we would normally make a factor or an indicator variable. His argument is that this makes it easier to set a prior on each the different levels of the category. What this does is split apply our prior to both levels of sex. If we wanted to get crazy I think if we wanted to get crazy we could actually estimate a multilevel model. Will it outperform a linear model? Â¯\\_(ãƒ„)_/Â¯. In this case we could create a fairly neutral prior \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\njust_adults = just_adults |>\n    mutate(sex = factor(male),\n           height_z = as.numeric(height_z))\n\nsex_priors = c(\n    # we do be because we are going to specify a model sans intercept\n    prior(normal(60, 10), class = b),\n    prior(uniform(0,10), class = sigma, lb = 0, ub = 10)\n)\n\nsex_model = brm(weight ~ 0 + sex,\n                prior = sex_priors,\n                data = just_adults)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.46 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 1:                0.014 seconds (Sampling)\nChain 1:                0.028 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 2:                0.012 seconds (Sampling)\nChain 2:                0.026 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 3:                0.011 seconds (Sampling)\nChain 3:                0.025 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.015 seconds (Warm-up)\nChain 4:                0.014 seconds (Sampling)\nChain 4:                0.029 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsex_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: weight ~ 0 + sex \n   Data: just_adults (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsex0    41.85      0.40    41.05    42.61 1.00     4502     2709\nsex1    48.60      0.43    47.73    49.42 1.00     3848     3013\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.52      0.20     5.14     5.94 1.00     3740     2674\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n\n\n\nNow we have our indicator variables. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsw_post_means = sex_model |>\n    gather_draws(b_sex0, b_sex1)\n\n\nggplot(sw_post_means, aes(x = .value, fill = .variable)) +\n    stat_halfeye() +\n    labs(fill = NULL, x = 'Posterior mean weight') +\n    theme_minimal() +\n    theme(legend.position = 'bottom')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nSo we could just subtract the the variables like this. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiffs_manual = sw_post_means |>\n    pivot_wider(names_from = .variable, values_from = .value) |>\n    mutate(diff = b_sex1 - b_sex0)\n\ndiffs_brms = sex_model |>\n    spread_draws(b_sex0, b_sex1) |>\n    mutate(diff = b_sex1 - b_sex0)\n\ntinytest::expect_equal(diffs_brms, diffs_manual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n----- PASSED      : <-->\n call| tinytest::expect_equal(diffs_brms, diffs_manual) \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nWe could also use marginaleffects. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(marginaleffects)\n\nme_contrasts = get_draws(avg_comparisons(sex_model))\n\ntinytest::expect_equal(me_contrasts$draw, diffs_brms$diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n----- PASSED      : <-->\n call| tinytest::expect_equal(me_contrasts$draw, diffs_brms$diff) \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(me_contrasts, aes(x = draw)) +\n    stat_halfeye() +\n    labs(x = 'posterior contrasts') +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nWe can also get posterior prediced draws with marginal effects via \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds =  tibble(sex = c(\"0\", \"1\")) |> \n  add_predicted_draws(sex_model, ndraws = 1000)\n\nggplot(preds, aes(x = .prediction, fill = sex)) +\n    stat_halfeye() +\n    theme_minimal() +\n    theme(legend.position = 'bottom') \n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nJust to be sure lets make sure the model makes sense \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_sex_priors = brm(\n    weight ~ 0 + sex,\n    prior = sex_priors,\n    data = just_adults,\n    sample_prior = 'only'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 1:                0.009 seconds (Sampling)\nChain 1:                0.019 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 2:                0.008 seconds (Sampling)\nChain 2:                0.018 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 3:                0.008 seconds (Sampling)\nChain 3:                0.018 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 4:                0.008 seconds (Sampling)\nChain 4:                0.018 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\npp_check(check_sex_priors)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nEhh it somewhat hard to tell but in all honesty it seems like we are a little bit streching the issue a bit. Now we can use a little parlor trick to avoid some of the the wonky syntax of the `nl` argument of brms. It will kick a stink but she still works.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(60, 10), class = b, coef = 'sex1'),\n            prior(lognormal(0, 1), class = b, coef = 'sex1:height_z'),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex <- brm(\n  bf(weight ~ 0 + sex + sex:height_z),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: Rejecting initial value:\nChain 1:   Error evaluating the log probability at the initial value.\nChain 1: Exception: lognormal_lpdf: Random variable is -1.51746, but must be nonnegative! (in 'anon_model', line 20, column 2 to column 40)\nChain 1: Rejecting initial value:\nChain 1:   Error evaluating the log probability at the initial value.\nChain 1: Exception: lognormal_lpdf: Random variable is -1.77275, but must be nonnegative! (in 'anon_model', line 20, column 2 to column 40)\nChain 1: \nChain 1: Gradient evaluation took 4.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 1:                0.024 seconds (Sampling)\nChain 1:                0.059 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 2:                0.021 seconds (Sampling)\nChain 2:                0.049 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: Rejecting initial value:\nChain 3:   Error evaluating the log probability at the initial value.\nChain 3: Exception: lognormal_lpdf: Random variable is -0.930299, but must be nonnegative! (in 'anon_model', line 20, column 2 to column 40)\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 3:                0.02 seconds (Sampling)\nChain 3:                0.047 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: Rejecting initial value:\nChain 4:   Error evaluating the log probability at the initial value.\nChain 4: Exception: lognormal_lpdf: Random variable is -1.30846, but must be nonnegative! (in 'anon_model', line 20, column 2 to column 40)\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 4:                0.019 seconds (Sampling)\nChain 4:                0.048 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\npriors2 <- c(prior(normal(60, 10), class = b, nlpar = a),\n            prior(lognormal(0, 1), class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), class = sigma, lb = 0, ub = 10))\n\nmodel_height_sex_wonky =brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors2,\n  chains = 4, cores = 4\n)\n\n# model_height_sex \n# Regression Coefficients:\n#               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sex0             45.15      0.45    44.27    46.00 1.00     2560     2372\n# sex1             45.15      0.46    44.22    46.04 1.00     2734     2716\n# sex0:height_z     5.08      0.48     4.14     6.02 1.00     2846     2773\n# sex1:height_z     4.65      0.42     3.82     5.47 1.00     2716     2618\n# Further Distributional Parameters:\n#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sigma     4.27      0.16     3.96     4.60 1.00     4054     2961\n\n# model_height_sex_wonky\n# Regression Coefficients:\n#               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sex0             45.15      0.45    44.27    46.00 1.00     2560     2372\n# sex1             45.15      0.46    44.22    46.04 1.00     2734     2716\n# sex0:height_z     5.08      0.48     4.14     6.02 1.00     2846     2773\n# sex1:height_z     4.65      0.42     3.82     5.47 1.00     2716     2618\n# \n# Further Distributional Parameters:\n#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n# sigma     4.27      0.16     3.96     4.60 1.00     4054     2961\n```\n:::\n\n\n\n\n\n\n\nSome of the logic of brms with the posterior is a little weird for me but there is not neccessarily a good comparisions in `marginaleffects` that I am aware of, but is probably going to be obvious if I just ask Vincent. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsex_height_weight_post_pred <- expand_grid(\n  height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 50),\n  sex = 0:1\n) |> \n  add_predicted_draws(model_height_sex, ndraw = 4000) |>\n  compare_levels(variable = .prediction, by = sex, comparison = list(c(\"0\", \"1\"))) |> \n  mutate(height_unscaled = (height_z * height_scale$scaled_scale) + height_scale$scaled_center)\n\n\nggplot(sex_height_weight_post_pred, aes(x = .prediction)) +\n  stat_halfeye(fill = 'red') +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen âˆ’ Men\", y = \"Density\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nHow do we interpret this kind of wonky thing? Well we are really justing talking about the distribution of the predicted differences between men and women's weight. If we want to talk about the differences meaningfully between two different distributions then we have to take the differences. T\n\n### Full Luxury Bayes \n\nReally what we want is one model to represent the entire causal system to get the effect of sex on weight without a ton of additional work. What we are going to do is basically estimate the effect of sex on weight through height. It is a little bit easier to think of the effect of like this. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndagify(Weight ~ Height,\n       Height ~ Sex) |>\n    ggdag() +\n    theme_dag()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nTo do this we have to set our priors accordingly to account for the path through height.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors <- c(prior(normal(60, 10), resp = weight, class = b, nlpar = a),\n            prior(lognormal(0, 1), resp = weight, class = b, nlpar = b, lb = 0),\n            prior(uniform(0, 10), resp = weight, class = sigma, lb = 0, ub = 10),\n            # prior(normal(160, 10), resp = height, class = b),\n            prior(normal(0, 1), resp = heightz, class = b),\n            prior(uniform(0, 10), resp = heightz, class = sigma, lb = 0, ub = 10))\n\nmodel_luxury <- brm(\n  bf(weight ~ 0 + a + b * height_z,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE) + \n    bf(height_z ~ 0 + sex) + \n    set_rescor(TRUE),\n  data = just_adults,\n  family = gaussian(),\n  prior = priors\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000212 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 3.184 seconds (Warm-up)\nChain 1:                1.582 seconds (Sampling)\nChain 1:                4.766 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000104 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 2.135 seconds (Warm-up)\nChain 2:                1.836 seconds (Sampling)\nChain 2:                3.971 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000104 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 3.522 seconds (Warm-up)\nChain 3:                1.763 seconds (Sampling)\nChain 3:                5.285 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000104 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 2.971 seconds (Warm-up)\nChain 4:                1.92 seconds (Sampling)\nChain 4:                4.891 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nluxury_post_mean_diff <- expand_grid(\n  height_z = seq(min(just_adults$height_z), max(just_adults$height_z), length.out = 50),\n  sex = 0:1\n) |> \n  add_epred_draws(model_luxury) |>\n  compare_levels(variable = .epred, by = sex, comparison = list(c(\"1\", \"0\")))\n\nluxury_post_mean_diff |> \n  filter(.category == \"weight\") |> \n  ggplot(aes(x = .epred)) +\n  stat_halfeye(fill = 'pink') +\n  labs(x = \"Posterior mean weight contrast (kg)\\nWomen âˆ’ Men\", y = \"Density\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n### An Aside on the `add_foo_draws` Family\n\n`Brms` has lots of ways to draw predictions from the posterior distribution! This is great, but they are a little bit different in subtle ways that can make it a hard to grasp at first. As with lots of things we are just going to follow an excellent [Andrew Heiss blog post](http://andrewheiss.com/blog/2022/09/26/guide-visualizing-types-posteriors/#tldr-diagrams-and-cheat-sheets) to understand the differences. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(palmerpenguins)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(ggtext)\n\ntheme_pred <- function() {\n  theme_minimal(base_family = \"Roboto Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 0),\n          legend.title = element_text(face = \"bold\"))\n}\n\ntheme_pred_dist <- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          axis.text.y = element_blank(),\n          panel.grid.major.y = element_blank(),\n          panel.grid.minor.y = element_blank())\n}\n\ntheme_pred_range <- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          panel.grid.minor.y = element_blank())\n}\n\nclrs <- MetBrewer::met.brewer(\"Java\")\n\npenguins =   penguins |>\ndrop_na(sex) |> \n  mutate(is_gentoo = species == \"Gentoo\") |> \n  mutate(bill_ratio = bill_depth_mm / bill_length_mm)\n\n\nmodel_normal <- brm(\n  bf(body_mass_g ~ flipper_length_mm),\n  family = gaussian(),\n  data = penguins\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.108 seconds (Warm-up)\nChain 1:                0.012 seconds (Sampling)\nChain 1:                0.12 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.116 seconds (Warm-up)\nChain 2:                0.013 seconds (Sampling)\nChain 2:                0.129 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.115 seconds (Warm-up)\nChain 3:                0.014 seconds (Sampling)\nChain 3:                0.129 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.093 seconds (Warm-up)\nChain 4:                0.014 seconds (Sampling)\nChain 4:                0.107 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\npenguins_avg_flipper <- penguins |> \n  summarize(flipper_length_mm = mean(flipper_length_mm))\n\n# Extract different types of posteriors\nnormal_linpred <- model_normal |> \n  linpred_draws(newdata = penguins_avg_flipper)\n\nnormal_epred <- model_normal |> \n  epred_draws(newdata = penguins_avg_flipper)\n\nnormal_predicted <- model_normal |> \n  predicted_draws(newdata = penguins_avg_flipper,\n                  seed = 12345)  \n\n\np1 <- ggplot(normal_linpred, aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[3]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Linear predictor** <span style='font-size: 14px;'>*Âµ* in the model</span>\",\n       subtitle = \"posterior_linpred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist() +\n  theme(plot.title = element_markdown())\n\np2 <- ggplot(normal_epred, aes(x = .epred)) +\n  stat_halfeye(fill = clrs[2]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Expectation of the posterior** <span style='font-size: 14px;'>E[*y*] and *Âµ* in the model</span>\",\n       subtitle = \"posterior_epred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\np3 <- ggplot(normal_predicted, aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[1]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(2900, 5500)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Posterior predictions** <span style='font-size: 14px;'>Random draws from posterior Normal(*Âµ*, *Ïƒ*)</span>\",\n       subtitle = \"posterior_predict(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\n(p1 / plot_spacer() / p2 / plot_spacer() / p3) +\n  plot_layout(heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nSo both `posterior_linpred` and `posterior_epred` are lookig at the $\\mu$ part of the model or effectively \n\n$$\n\\mu = \\alpha + \\beta\\text{Flipper Length}\n$$\n\n\nFor `posterior_linpred` what we are doing are we going to take the individual predictions from our model, the intercept and multiply them by the average flipper length. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_predictions_manual = model_normal |>\nspread_draws(b_Intercept, b_flipper_length_mm) |>\n mutate(mu = b_Intercept + \n           (b_flipper_length_mm * penguins_avg_flipper$flipper_length_mm))\n\ntinytest::expect_equal(linear_predictions_manual$mu, normal_linpred$.linpred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n----- PASSED      : <-->\n call| tinytest::expect_equal(linear_predictions_manual$mu, normal_linpred$.linpred) \n```\n\n\n:::\n:::\n\n\n\n\n\n\nReally importanly what you will notice is that we don't incorporate any inforamtion from $\\sigma$ or the posterior or the part of the model that remains uncertain after modeling the outcome. As you might imagine that may have a some important implications on what the predictions from the posterior look like! \n\n`posterior_predict` uses this sigma term to effectively take samples from the normal around the mu with the sigma as a standard deviation. So the only thing we change in our code, is this \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npostpred_manual = model_normal |>\nspread_draws(b_Intercept, b_flipper_length_mm, sigma) |>\n mutate(mu = b_Intercept + \n        (b_flipper_length_mm * penguins_avg_flipper$flipper_length_mm),\n       predictions = rnorm(n(), mu, sigma))\n\ntinytest::expect_equal(postpred_manual$predictions, normal_predicted$.prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n----- FAILED[data]: <-->\n call| tinytest::expect_equal(postpred_manual$predictions, normal_predicted$.prediction)\n diff| Mean relative difference: 0.1059483 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nThat makes sense since we are taking some samples. However, if we plot them they are going to look the same. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmanual = ggplot(postpred_manual, aes(x = predictions)) + \n    stat_halfeye(fill = 'hotpink') +\n    labs(title = \"**Posterior predictions** <span style='font-size: 14px;'>Random draws from posterior Normal(*Âµ*, *Ïƒ*)</span>\", subtitle = \"rnorm(b_Intercept + (b_flipper_length_mm * 201), sigma)\") +\n    theme_pred_dist() +\n    theme(plot.title = element_markdown())\n\nmanual /p3\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nSince we are incorporating the remaining uncertainty from the model we are going to have a little bit of a larger spread. \n\n\n`epreds` is a little weird. In many cases they can be the same as the linear predcitons. In fact if we did `epreds`we would get just get the linear predicitions returned back to us. `epreds` is short for Expected predicitions. This will combine the `E[y] ` and $\\mu$. Thankfully we have a really easy case to look at \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_mod = brm(\n    is_gentoo ~ flipper_length_mm,\n    family = bernoulli(link = 'logit'),\n    data = penguins\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.041 seconds (Warm-up)\nChain 1:                0.039 seconds (Sampling)\nChain 1:                0.08 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.042 seconds (Warm-up)\nChain 2:                0.04 seconds (Sampling)\nChain 2:                0.082 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.04 seconds (Warm-up)\nChain 3:                0.034 seconds (Sampling)\nChain 3:                0.074 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.042 seconds (Warm-up)\nChain 4:                0.041 seconds (Sampling)\nChain 4:                0.083 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nepreds_logit = logit_mod |>\n    epred_draws(newdata = penguins_avg_flipper)\n\nlinear_pres_logit = logit_mod |>\n    linpred_draws(newdata = penguins_avg_flipper)\n\nepreds_logit_plot = ggplot(epreds_logit, aes(x = .epred)) +\n    stat_halfeye() +\n    labs(title = 'Expected Predictions') +\n    theme_minimal()\n\nlogit_linpreds = ggplot(linear_pres_logit, aes(x = .linpred)) +\n    stat_halfeye() +\n    labs(title = 'Linear Predicitions')+ \n    theme_minimal()\n\n\nepreds_logit_plot/logit_linpreds\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThese are super different! In this case `epreds` is going to take the predictions on the probability scale while `linpreds` is going to use the logit scales or the log odds scale(ewww). One of the things about `epreds` is it will shift a lot. For a beta regression the expectation will effectively be the average of the posterior's average. \n\n\n### Working with exponetials and Stratifying by Continous variables\n\nWhen we adjust for a continous variable we are going adjust for the actual values of the continous variables and the slope because every value produces a different relationshiop between the treatment and the ouctcome. Scaling helps because we can think of big effects in terms of the mean and the sd. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata('WaffleDivorce', package = 'rethinking')\n\n\n\n\nwaffle_clean = WaffleDivorce |>\n    janitor::clean_names() |>\n    mutate(across(c(marriage, divorce, median_age_marriage),  \\(x) scale(x), .names = \"{col}_scaled\")) |>\n    mutate(across(c(marriage, divorce, median_age_marriage), \\(x)  as.numeric(scale(x)), .names = \"{col}_z\"))\n\nwaffle_priors = c(\n    prior(normal(0, 0.2), class = Intercept),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z'),\n    prior(normal(0,0.5), class = b, coef = 'marriage_z'),\n    prior(exponential(1), class = sigma))\n\n\nscaled_waffles = brm(\n    divorce_z ~ marriage_z + median_age_marriage_z,\n    data = waffle_clean,\n    prior = waffle_priors,\n    sample_prior = 'only' \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 1:                0.01 seconds (Sampling)\nChain 1:                0.021 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 2:                0.01 seconds (Sampling)\nChain 2:                0.021 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 3:                0.01 seconds (Sampling)\nChain 3:                0.021 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 4:                0.009 seconds (Sampling)\nChain 4:                0.019 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\n# range(waffle_clean$median_age_marriage_z)\n\ncheck_priors = tibble(median_age_marriage_z = seq(-3, 3),\n                     marriage_z = 0) |>\n                add_epred_draws(scaled_waffles, ndraws = 100)\n\nggplot(check_priors, aes(x = median_age_marriage_z, y = .epred, group = .draw)) +\n    geom_line(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThese are some of the implied regression lines from the model. For the most part we some pretty plausible regression lines. We have some weird ones where as the age of marriage increases the divorce rate decreases. Other regression lines we have mostly flat which I guess would make sense since we are not stratifying by state. We can then go ahead and estimate the model. I will spare myself the tedium of specifying two models. Instead we will estimate the effect of age of marriage through marriage rate. I suppose I didn't explain it when I did it earlier because I was getting to angry at getting the `:` syntax to work correctly. \n\nSo the idea is that we really have two models. The effect of Age on Divorce and the effect of Age on marriage. So we have to tell `brms` how do to this because simply doing \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm(divorce_z ~ marriage_z + age_z, data = d)\n```\n:::\n\n\n\n\n\n\n\nWould just adjust for the age of marriage in our model. This is great if we are only interested in the effect of marriage rate on divorce but is not our stated goal. There are lots of ways to invoke this in brms. I will probably just separate these into two different objects just for ease of reading. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwaffle_priors_two = c(\n    prior(normal(0, 0.2), class = Intercept, resp = divorcez),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z', resp = divorcez),\n    prior(normal(0,0.5), class = b, coef = 'marriage_z', resp = divorcez),\n    prior(exponential(1), class = sigma, resp = divorcez),\n    prior(normal(0, 0.2), class = Intercept, resp = marriagez),\n    prior(normal(0, 0.5), class = b, coef = 'median_age_marriage_z', resp = marriagez),\n    prior(exponential(1), class = sigma, resp = marriagez))\n```\n:::\n\n\n\n\n\n\nSince we technically have two regression models smooshed together we have to add priors for this model as well. To make sure `brms` doesn't get mad at us we feed just use `resp` to tell it what equation the prior is for. We could obviously change the prior around age of marriage. This could get **super** complicated if we needed it, but for now we are gonna k.i.s.s. \n\nIn the gender example that we worked on earlier we did `set_rescor(TRUE)` which is going to allow correlation betwee the two different equations. This makes sense because we are making the assumption that there are probably other factors that influence this relationship that we don't observe. In our waffle divorce model we would also want to include the effect of region/state. But in our DAG we don't include it for pedagogical purposes\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndivorce_model = bf(divorce_z ~ median_age_marriage_z + marriage_z)\n\nmarriage_rate_model = bf(marriage_z ~ median_age_marriage_z)\n\nconflicted::conflict_prefer('stanfit', 'rstan')\n\nfull_luxury_model = brm(\n    divorce_model + marriage_rate_model + set_rescor(FALSE),\n    prior = waffle_priors_two,\n    data = waffle_clean,\n    cores = 4\n)\n```\n:::\n\n\n\n\n\n\nWhats cool about this approach is we have modeled a few effects. We can get the effect of age on divorce and age on marriage rate. All we need to do is extract the predictions from the posterior and set the correct response variable.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndivorce_predictions = tibble(median_age_marriage_z = seq(-2, 2, length.out = 40 ),\n                             # make sure we don't use a grid of marriage rates \n                             marriage_z = 0) |>\n                       add_predicted_draws(full_luxury_model, resp = 'divorcez')\n\nmarriage_predictions = tibble(median_age_marriage_z = seq(-2, 2, length.out = 40 )) |>\n                       add_predicted_draws(full_luxury_model, resp = 'marriagez')\n```\n:::\n\n\n\n\n\n\n\n\n## Regularization and Cross-Validation in Bayesian Models \n\nRegularization in Bayes works a bit differently then in frequentism/machine learning. Instead of simply adding a penalty term to the sum of the squared residuals we can impose some what 'stronger weakly informative priors' or there are priors that are kid of just used for regularization. To pick a model that performs well we should probably have an idea about how to pick a good model. \n\nA good model is one that balances good fit with flexibility to incorporate new data and describes the world realistically. This is probably the most straightforward way to describe the bias variance tradeoff. We will use the example from the book of brain size vs body mass \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# getting tired of  + theme_minmal\n\ntheme_set(theme_minimal())\n\n d <- \n  tibble(species = c(\"afarensis\", \"africanus\", \"habilis\", \"boisei\", \"rudolfensis\", \"ergaster\", \"sapiens\"), \n         brain   = c(438, 452, 612, 521, 752, 871, 1350), \n         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))\nggplot(d, aes(x = mass, y = brain)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nThis will also be a useful way to learn how to use the update function. We are standardizing the variables becaus\n\n::: {.callout}\nwe want to standardize body massâ€“give it mean zero and standard deviation oneâ€“and rescale the outcome, brain volume, so that the largest observed value is 1. Why not standardize brain volume as well? Because we want to preserve zero as a reference point: No brain at all. You canâ€™t have negative brain. I donâ€™t think.\n\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = d |>\n    mutate(mass_std  = (mass - mean(mass)) / sd(mass),\n         brain_std = brain / max(brain))\n\nbrain_model_priors = c(\n    prior(normal(0.5, 1), class = Intercept),\n    prior(normal(0,10), class = b),\n    prior(lognormal(0,1), class = sigma))\n\n\nbrain_model = brm(\n    brain_std  ~  mass_std,\n    prior = brain_model_priors,\n    data = d,\n    file = 'fits/brain_model',\n    chains = 4, cores = 4\n)\n\nbrain_sqr = update(\n    brain_model,\n    newdata = d,\n    brain_std ~ mass_std + I(mass_std^2),\n    chains =4, cores =4,\n    file = 'fits/brain_model_sqr'\n)\n\nbrain_cubed = update(\n    brain_model,\n    newdata = d, \n         formula = brain_std ~ mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4),\n         iter = 2000, warmup = 1000, chains = 4, cores = 4,\n         seed = 7,\n         control = list(adapt_delta = .995)\n)\n```\n:::\n\n\n\n\n\n\n\nI am less dedicated (to this excerise than Soloman Kurz)[https://bookdown.org/content/4857/ulysses-compass.html#the-problem-with-parameters] so I am not going to go through the trouble of messing with stan directly. But we can see the logic of what is going on \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrain_loo_lins =\\(mod, ylim = range(d$brain_std)){\n\n nd <- tibble(mass_std = seq(from = -2, to = 2, length.out = 200))\n  \n  # simulate and wrangle\n  fitted(mod, newdata = nd, probs = c(.055, .945)) |> \n    data.frame() |> \n    bind_cols(nd) |> \n    \n    # plot!  \n    ggplot(aes(x = mass_std)) +\n    geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5)) +\n    geom_point(data = d,\n               aes(y = brain_std)) +\n    labs(\n         x = \"body mass (std)\",\n         y = \"brain volume (std)\") +\n             coord_cartesian(xlim = c(-1.2, 1.5),\n                    ylim = c(0.3, 1.0))\n  \n\n}\n\nmod_list = list(brain_model, brain_sqr, brain_cubed)\n\nplots = purrr::map(mod_list, \\(x) brain_loo_lins(x)) \n\nwrap_plots(plots, ncol =1)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nObviously fitting a bunch of terms to like 4 data points is not an ideal way to model data. But this kind of mimics how we do a lot of stats in data science. One way to can shrink our estimates is to use regularization. To do this we can just set a more restrictive prior. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat = tibble(x = seq(from = - 3.5, to = 3.5, by = 0.01)) |>\n  mutate(a = dnorm(x, mean = 0, sd = 0.2),\n         b = dnorm(x, mean = 0, sd = 0.5),\n         c = dnorm(x, mean = 0, sd = 1.0)) |> \n  pivot_longer(-x) \n\n\nggplot(dat, aes(x = x, y = value, color = name, fill = name)) +\n    geom_area(alpha = 0.4, linewidth = 0.5, position = 'identity') +\n    theme(legend.position = 'none')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nWhat is really strange if you are coming from a pure machine learning background is that we don't neccesasrily need to implement a pure version of Ridge regression in Bayes. We can simply place more restrictive priors on the model using the good old fashion $\\beta \\approx N(0,1)$ or placing a Laplace prior on the coefficients to achieve the same effect. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sim   <- 1e3\nkseq    <- 1:5\n\nmake_sim <- function(n, b_sigma) {\n  sapply(kseq, function(k) {\n    print(k);\n    r <- replicate(n_sim, rethinking::sim_train_test(N = n, k = k, b_sigma = b_sigma));c(mean(r[1, ]), mean(r[2, ]), stats::sd(r[1, ]), stats::sd(r[2, ])) \n    }\n    ) |> \n    \n    # this is a new line of code\n    data.frame()\n}\n\ns <-\n  crossing(n  = c(20, 100),\n           b_sigma = c(1, 0.5, 0.2)) |> \n  mutate(sim = map2(n, b_sigma, make_sim)) |> \n  unnest(sim)\n\n\nn_sim   <- 1e3\nn_cores <- 8\nkseq    <- 1:5\n\n# define the simulation function\nmy_sim <- function(k) {\n  \n  print(k);\n  r <- replicate(n_sim, rethinking::sim_train_test(N = n, k = k));\n  c(mean(r[1, ]), mean(r[2, ]), stats::sd(r[1, ]), stats::sd(r[2, ]))\n  \n}\n\n\n# here's our dev object based on `N <- 20`\nn      <- 20\ndev_20 <-\n  sapply(kseq, my_sim)\n\n# here's our dev object based on N <- 100\nn       <- 100\ndev_100 <- \n  sapply(kseq, my_sim)\n\n\n\n\ndev_tibble <-\n  rbind(dev_20, dev_100) |> \n  data.frame() |> \n  mutate(statistic = rep(c(\"mean\", \"sd\"), each = 2) |> rep(x = _, times = 2),\n         sample    = rep(c(\"in\", \"out\"), times = 2) |> rep( x = _, times = 2),\n         n         = rep(c(\"n = 20\", \"n = 100\"), each = 4)) |> \n  pivot_longer(-(statistic:n)) |> \n  pivot_wider(names_from = statistic, values_from = value) |>\n  mutate(n     = factor(n, levels = c(\"n = 20\", \"n = 100\")),\n         npar = str_extract(name, \"\\\\d+\") |> as.double()) |> \n  mutate(npar = ifelse(sample == \"in\", npar - .075, npar + .075))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nThe argument that Rich makes is that regularization is often a bit skeptical in Bayesian statistics. The priors are bit harder to tune. In frequentist statistics we search a large number of very small values of the hyperparameter. This can take a fair amount of time. Now think about searching through the posterior distribution for a ton of different models for the ideal prior. This can take a long time. However, there are a lot of work going into regularization priors since we use them for BART and other estimation methods. \n\n\n## Taming Markov Chains\n\nAll the core algorithms have different ways to sample from the posterior. Stan uses a Hamiltonian Monte Carlo (HMC) \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rethinking)\nlibrary(animation)\n\nmyU2 <- function( q , a=0 , b=1 , k=0 , d=0.5 ) {\n    s <- exp(q[2]) # sigma on log latent scale\n    mu <- q[1]\n    U <- sum( dnorm(y,mu,s,log=TRUE) ) + dnorm(mu,a,b,log=TRUE) + dnorm(q[2],k,d,log=TRUE)\n    return( -U )\n}\n\n# gradient function\n# need vector of partial derivatives of U with respect to vector q\nmyU_grad2 <- function( q , a=0 , b=1 , k=0 , d=0.5 ) {\n    mu <- q[1]\n    s <- exp(q[2])\n    G1 <- sum( y - mu ) * exp(-2*q[2]) + (a - mu)/b^2 #dU/dmu\n    G2 <- sum( (y - mu)^2 ) * exp(-2*q[2]) - length(y) + (k-q[2])/d^2 #dU/ds\n    return( c( -G1 , -G2 ) ) # negative bc energy is neg-log-prob\n}\n\n# test data\nset.seed(7)\ny <- abs(rnorm(50))\ny <- c( y , -y ) # ensure mean is zero\n\n###########\n# example paths\nlibrary(shape) # for good arrow heads\n# blank(bty=\"n\")\n\n# priors\npriors <- list()\npriors$a <- 0\npriors$b <- 1\npriors$k <- 0\npriors$d <- 0.3\n\n#ss <- ss + 1\nset.seed(42) # seed 9 for examples\n\n# init\nn_samples <- 4\nQ <- list()\nQ$q <- c(-0.4,0.2)\nxr <- c(-0.6,0.6)\nyr <- c(-0.25,0.4)\n\nstep <- 0.02\nL <- 12 # 0.02/12 okay sampling --- 0.02/20 is good for showing u-turns\nxpos <- c(4,2,1,2) # for L=20\n#xpos <- c(2,3,2,1) # for L=55\npath_col <- col.alpha(\"black\",0.5)\n\ndraw_bg <- function() {\n    plot( NULL , ylab=\"log_sigma\" , xlab=\"mu\" , xlim=xr , ylim=yr )\n    # draw contour of log-prob\n    cb <- 0.2\n    mu_seq <- seq(from=xr[1]-cb,to=xr[2]+cb,length.out=50) \n    logsigma_seq <- seq(from=yr[1]-cb,to=yr[2]+cb,length.out=50)\n    z <- matrix(NA,length(mu_seq),length(logsigma_seq))\n    for ( i in 1:length(mu_seq) )\n        for ( j in 1:length(logsigma_seq) )\n            z[i,j] <- myU2( c( mu_seq[i] , logsigma_seq[j] ) , a=priors$a , b=priors$b , k=priors$k , d=priors$d )\n    cl <- contourLines( mu_seq , logsigma_seq , z , nlevels=30 )\n    for ( i in 1:length(cl) ) lines( cl[[i]]$x , cl[[i]]$y , col=col.alpha(\"black\",0.5) , lwd=1 )\n}\n\n\nQ <- list()\nQ$q <- c(-0.4,0.2) # start point\nxr <- c(-0.4,0.4) # x range in plot\nyr <- c(-0.25,0.3) # y range in plot\n\ndraw_bg()\n\nn_samples <- 10\n# points( Q$q[1] , Q$q[2] , pch=4 , col=\"black\" )\npts <- matrix(NA,nrow=n_samples,ncol=3)\n\nfor ( i in 1:n_samples ) {\n\n    Q <- HMC2( myU2 , myU_grad2 , step , L , Q$q , a=priors$a , b=priors$b , k=priors$k , d=priors$d )\n\n    draw_bg()\n\n    # draw previous points\n    if ( i > 1 ) {\n        for ( j in 1:(i-1) ) {\n            V <- 0.9\n            points( pts[j,1] , pts[j,2] , pch=ifelse( pts[j,3]==1 , 1 , 16 ) , col=grau(V) , lwd=2 )\n        }\n    }\n\n    # draw trajectory\n    for ( l in 1:L ) {\n        lines( Q$traj[l:(l+1),1] , Q$traj[l:(l+1),2] , col=\"white\" , lwd=8 )\n        lines( Q$traj[l:(l+1),1] , Q$traj[l:(l+1),2] , col=4 , lwd=5 )\n        ani.record()\n    }\n    #points( Q$traj[2:L+1,] , pch=16 , col=\"white\" , cex=0.3 )\n\n    # draw new point\n    pts[i,1:2] <- Q$traj[L+1,]\n    pts[i,3] <- Q$accept\n\n    #Arrows( Q$traj[L,1] , Q$traj[L,2] , Q$traj[L+1,1] , Q$traj[L+1,2] , arr.length=0.3 , arr.adj = 0.7 , col=4 )\n    #text( Q$traj[L+1,1] , Q$traj[L+1,2] , i , cex=1.2 , pos=1 , offset=0.4 )\n    \n    points( Q$traj[L+1,1] , Q$traj[L+1,2] , pch=ifelse( Q$accept==1 , 1 , 16 ) , col=ifelse( Q$accept==1 , 4 , 2 ) , lwd=2 )\n\n    invisible( replicate( 3 , ani.record() ) )\n\n}\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-50-10.png){width=672}\n:::\n:::\n\n\n\n\n\n\nIn a way we are effectively just using gradient descent to get our posterior. What we are doing is just throwing a marble into a bowl with n-dimensions and then just recording where it lands. Since it is a bowl the marble is going to end towards the center of the bowl. We are going to use step size to tell our arm how hard to throw the marble. In gradient descent we really just use these throws to give the marble a hint what path would be faster to minimize a loss function.\n\nIn two or three dimensions this is hard when we start adding in the complexities of sampling from a huge dimensional space lots of problems can arise. As stated early on problems with computing or divergent chains are some indicators that our model may not be working all that well. When taming our markov chains it can help to set more informative priors. Even Stan doesn't get mad at you its important to inspect the health of the markov chains. To make this easier on ourselves lets just simulate a model. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata('Wines2012', package = 'rethinking')\n\n\nwine_clean = Wines2012 |>\n    mutate(score_z = scale(score),\n           wine = as.numeric(wine))\n\npriors = c(\n    prior(normal(0,1), class = b, coef = 'wine'),\n    prior(exponential(1), class = sigma)\n)\n\nwine_score_model = brm(score_z ~ 0 + wine,\nprior = priors,\n data = wine_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.009 seconds (Sampling)\nChain 1:                0.021 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 2:                0.009 seconds (Sampling)\nChain 2:                0.02 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.021 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 4:                0.008 seconds (Sampling)\nChain 4:                0.021 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(wine_score_model)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n### Trace Plots \n\nMost of the libraries have some built in method of trace plots. Visual diagnostics of MCMC chains tend towards the idea of 'this looks like a jumbly mess' so when it looks like a random mess that is good.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MetBrewer)\nmanual_trace_data = as_draws_df(wine_score_model)\n\nggplot(manual_trace_data, aes(x = .iteration, y  = b_wine, color = as.factor(.chain))) +\n    geom_line(alpha = 0.5) +\n    scale_color_met_d(name = 'Lakota')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThis is kind of what you want it should look like lots of random lines. Lets make some bad priors to see what happens\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors_bad = c(\n    prior(lognormal(0,10), class = b, coef = 'wine'),\n    prior(exponential(4), class = sigma)\n)\n\nbad_wine_score_model = brm(score_z ~ 0 + wine,\nprior = priors_bad,\n data = wine_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.762 seconds (Warm-up)\nChain 1:                0.74 seconds (Sampling)\nChain 1:                1.502 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: Rejecting initial value:\nChain 2:   Error evaluating the log probability at the initial value.\nChain 2: Exception: lognormal_lpdf: Random variable is -0.05627, but must be nonnegative! (in 'anon_model', line 19, column 2 to column 41)\nChain 2: Rejecting initial value:\nChain 2:   Error evaluating the log probability at the initial value.\nChain 2: Exception: lognormal_lpdf: Random variable is -0.851774, but must be nonnegative! (in 'anon_model', line 19, column 2 to column 41)\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.729 seconds (Warm-up)\nChain 2:                1.211 seconds (Sampling)\nChain 2:                1.94 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.767 seconds (Warm-up)\nChain 3:                1.054 seconds (Sampling)\nChain 3:                1.821 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: Rejecting initial value:\nChain 4:   Error evaluating the log probability at the initial value.\nChain 4: Exception: lognormal_lpdf: Random variable is -0.727429, but must be nonnegative! (in 'anon_model', line 19, column 2 to column 41)\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.754 seconds (Warm-up)\nChain 4:                0.789 seconds (Sampling)\nChain 4:                1.543 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nbad_chains = as_draws_df(bad_wine_score_model)\n\nggplot(bad_chains, aes(x = .iteration, y = b_wine,\n                       color = as.factor(.chain),\n                       group = as.factor(.chain))) +\n    geom_line() +\n scale_color_met_d(name = 'Lakota')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nI played up the priors a big but as you can see these chains are really bad. We have what looks to be clear trends in the first and fourth chains. We cahn't really even see the second and third chain. Overall this is a mess. \n\n## Trace Rank Plots (Trank Plots)\n\nWe can also use rank orders to get a sense of the health of the chains. Basically no chain should consistently look the best. They should look like some jumbled up mess. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(bad_wine_score_model) |>\n    bayesplot::mcmc_rank_overlay()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nAs you can see it looks like a few chains are actually broken while others just seem to be doing bettter than others. Good trank plots look more like this:\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(wine_score_model) |>\n    bayesplot::mcmc_rank_overlay() +\n    scale_y_continuous(limits = c(30,65)) +\n    scale_color_met_d(name = 'Lakota')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nwhere no one chain consistently outranks any other chain consistently. \n\n\n## Numeric diagnositcs \n\n### $\\hat{R}$ \n\nWhen chains converge the end of the chain and the beggining of a chain are exploring the same region. Independent chains explore the same or similar region. $\\hat{R}$ is effectively a ratio of variances so we can think of it as \n\n$$\n\\hat{R} = \\frac{\\text{Within Chain Variance}}{\\text{Total Variance of All Chains}}\n$$\n\nIn practice this means that values closer to give us some indication of how well are chains are doing. Values like 1.01 could tell us that we just need to run the chains for longer or that we need to go back and rethink our priors.  \n\n### n_eff or Number of Effective Samples\n\n\nThis is an estimate of the number of effective samples what this is kind of telling us is that \n\n::: {.callout-important}\nHow long would the chain be if each sample was independent of the one before it\n:::\n\nMcElreath argues that we can think of `rnorm` as the ideal behavior of `n_eff` where each draw from a normal distribution is independent of the one before it. So our number of `n_eff` would be equal to what ever you feed the `n` argument. The same idea applies to `n_eff` where we are going to get some number that is generally less than the number of samples we are taking, but sometimes Stan will find places where we can grow the length of the chain. This is not a bug because, well, when we are traversing multidimensional spaces we are not going to be as clever about that as some may think. We would see the number off effective samples shrink if we had autocorrelation because past values of the variables are giving us more information about the variable we are modeling so the length of the chain will be much shorter. The toy bad model \n\n### A toy exmaple\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(bad_wine_score_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: score_z ~ 0 + wine \n   Data: wine_clean (Number of observations: 180) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nwine     0.00      0.00     0.00     0.00 1.18       20       66\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.24      0.47     0.83     2.04 3.47        4       16\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAs we can see in the basd wine model we have really really short chains and Rhat values that tell us that the chains aren't mixing all that well! When we made the trace plot we saw that some chains were actually broken so the short length of chains make a ton of sense. When we compare this to a good model the difference is pretty stark \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(wine_score_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: score_z ~ 0 + wine \n   Data: wine_clean (Number of observations: 180) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nwine    -0.01      0.01    -0.02     0.01 1.00     3758     2717\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.00      0.05     0.90     1.11 1.00     2974     2528\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### The Folk Theorum of Statistical Computing \n\nOften times when we have computational problems with our model we have issues with our scientific problems with our model. Calculating the posterior is hard thats why the racist Sir Ronald Fisher was a coward and took the log of the likelihood. When we are first building a model we will often have problems getting the chains to work well because our assumptions about the world get passsed onto our model. The trouble that we are having sampling from the posterior distribution maybe because it doesn't make any sense.\n\n\n\n\n## Building an Interaction in brms \n\nWe often have conditional hypothesis especially in the social sciences. Political science in particular uses a lot of interaction terms. Some of our most cited statistics articles are interaction terms. Importanly for multilevel models we often times have interactions with lots of factors. Inidividuals are going to interact with where they are from or what party they identify with. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata('rugged', package = 'rethinking')\n\nrugged = rugged |>\n    drop_na(rgdppc_2000) |>\n    mutate(log_gdp = log(rgdppc_2000),\n           log_gdp_z = log_gdp/mean(log_gdp),\n           rugged_std = rugged/max(rugged),\n           rugged_std_c = rugged_std - mean(rugged_std))\n```\n:::\n\n\n\n\n\n\nLets take a crack at it with some bad priors \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors_first_cut = c(\n    prior(normal(1, 1), class = Intercept),\n    prior(normal(0,1), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_first_cut,\n    sample_prior = 'only'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 1:                0.01 seconds (Sampling)\nChain 1:                0.019 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 2:                0.01 seconds (Sampling)\nChain 2:                0.019 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.018 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 4:                0.009 seconds (Sampling)\nChain 4:                0.019 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |>\n    add_linpred_draws(check_priors, ndraws = 100) |>\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nHere we are getting some pretty crazy looking regression lines when we move back to the unscaled version. By scaling things we can make lots of priors look pretty reasonable but when we put them back on the og scale we are going to run into some issues. \n\n\nLets constrain the priors a bit more to the defaultish values Rich suggests \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors_rich = c(\n    prior(normal(1, 0.1), class = Intercept),\n    prior(normal(0,0.5), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_rich,\n    sample_prior = 'only'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.009 seconds (Sampling)\nChain 1:                0.021 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 2:                0.009 seconds (Sampling)\nChain 2:                0.021 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.019 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.01 seconds (Sampling)\nChain 4:                0.022 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |>\n    add_linpred_draws(check_priors, ndraws = 100) |>\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nHere we are getting slightly more plausible lines but we still have more than our fair share of weird worlds. We could make it a little bit better by tuning the prior a bit more. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npriors_rich = c(\n    prior(normal(1, 0.1), class = Intercept),\n    prior(normal(0,0.3), class = b),\n    prior(exponential(1), class = sigma))\n\n\n\ncheck_priors = brm(\n    log_gdp_z ~ rugged_std_c,\n    data = rugged, \n    prior = priors_rich,\n    sample_prior = 'only'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 1:                0.009 seconds (Sampling)\nChain 1:                0.02 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 2:                0.009 seconds (Sampling)\nChain 2:                0.02 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.02 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 4:                0.01 seconds (Sampling)\nChain 4:                0.021 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\npriors_dat = tibble(rugged_std_c = seq(-2,2)) |>\n    add_linpred_draws(check_priors, ndraws = 100) |>\n    mutate(unscaled_rugged = rugged_std_c + mean(rugged$rugged_std))\n\n\nggplot(priors_dat, aes(x = unscaled_rugged, y = .linpred, group = .draw)) +\n    geom_line() +\n    coord_cartesian(xlim = c(0, 1),\n                    ylim = c(0.5, 1.5))\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nCool now we just need to add an interaction with Rich's index approach. Now we can add the priors to each of the 'intercept terms'. The annoying part is that the nlpar are really just place holders where we are going to set priors for each level of the interaction. This would be fine but for now this would be incredibly annoying. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrugged = rugged |>\n    mutate(cid = ifelse(cont_africa == 1, '1', '2'))\n\nint_priors = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n                prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n                prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n                prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n                prior(exponential(1), class = sigma))\n\n\nint_model_binary = brm(\n    bf(log_gdp_z ~ 0 + a  + b * rugged_std_c,\n    a ~ 0 + cid,\n    b ~ 0 + cid,\n    nl = TRUE),\n    data = rugged,\n    prior = int_priors)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.067 seconds (Warm-up)\nChain 1:                0.065 seconds (Sampling)\nChain 1:                0.132 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.078 seconds (Warm-up)\nChain 2:                0.064 seconds (Sampling)\nChain 2:                0.142 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.071 seconds (Warm-up)\nChain 3:                0.067 seconds (Sampling)\nChain 3:                0.138 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.07 seconds (Warm-up)\nChain 4:                0.058 seconds (Sampling)\nChain 4:                0.128 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nI have never directly interpreted an interaction term in my life and have been told never too. So I am just going to run it through marginaleffects. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprds  = predictions(int_model_binary,\n                    newdata = datagrid(cid = c('1', '2'),\n                                       rugged_std_c = mean))\n\npreds_plot = get_draws(prds) |>\n    transform(type = 'response') |>\n    mutate(cid = ifelse(cid == '1', 'Africa', 'Not Africa'))\n\n\nggplot(preds_plot, aes(x = draw, fill = cid)) +\n    stat_halfeye() +\n    scale_fill_met_d('Lakota') +\n    labs(y = 'Density', x = 'Predicted Values of Log GDP')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nHere are computing predictions across each level of our binary indicator while holding ruggedness at its mean value. Here we are seeing that the predictionns for continents outside of Africa cluster around above one indicating we would have above average GDP per capita when ruggedness is held at its mean values. This is a little bit weird to think about since we don't really have a good intuition about the values of the data. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunscaled = preds_plot |>\n    mutate(draw = exp(draw * mean(rugged$log_gdp)))\n\n\nggplot(unscaled, aes(x = draw, fill = cid)) +\n    stat_halfeye() +\n    scale_fill_met_d('Lakota') +\n    labs(y = 'Density', x = 'Predicted Values GDP')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nTo replicate the work done in Rethinking we can simply do:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- \n  crossing(cid= 1:2,\n           rugged_std = seq(from = -0.2, to = 1.4, length.out = 30)) |> \n  mutate(rugged_std_c = rugged_std - mean(rugged$rugged_std))\n\ncountries <- c(\"Equatorial Guinea\", \"South Africa\", \"Seychelles\", \"Swaziland\", \"Lesotho\", \"Rwanda\", \"Burundi\", \"Luxembourg\", \"Greece\", \"Switzerland\", \"Lebanon\", \"Yemen\", \"Tajikistan\", \"Nepal\")\n\nfit = fitted(\n    int_model_binary,\n    newdata = nd, \n    probs = c(0.015, 0.985)\n) |>\n    data.frame() |>\n    bind_cols(nd) |>\n    mutate(cont_africa = ifelse(cid == 1, 'African Nations', 'Non-African Nations'))\n\ncleaned = rugged |>\n    mutate(cont_africa = ifelse(cid == 1, 'African Nations', 'Non-African Nations'))\n\n\nggplot(cleaned, aes(x = rugged_std, y = log_gdp_z, fill = cont_africa, color = cont_africa)) +\n    geom_smooth(data = fit,\n                aes(y = Estimate, ymin = Q1.5, ymax = Q98.5)) +\n    coord_cartesian(xlim = c(0, 1)) +\n    theme(legend.position = \"none\") +\n    facet_wrap(vars(cont_africa))\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-65-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n## Continous Interactions\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(tulips, package = 'rethinking')\n\nd = tulips |>\n    mutate(blooms_std  = blooms/max(blooms),\n           water_cent = water - mean(water),\n           shade_cent = shade - mean(shade))\n```\n:::\n\n\n\n\n\n\n\nFortunately in the case of continous by continous interactions we really only need to put a prior on one term so we can just do. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncont_priors = c(\n    prior(normal(0.5, 0.25), class = Intercept),\n    prior(normal(0, 0.25), class = b, coef = water_cent),\n    prior(normal(0, 0.25), class = b, coef = shade_cent),\n    prior(normal(0, 0.25), class =b, coef = 'water_cent:shade_cent'),\n    prior(exponential(1), class = sigma)\n)\n\ncont_inter = brm(\n    blooms_std ~ water_cent:shade_cent + water_cent + shade_cent,\n    data = d,\n    prior = cont_priors\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.016 seconds (Warm-up)\nChain 1:                0.016 seconds (Sampling)\nChain 1:                0.032 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.017 seconds (Warm-up)\nChain 2:                0.016 seconds (Sampling)\nChain 2:                0.033 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.018 seconds (Warm-up)\nChain 3:                0.016 seconds (Sampling)\nChain 3:                0.034 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.018 seconds (Warm-up)\nChain 4:                0.016 seconds (Sampling)\nChain 4:                0.034 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n\n\n\nCool now lets try to recreate the plot in Rethinking using `marginaleffects` \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds = get_draws(\n    predictions(cont_inter, newdata = datagrid(\n        shade_cent = -1:1,\n        water_cent = -1:1\n    ))\n) |>\n    mutate(nice_labs = glue::glue('Shade (centered) = {shade_cent}'))\n\nggplot(preds, aes(x = water_cent, y = draw, group = drawid)) +\n    geom_smooth(alpha = 1/5) +\n    geom_point(alpha = 0.5) +\n    facet_wrap(vars(nice_labs)) +\n    labs(y = 'Blooms (Standardized)', x = 'Water (centered)')\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-68-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Modeling DGP's of all different flavors. \n\nIt would be nice if everything was linear. It is nice that OLS generally performs pretty well in a lot of cases where it has no reason behaving that way. However, modeling the DGP correctly often is helpful if anything to give us some sanity checks. \n\n\n## Logits \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(UCBadmit,package = 'rethinking')\n\nadmit_data = UCBadmit\n```\n:::\n\n\n\n\n\n\nCounts and events are inherently bounded either by force of de facto bounded. We have been using the exponential distribution in a lot of our priors. Effectively the exponetial distribution is just a distribution that tells us about a time to an event that has a constant rate. We have really just been using it to constrain the unexplained variance to be postive. The difficulty with setting priors for a logit is that we are doing it on the log odds scale so they don't always have an intuitive mapping as they did in OLS land. \n\nWe can kind of think of it as anything +4 = almost always and anything -4=almost never so when we stretch out the prior or compress it on the logit scale it will create probability distributions we may not have a good intuition of what the prior should be. Fortunately we can get a handle on this \n\n\n\n\n\n\n### Beta Regression \n\nThese are uber popular amongst the Bayesians that I know for a variety of reasons. When we assume that there is a gaussian distribution we are assuming that the distribution is symetric around the mean.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(a = rnorm(n = 1000, mean = 10))|>\nmutate(b = ifelse(a > 10, 'Above the mean', 'Below the Mean')) |>\ngroup_by(b) |>\nsummarise(total = n()) |>\nmutate(proportion = total/sum(total)) |>\ntinytable::tt()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_zmxseish6biqlx90phdz(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_zmxseish6biqlx90phdz\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_zmxseish6biqlx90phdz');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_zmxseish6biqlx90phdz(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_zmxseish6biqlx90phdz\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 0, j: 0 }, { i: 0, j: 1 }, { i: 0, j: 2 },  ], css_id: 'tinytable_css_qnqhgvrackg5ouhej7qn',}, \n          { positions: [ { i: 2, j: 0 }, { i: 2, j: 1 }, { i: 2, j: 2 },  ], css_id: 'tinytable_css_44rlqdmctdz1kjb305k1',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_zmxseish6biqlx90phdz(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_qnqhgvrackg5ouhej7qn, .table th.tinytable_css_qnqhgvrackg5ouhej7qn { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_44rlqdmctdz1kjb305k1, .table th.tinytable_css_44rlqdmctdz1kjb305k1 { border-bottom: solid #d3d8dc 0.1em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_zmxseish6biqlx90phdz\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\">b</th>\n                <th scope=\"col\">total</th>\n                <th scope=\"col\">proportion</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>Above the mean</td>\n                  <td>459</td>\n                  <td>0.459</td>\n                </tr>\n                <tr>\n                  <td>Below the Mean</td>\n                  <td>541</td>\n                  <td>0.541</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\n\n\nAs we can see we have rougly equal amounts of values above and below the mean. While this works in lots of processes this can impose unrealistic expectations on the model. In Bayes rules they use the example of election polls where support may not be symetrically distributed around the average. If we had a graph that looked more like this. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nelections = tibble(polls = seq(0.25, 0.4, length.out = 10000)) |>\n    slice_sample(n = 200) |>\n    mutate(polls_normal = rnorm(200, mean = 0.3, sd = 0.1)) |>\n    pivot_longer(everything()) |>\n    mutate(name = ifelse(name == 'polls', 'Real Polls', 'Polls Assuming Normal Distribution'))\n\nggplot(elections, aes(x = value)) +\n    geom_histogram(color = 'white') +\n    facet_wrap(vars(name))\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nMaybe in expectiation we could get to the normal distribution but that is not all that helpful. The beta distrbution is a litle bit closer to how the polls behave and lots of other proportions. So we effectively set a beta prior like this \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumber_correct = 3\n\nnumber_incorrect = 7 \n\ntibble(exam_score = rbeta(100, shape1 =  number_correct, shape2 = number_incorrect)) |>\n    mutate(exam_score = exam_score * 10) |>\n    ggplot(aes(x = exam_score)) +\n    geom_histogram(color = 'white', binwidth = 1)\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nOne of the things that makes the Beta prior a little bit weird if you have never played around with it is that we would be defined as \n\n$$\n\\pi \\sim  Beta(\\alpha, \\beta)\n$$\n\n\n\n\n\nWhere $\\alpha$ kind of represents the most frequent values and $\\beta$ kind of stretches the distribution backwards or forwards. If it is less than the most frequent value it will stretch it towards 1 while if it is greater than the most frequent value it will stretch it back towards zero and if it is really close to the most frequent value, \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarious_betas = purrr::map(c(1,2,3,4,5,6,7,8,9,10,11), \\(x) tibble( vals = rbeta(n = 1000, shape1 = 4, shape2 = x))) |>\n    list_rbind(names_to = 'beta_shape') |>\n    mutate(beta_shape = glue::glue('&beta; = {beta_shape}'))\n\n\nggplot(various_betas, aes(x = vals)) +\n    geom_histogram(color = 'white') +\n    facet_wrap(vars(beta_shape)) +\n    theme(strip.text = element_markdown())\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-73-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nI am just going to follow [Andrew Heiss's blog post on beta regression](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvdem_clean <- vdemdata::vdem |> \n  select(country_name, country_text_id, year, region = e_regionpol_6C,\n         polyarchy = v2x_polyarchy, corruption = v2x_corr, \n         civil_liberties = v2x_civlib, prop_fem = v2lgfemleg, v2lgqugen) |> \n  filter(year >= 2010, year < 2020) |> \n  drop_na(v2lgqugen, prop_fem) |> \n  mutate(quota = v2lgqugen > 0,\n         prop_fem = prop_fem / 100,\n         polyarchy = polyarchy * 100)\n```\n:::\n\n\n\n\n\n\nOne of the things about tuning a beta prior is that they can very easily get you some weird shapes. If we recast the problem a bit we can turn it into a $\\mu$ and $\\phi$(kind of like the variance). Where we can think of the problem along these lines.  \n\n$$\n\\begin{aligned}\nshape1 = \\mu \\times \\phi\n\\end{aligned}\n$$\n\n$$\nshape2 = (1 - \\mu) \\times \\phi\n$$\n\n\nWe can wrap our head around it a bit more if we translate it into an R function \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapes_to_muphi <- function(shape1, shape2) {\n  mu <- shape1 / (shape1 + shape2)\n  phi <- shape1 + shape2\n  return(list(mu = mu, phi = phi))\n}\n\nmuphi_to_shapes <- function(mu, phi) {\n  shape1 <- mu * phi\n  shape2 <- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\nshapes_to_muphi(shape1 = 6, shape2 = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$mu\n[1] 0.6\n\n$phi\n[1] 10\n```\n\n\n:::\n\n```{.r .cell-code}\nshapes_to_muphi(shape1 = 4, shape2 = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$mu\n[1] 0.4\n\n$phi\n[1] 10\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nThis helpful because in Bayesian beta regression we are going to use brms to model each the $\\mu$ part and the $\\phi$ part. In the model we are just going to use the version of the data that Andrew does to make things easier to compare.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nvdem_2015 = vdem_clean |>\n    filter(year == 2015) |>\n    mutate(prop_fem = ifelse(prop_fem == 0, 0.001, prop_fem))\n```\n:::\n\n\n\n\n\n\n\nSo lets do a little investigation of our quota variable roughly 40% of countries don't have a quota and just under 60% of countries don't have a gender quota. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvdem_2015 |>\n    group_by(quota) |>\n    summarise(count = n()) |>\n    mutate(prop = count/ sum(count)) |>\n    ggplot(aes(x = quota, y = prop)) +\n    geom_col()\n```\n\n::: {.cell-output-display}\n![](Bayesian-Stats_files/figure-html/unnamed-chunk-77-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\nSo we may want to frame the prior around the no quota \n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n## Multilevel Models\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}