{
  "hash": "aeefa32572ae4225424295bca542cf73",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayesian Statistics \n\nBayesianism has been on the to do list for awhile since well they make the prettiest plots. More importantly we get more coherent measures of uncertainty and are closer to how we actually think of the world. There are lots of broad applications for Bayesianism in the private industry. The betting market for sports and how we think about teams is effectively Bayesian. We incorporate a lot of information about the observable parts of a team like schedule, roster talent, coaching, and guesses about the probablity that the roster will stay healthy. These are all priors we have about things that relate to the success of the team going into the season. As the season progresses and we get data our prior will update. In the case of the Saints this year our prior updated to make them a good team after one game moving our guess about the number of games they would win this year to probably more than their projected win total. With the second win this may have not moved our prior much at all. However, as attrition set in then our prior shifted a little bit back toward the pre-season total so on and so forth.\n\nBayesianism a formalization of this process by encoding our beliefs in probability distributions. Our prior is really just our belief about the distribution and plausible range of values for that variable before ever seeing the data. So if we were to set a prior for a coin we would set the prior as somthing kind of loosy goosy as this \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_heads = rbinom(n = 2, size = 1000, prob = 0.5)\n\nprior_heads[1]/sum(c(prior_heads[1], prior_heads[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5167323\n```\n\n\n:::\n:::\n\n\n\n\n\n\nSo in this case we are just stating that the probability of heads is about 50 percent over a thousand trials assuming a fair coin. However, lets say that we know the coin is biased in a known way that makes it come up heads 61% of the time. We could then set our prior that it willl come up heads as this \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_heads_biased = rbinom(n = 2, size = 1000, prob = .61)\n```\n:::\n\n\n\n\n\n\nThis is not neccessarily all that unique from a standard null hypothesis which we covered in the opening \"chapter\" of this book. It is the combination of the prior and the posterior which makes the Bayesian machine go brrr. \n\nThe posterior can we operationalized along these lines \n\n$$\nPosterior = \\frac{\\text{Probability of Data} \\times Prior}{\\text{Mean probability of Data}}\n$$\n\nWhere we are averaging the probability of the prior. If we derive this out more formally we will see that the posterior is actually proportional to the prodct of the prior and the probability of the data. We can overcome a bad prior given an infinite amount of data, but this process will be inefficient and critically we will never reach infinite data. Priors in Bayesian inference are important computationally as well as substantively. \n\n### How to set priors\n\nMost of the time people say just use your substantive knowledge. But that's generally not helpful if you haven't done that in a statistical setting. Lets take it step by step. First we generally outline what we link the distribution of our dependen variable would be. For our outcome variable we may think that its a normal distribution so we would write it like this\n\n$$\nOutcome \\sim Normal(\\mu, \\sigma)\n$$\n\n\nIf it is a binary outcome we would write it like this \n\n\n$$\nOutcome \\sim Bernouli(\\text{Number of Trials},\\text{Probability true})\n$$\n\nThe next step is is we have to think about our generative model. So to ground our analysis it helps to start from a DAG. We generally have beliefs about an intervention in the world. It helps to start from a causal model because we generally have beliefs about what parts of the model we care about. That way we can start thinking about the potential relationships in our data. We then have to think of the plausible ranges for these values. From there once we get our model of the world we should start thinking about our prior we may start with a conservative prior that is written out like this \n\n\n$$\n\\begin{align}\n\\textbf{Outcome} \\sim N(\\mu, \\sigma) \\\\[8pt]\n\\textbf{Predictor One} \\sim N (3, 1)\n\\end{align}\n$$\n\nWhere we are shifting the mean a bit and putting a somewhat conservative prior. This is generally fine but there are lots of priors we can set. What we would want to do is simulate out our entire set of models and priors to see if this is a realistic version of the world that we have constructed.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}