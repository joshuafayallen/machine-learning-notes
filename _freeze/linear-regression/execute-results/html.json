{
  "hash": "5c9471d482f2bf7ba4d4b94775f181fb",
  "result": {
    "engine": "knitr",
    "markdown": "# Linear Regression and Shrinkage Estimators\n\n\nSince the bulk of your work will be building machnine learning models it is probably going to be important that you get way more comfortable with machine learning in python. You are a bit of a unicorn in the sense that you will do anything but learn pandas. So you are going to have to make sure that this isn't new information. \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\"}\nimport polars as pl \nimport numpy as np\nimport pandas as pd \nimport statsmodels.formula.api as smf \nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV, train_test_split, ShuffleSplit, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import mean_squared_error, make_scorer\nimport polars.selectors as cs\nfrom marginaleffects import *\nfrom plotnine import *\nfrom great_tables import GT \n\n\nboston = pl.read_csv('data/Boston.csv').to_pandas()\n```\n:::\n\n\n\n\n\nSince R is kind of your native language the way Python does things is weird to you so a simple linear model like this \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston = read.csv('data/Boston.csv')\n\nmod = lm(medv ~ lstat, data = boston)\n```\n:::\n\n\n\n\n\nbecomes this (monster) in python where you now also have to tell it that you need the constant. Which is frankly crazy. \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nform_model = smf.ols('medv ~ lstat', data = boston).fit()\n\nx = boston['lstat']\n\nx = sm.add_constant(x)\n\nsm_model = sm.OLS(boston['medv'], x).fit()\n\nform_model.summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.544</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.543</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   601.6</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 26 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>5.08e-88</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>16:23:15</td>     <th>  Log-Likelihood:    </th> <td> -1641.5</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3287.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   504</td>      <th>  BIC:               </th> <td>   3295.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   34.5538</td> <td>    0.563</td> <td>   61.415</td> <td> 0.000</td> <td>   33.448</td> <td>   35.659</td>\n</tr>\n<tr>\n  <th>lstat</th>     <td>   -0.9500</td> <td>    0.039</td> <td>  -24.528</td> <td> 0.000</td> <td>   -1.026</td> <td>   -0.874</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>137.043</td> <th>  Durbin-Watson:     </th> <td>   0.892</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 291.373</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.453</td>  <th>  Prob(JB):          </th> <td>5.36e-64</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.319</td>  <th>  Cond. No.          </th> <td>    29.7</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n:::\n\n```{.python .cell-code}\n\n# sm_model.summary()\n\n```\n:::\n\n\n\n\n\nLike an intersting cultural difference between these two is how we do things after estimation. For R apply functions to it since R is a more functionally oriented language. However if we access the object we created we have a whole host of class methods for this task. So if we wanted to predict what would happen at specified values we would do something to the effect of \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})\n\nx_new = sm.add_constant(new_df)\n\npreds = sm_model.get_prediction(x_new)\n\n## this prints a huge array\npreds_mean = preds.predicted_mean\n\n```\n:::\n\n\n\n\nMultiple regression works somewhat similarly. Unfortunately it takes this hideous form\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny = boston['medv']\n\nx = boston[['crim', 'age']]\n\nx = sm.add_constant(x)\n\nsm.OLS(y, x).fit().summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.217</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.213</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   69.52</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 26 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>2.20e-27</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>16:23:16</td>     <th>  Log-Likelihood:    </th> <td> -1778.5</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3563.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   503</td>      <th>  BIC:               </th> <td>   3576.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>   29.8007</td> <td>    0.971</td> <td>   30.698</td> <td> 0.000</td> <td>   27.893</td> <td>   31.708</td>\n</tr>\n<tr>\n  <th>crim</th>  <td>   -0.3118</td> <td>    0.045</td> <td>   -6.914</td> <td> 0.000</td> <td>   -0.400</td> <td>   -0.223</td>\n</tr>\n<tr>\n  <th>age</th>   <td>   -0.0896</td> <td>    0.014</td> <td>   -6.499</td> <td> 0.000</td> <td>   -0.117</td> <td>   -0.062</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>189.020</td> <th>  Durbin-Watson:     </th> <td>   0.710</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 553.472</td> \n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.831</td>  <th>  Prob(JB):          </th> <td>6.53e-121</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 6.583</td>  <th>  Cond. No.          </th> <td>    199.</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n:::\n\n```{.python .cell-code}\n\n# smf.ols('medv ~ crim + age', data = boston).fit().summary()\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(medv ~ crim + age, data = boston))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ crim + age, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.940  -4.991  -2.420   2.110  32.033 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 29.80067    0.97078  30.698  < 2e-16 ***\ncrim        -0.31182    0.04510  -6.914 1.43e-11 ***\nage         -0.08955    0.01378  -6.499 1.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.157 on 503 degrees of freedom\nMultiple R-squared:  0.2166,\tAdjusted R-squared:  0.2134 \nF-statistic: 69.52 on 2 and 503 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\nWhat starts to get interesting is that what if we need we want to fit everything in one go? In R this is pretty simple \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(medv ~ ., data = boston))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ ., data = boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4167  -2.8190  -0.5834   2.0250  26.1489 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.642977   4.934432   8.439 3.59e-16 ***\nX            -0.002426   0.002103  -1.154 0.249130    \ncrim         -0.122172   0.032996  -3.703 0.000238 ***\nzn            0.048513   0.013939   3.480 0.000545 ***\nindus         0.012833   0.062126   0.207 0.836433    \nchas          2.858484   0.869863   3.286 0.001088 ** \nnox         -18.546508   3.854423  -4.812 1.99e-06 ***\nrm            3.685614   0.420780   8.759  < 2e-16 ***\nage           0.001098   0.013502   0.081 0.935242    \ndis          -1.507860   0.202099  -7.461 3.92e-13 ***\nrad           0.307457   0.068691   4.476 9.46e-06 ***\ntax          -0.011976   0.003849  -3.112 0.001969 ** \nptratio      -0.932889   0.132223  -7.055 5.88e-12 ***\nlstat        -0.553515   0.050658 -10.926  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.796 on 492 degrees of freedom\nMultiple R-squared:  0.735,\tAdjusted R-squared:  0.728 \nF-statistic:   105 on 13 and 492 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nWhereas in python you need to do something like this \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsmf.ols('medv ~' + '+'.join(boston.columns.difference(['medv'])), data = boston).fit().summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.734</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.728</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   113.5</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 26 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>2.23e-133</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>16:23:16</td>     <th>  Log-Likelihood:    </th> <td> -1504.9</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3036.</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   493</td>      <th>  BIC:               </th> <td>   3091.</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   41.6173</td> <td>    4.936</td> <td>    8.431</td> <td> 0.000</td> <td>   31.919</td> <td>   51.316</td>\n</tr>\n<tr>\n  <th>age</th>       <td>    0.0036</td> <td>    0.013</td> <td>    0.271</td> <td> 0.787</td> <td>   -0.023</td> <td>    0.030</td>\n</tr>\n<tr>\n  <th>chas</th>      <td>    2.8400</td> <td>    0.870</td> <td>    3.264</td> <td> 0.001</td> <td>    1.131</td> <td>    4.549</td>\n</tr>\n<tr>\n  <th>crim</th>      <td>   -0.1214</td> <td>    0.033</td> <td>   -3.678</td> <td> 0.000</td> <td>   -0.186</td> <td>   -0.057</td>\n</tr>\n<tr>\n  <th>dis</th>       <td>   -1.4908</td> <td>    0.202</td> <td>   -7.394</td> <td> 0.000</td> <td>   -1.887</td> <td>   -1.095</td>\n</tr>\n<tr>\n  <th>indus</th>     <td>    0.0135</td> <td>    0.062</td> <td>    0.217</td> <td> 0.829</td> <td>   -0.109</td> <td>    0.136</td>\n</tr>\n<tr>\n  <th>lstat</th>     <td>   -0.5520</td> <td>    0.051</td> <td>  -10.897</td> <td> 0.000</td> <td>   -0.652</td> <td>   -0.452</td>\n</tr>\n<tr>\n  <th>nox</th>       <td>  -18.7580</td> <td>    3.851</td> <td>   -4.870</td> <td> 0.000</td> <td>  -26.325</td> <td>  -11.191</td>\n</tr>\n<tr>\n  <th>ptratio</th>   <td>   -0.9375</td> <td>    0.132</td> <td>   -7.091</td> <td> 0.000</td> <td>   -1.197</td> <td>   -0.678</td>\n</tr>\n<tr>\n  <th>rad</th>       <td>    0.2894</td> <td>    0.067</td> <td>    4.325</td> <td> 0.000</td> <td>    0.158</td> <td>    0.421</td>\n</tr>\n<tr>\n  <th>rm</th>        <td>    3.6581</td> <td>    0.420</td> <td>    8.705</td> <td> 0.000</td> <td>    2.832</td> <td>    4.484</td>\n</tr>\n<tr>\n  <th>tax</th>       <td>   -0.0127</td> <td>    0.004</td> <td>   -3.337</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n</tr>\n<tr>\n  <th>zn</th>        <td>    0.0470</td> <td>    0.014</td> <td>    3.384</td> <td> 0.001</td> <td>    0.020</td> <td>    0.074</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>171.096</td> <th>  Durbin-Watson:     </th> <td>   1.077</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 709.937</td> \n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.477</td>  <th>  Prob(JB):          </th> <td>6.90e-155</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 7.995</td>  <th>  Cond. No.          </th> <td>1.17e+04</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.17e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n\n:::\n:::\n\n\n\n\nor \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = boston.drop(columns=['medv'])\n\nx = sm.add_constant(x)\n\ny = boston['medv']\n\n\nsm.OLS(y,x).fit().summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.735</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.728</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   105.0</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 26 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>1.26e-132</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>16:23:16</td>     <th>  Log-Likelihood:    </th> <td> -1504.2</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3036.</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   492</td>      <th>  BIC:               </th> <td>   3096.</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>   41.6430</td> <td>    4.934</td> <td>    8.439</td> <td> 0.000</td> <td>   31.948</td> <td>   51.338</td>\n</tr>\n<tr>\n  <th></th>        <td>   -0.0024</td> <td>    0.002</td> <td>   -1.154</td> <td> 0.249</td> <td>   -0.007</td> <td>    0.002</td>\n</tr>\n<tr>\n  <th>crim</th>    <td>   -0.1222</td> <td>    0.033</td> <td>   -3.703</td> <td> 0.000</td> <td>   -0.187</td> <td>   -0.057</td>\n</tr>\n<tr>\n  <th>zn</th>      <td>    0.0485</td> <td>    0.014</td> <td>    3.480</td> <td> 0.001</td> <td>    0.021</td> <td>    0.076</td>\n</tr>\n<tr>\n  <th>indus</th>   <td>    0.0128</td> <td>    0.062</td> <td>    0.207</td> <td> 0.836</td> <td>   -0.109</td> <td>    0.135</td>\n</tr>\n<tr>\n  <th>chas</th>    <td>    2.8585</td> <td>    0.870</td> <td>    3.286</td> <td> 0.001</td> <td>    1.149</td> <td>    4.568</td>\n</tr>\n<tr>\n  <th>nox</th>     <td>  -18.5465</td> <td>    3.854</td> <td>   -4.812</td> <td> 0.000</td> <td>  -26.120</td> <td>  -10.973</td>\n</tr>\n<tr>\n  <th>rm</th>      <td>    3.6856</td> <td>    0.421</td> <td>    8.759</td> <td> 0.000</td> <td>    2.859</td> <td>    4.512</td>\n</tr>\n<tr>\n  <th>age</th>     <td>    0.0011</td> <td>    0.014</td> <td>    0.081</td> <td> 0.935</td> <td>   -0.025</td> <td>    0.028</td>\n</tr>\n<tr>\n  <th>dis</th>     <td>   -1.5079</td> <td>    0.202</td> <td>   -7.461</td> <td> 0.000</td> <td>   -1.905</td> <td>   -1.111</td>\n</tr>\n<tr>\n  <th>rad</th>     <td>    0.3075</td> <td>    0.069</td> <td>    4.476</td> <td> 0.000</td> <td>    0.172</td> <td>    0.442</td>\n</tr>\n<tr>\n  <th>tax</th>     <td>   -0.0120</td> <td>    0.004</td> <td>   -3.112</td> <td> 0.002</td> <td>   -0.020</td> <td>   -0.004</td>\n</tr>\n<tr>\n  <th>ptratio</th> <td>   -0.9329</td> <td>    0.132</td> <td>   -7.055</td> <td> 0.000</td> <td>   -1.193</td> <td>   -0.673</td>\n</tr>\n<tr>\n  <th>lstat</th>   <td>   -0.5535</td> <td>    0.051</td> <td>  -10.926</td> <td> 0.000</td> <td>   -0.653</td> <td>   -0.454</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>168.602</td> <th>  Durbin-Watson:     </th> <td>   1.082</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 688.210</td> \n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.459</td>  <th>  Prob(JB):          </th> <td>3.61e-150</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 7.912</td>  <th>  Cond. No.          </th> <td>1.38e+04</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.38e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n\n:::\n:::\n\n\n\n\n## Transformations\n\nWe can start to do things like add transformations like this \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = boston.drop(columns = 'medv')\n\ny = boston['medv']\n\nx['sqr_lstat'] = x['lstat'] **2\n\ny['medv'] = np.log(y['medv'])\n\nx = sm.add_constant(x) \n\nsm.OLS(y,x).fit().summary()\n```\n:::\n\n\n\n\n\nFor interactions we do something like this \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = boston[['age', 'lstat']]\n\nx['lstat:age'] = x['age'] * x['lstat']\n\nx = sm.add_constant(x) \n\ny = boston['medv']\n\n\nsm.OLS(y, x).fit().summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.556</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   209.3</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 26 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>4.86e-88</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>16:23:16</td>     <th>  Log-Likelihood:    </th> <td> -1635.0</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3278.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   502</td>      <th>  BIC:               </th> <td>   3295.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>     <td>   36.0885</td> <td>    1.470</td> <td>   24.553</td> <td> 0.000</td> <td>   33.201</td> <td>   38.976</td>\n</tr>\n<tr>\n  <th>age</th>       <td>   -0.0007</td> <td>    0.020</td> <td>   -0.036</td> <td> 0.971</td> <td>   -0.040</td> <td>    0.038</td>\n</tr>\n<tr>\n  <th>lstat</th>     <td>   -1.3921</td> <td>    0.167</td> <td>   -8.313</td> <td> 0.000</td> <td>   -1.721</td> <td>   -1.063</td>\n</tr>\n<tr>\n  <th>lstat:age</th> <td>    0.0042</td> <td>    0.002</td> <td>    2.244</td> <td> 0.025</td> <td>    0.001</td> <td>    0.008</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>135.601</td> <th>  Durbin-Watson:     </th> <td>   0.965</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 296.955</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.417</td>  <th>  Prob(JB):          </th> <td>3.29e-65</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.461</td>  <th>  Cond. No.          </th> <td>6.88e+03</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.88e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n\n:::\n:::\n\n\n\n\n\nFor qualitative variables we need to switch to a new dataset. There is a lot of interesting information in multicategory variables. One thing that we have to keep in mind when using qualitative variables normally is that we have a reference category. This may not always be straightforward to infer and we are throwing information away that is interesting. One hot encoding or breaking out the qualitative variable to indicatior variables is a nice way to do this, \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncarseats = pl.read_csv('data/Carseats.csv')\n\ncarseats.select(pl.col('ShelveLoc').unique()).head()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (3, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ShelveLoc</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Bad&quot;</td></tr><tr><td>&quot;Medium&quot;</td></tr><tr><td>&quot;Good&quot;</td></tr></tbody></table></div>\n```\n\n:::\n\n```{.python .cell-code}\ncars_small = carseats.select(pl.col('Sales', 'CompPrice', 'Income', 'ShelveLoc')).to_dummies(cs.string())\n\nx = cars_small.select(pl.exclude('Sales')).to_pandas()\n\ny = cars_small.to_pandas()['Sales']\n\nx = sm.add_constant(x)\n\nsm.OLS(y, x).fit().summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.352</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.345</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   53.63</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 26 Feb 2025</td> <th>  Prob (F-statistic):</th> <td>4.38e-36</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>16:23:16</td>     <th>  Log-Likelihood:    </th> <td> -895.60</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   1801.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   395</td>      <th>  BIC:               </th> <td>   1821.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>            <td>    3.8101</td> <td>    0.755</td> <td>    5.046</td> <td> 0.000</td> <td>    2.326</td> <td>    5.295</td>\n</tr>\n<tr>\n  <th>CompPrice</th>        <td>    0.0106</td> <td>    0.007</td> <td>    1.420</td> <td> 0.156</td> <td>   -0.004</td> <td>    0.025</td>\n</tr>\n<tr>\n  <th>Income</th>           <td>    0.0184</td> <td>    0.004</td> <td>    4.473</td> <td> 0.000</td> <td>    0.010</td> <td>    0.026</td>\n</tr>\n<tr>\n  <th>ShelveLoc_Bad</th>    <td>   -0.9343</td> <td>    0.312</td> <td>   -2.994</td> <td> 0.003</td> <td>   -1.548</td> <td>   -0.321</td>\n</tr>\n<tr>\n  <th>ShelveLoc_Good</th>   <td>    3.8167</td> <td>    0.322</td> <td>   11.843</td> <td> 0.000</td> <td>    3.183</td> <td>    4.450</td>\n</tr>\n<tr>\n  <th>ShelveLoc_Medium</th> <td>    0.9277</td> <td>    0.287</td> <td>    3.235</td> <td> 0.001</td> <td>    0.364</td> <td>    1.492</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.001</td> <th>  Durbin-Watson:     </th> <td>   1.941</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 1.000</td> <th>  Jarque-Bera (JB):  </th> <td>   0.030</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.003</td> <th>  Prob(JB):          </th> <td>   0.985</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.958</td> <th>  Cond. No.          </th> <td>7.74e+17</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.38e-29. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular.\n```\n\n:::\n:::\n\n\n\n\nFor quick and dirty things this is nice and fairly straightforward. We are not really going to delve to deep on fitting a ton of models but really this would just involve some F-string. Instead we are going to focus on the machine learning workflow. Going through step by step and doing these are not terribily time consuming but as things get more complicated we are going to need a more robust framework to deal with this. \n\n\n## Regression Assumptions\n\nOur basic assumptions of linear regression are that \n\n1. There are linear relationship between our outcome and our predictors \n  - This is something we violate all the time. For the most part we can transform the dependent or independent variable to dependent variable\n2. No Perfect multicolinearity. This is effectively a mathematical constraint. If any of the predictors are an exact linear combination of each other then we can't actually calculate the model. Software solves this for us and kicks out various terms. It will do this arbitrarily so. \n  - In practice this assumption is never violated but we still need to worry about some collinearity. The general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can't systematically account for whether this will \n\nThe general idea is that when we are measuring a concept with variables that are really correlated with each other then we are not going to get a good understanding of what each variable is contributing on their own. This is not a statistical problem it is a research design problem. We can't systematically account for whether this will inflate the standard errors  or deflate them. Meaning that if we are interested in variable importance or making statements about what moving one variable up or down will do. \n\n3. Spepherical Error term aka IID error assumption \n\nFor simplicity we can group two related assumptions \n\nThe first part is that we expect homogeneity \n\n$$\n\\begin{equation}\n  var(\\mu_i) = E[\\mu_i] - E[\\mu_i | X_i)^2]\n\\end{equation}\n$$\n\nThe second part is that there is no autocorrelation. \n\n$$\n\\begin{equation}\n  cov(\\mu_i, \\mu_j | X_i, X_j) = 0\n\\end{equation}\n$$\n\n\nIn practice when we violate this asumption our coefficients are unaffected but our standard errors can be drastically wrong. We can correct the standard errors to account for violations by loosening up how we compute the variance-covariance matrix. In some ways this is acceptable under very specific settings however. This can generally point to deeper issues with modeling the data. Like one way to trigger heterosckedasicity is simply to model binary data with OLS. A more appropriate solution would just to be model the DGP correctly.\n\n\n## Shrinkage Estimators \n\nOne solution that this flavor of statistics has proposed to reduce the variance of OLS estimates when the number of predictors large or the predictors are really collinear than we can use shirkage aka regularization to penalize regression coefficients towards zero. \n\nRidge regression and LASSO regression are not entirely different than OLS regression. Each of these estimators reduces the residual sum of squares however we tack on a penalty term to the lasso and ridge estimators. \n\n\n\n\n```{=tex}\n\\begin{align}\n\n\\text{OLS} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 \\\\\n\n\\text{Ridge} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum \\beta^{2}_{j} \\\\\n\n\\text{LASSO} = \\sum_{x,y \\in D}(y-\\text{prediction}(x))^2 + \\lambda \\sum |{\\beta_{j}}|\n\n\\end{align}\n\n\n\n```\n\n\n\n\nWhat is going on underneath the hood? \n\n- Ridge penalty we are summing the squared coefficients and multiplying it by the hyperparameter. \n  - Effectively imposing a penalty equivelent to the square of the magnitude of the coefficients\n- LASSO penalty we are just summing the absolute values of the coefficients. \n    - Effectively imposing a penalty equivelent to the absolute value of the magnitude of coefficients \n\nThe reason why we want to penalize a model is that as we start training and start adding in variables we think help predict our outcome our models are going to start to do a better job fitting the noise. Each approach has pros and cons. A ridge penalty will let all predictors enter into the predictions even if they don't contribute much. A lasso penalty will implictly start to kick out variables that don't contribute anything to the model. Theoretically this is important if we have a high amount of multicollinearity/we don't wanted automated feature selection. Effectively this question is a little theoretical in practice we are going to compare the predicitve accuracy of the two to find this out. The ridge and lasso penalties will appear in other cases so I will probably go over this again. \n\n## What is multicollinearity and how do we fix it? \n\nTo set up this question lets think of a hypothetical business problem. We are trying to assign somebody a premium based on the data we have on other people. We have access to their age, their location, past driving history, make and model of their car, and gender. Theoretically we would expect these variables to be related to one another in some way. For example make and model of their car, age, and gender is probably going to tell us something about their past driving history. We would expect that a 20 year old male driving a sports car is probably going to have a higher likelihood of having one or more speeding tickets then a female driving the same car. When we go to model this relationship each of these variables are going to be related to each other. \n\nLets setup an OLS with this example where age enters into the model in years, car type is a factor with a Toyota Corolla as it reference level, past incidents is the log of past driving behavior, location is a simple indicator variable where rural is the reference level, and gender is a indicator variable where the reference level is female.\n\n$$\nPremium = \\alpha + \\beta_{1} Age + \\beta_{2} \\text{car_type} + \\beta_{3} location + \\beta_{4} \\log{history} + \\beta_{5} Gender + \\varepsilon\n$$\n\nFor simplicity we will focus on gender as the primary explainer for insurance premiums. Lets say that gender and location are highly correlated this may occur for a variety of reasons e.g. presence of a military base, presence of a college campus etc. When we go to interpret the effect of being male on an insurance premium we hold all variables in the equation constant. However, if the moving from a rural area to the city is highly deterministic of gender we can neatly unpack the effect of gender on insurance premium. While an interpretational problem this makes it difficult to unpack the effect of a single variable on insurance premiums. \n\n\nAnother important factor is multicollinearity impacts our uncertainty estimates either biasing them down leading us to fail to reject the null more times than we should or can bias our standard errors downwards over rejecting the null hypothesis [@gujarati2012basic; @lenzAchievingStatisticalSignificance2021]. Lets take a simulated example to make this point a bit more explicit \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(performance)\nlibrary(magrittr)\nlibrary(tidyverse)\n\n\n\n\ngenerate_dummy_data <- function(data, seed = NULL) {\n  set.seed(seed = seed)\n  data <- dplyr::tibble(gender = rbinom(100, 1, 0.5),\n                        location = ifelse(runif(100) < 0.9, gender, 1 - gender),\n                        y = 5 + 3 * gender + rnorm(100, 0, 3))\n  return(data)\n}\n\ngenerate_dummy_model <- function(data) {\n  model <- lm(y ~ gender + location, data = data)\n  return(model)\n}\n\nextract_model_coefficients <- function(model) {\n  coefs <- broom::tidy(model) |> janitor::clean_names()\n  # replace dots and white spaces with underscores\n  return(coefs)\n}\n\nrun_lm_simulation <- function(data_generator, model_generator, n = 1000) {\n  simulation <- \n    dplyr::tibble(i = 1:n) %>% \n    dplyr::mutate(\n      data = purrr::map(.x = i, .f = ~ data_generator(seed = .x)),\n      model = purrr::map(.x = data, .f = model_generator),\n      coefs = purrr::map(.x = model, .f = extract_model_coefficients)\n      ) %>%\n  return(simulation)}\n\ndummy_simulation <- run_lm_simulation(data_generator = generate_dummy_data,\n                                      model_generator = generate_dummy_model, \n                                      n = 1000)\n\n\n\nsimulated_p = dummy_simulation |>\n    unnest(coefs) |>\n    filter(term != '(Intercept)')\n\n\n\nggplot(simulated_p, aes(x = p_value, fill = term)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_wrap(vars(term), scales = \"free_y\", ncol = 1) +\n  labs(title = \"Distribution of P-Values for x and x_collinear\",\n       x = \"P-Value\",\n       y = \"Frequency\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/sim-1.png){width=672}\n:::\n:::\n\n\n\n\nIn this case we see that we have some encouraging results that we are not going to over reject gender but if we look at location we are much more prone to failing to reject the null hypothesis. This is fine in a simulated world because we induced this relationship ourselves. However, in the real world that is not available to us. \n\nTo get around multicollinearity we use methods that penalize our coeficients towards zero. The two most common forms of penalization are known as ridge and LASSO regression. The canonical regression equation minimizes the sum of squared residuals. Regularization adds a penalty term to this equation that shrinks the coefficient estimates toward zero. Ridge regression (L2 penalty) adds the sum of the squared coefficients to the loss function, which discourages large coefficient values but does not set them to zero. LASSO regression (L1 penalty) adds the sum of the absolute values of the coefficients, which can shrink some coefficients to exactly zero, effectively performing feature selection.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\nhitters = read_csv('data/Hitters.csv') |>\n    janitor::clean_names() |>\n    filter(!is.na(salary))\n\n\nridge_spec = linear_reg(mixture = 0, penalty = 0) |>\n    set_mode('regression') |>\n    set_engine('glmnet') |>\n    fit(salary ~ ., data = hitters)\n\nridge_spec |>\n    autoplot() + \n    theme_minimal() +\n    labs(title = 'Ridge Regularization')\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/test-1.png){width=672}\n:::\n:::\n\n\n\n\nAs we can see with the ridge penalty as we increase the size of the penalty the closer to zero these coefficients get. For some coefficients they get pushed towards zero immediately. One of the things that happens with multicollinear data is that jumps in one variable can cause massive swings in another in ways that aren't entirely transparent. In prediction context we may not neccessarily care about the individual impact of one of 100 variables. We are more concerned that our model is healthy and making good predictions on data that it hasn't seen. By regularizing our model we penalize the model for being extremely flexible this may increase our bias a bit but this comes at the gain of reducing the variance. What this means is that we are going to get a healthier prediction machine. For a LASSO model some of these coefficients will be zero just by the nature of the penalty. Effectively performing feature selection for us.\n\n## Training these things \n\n\nOne important thing to note is that neither when we change the scale of our predictor variable the LASSO and Ridge esstimates will not adjust accordingly because the larger coefficients that result from say salary and age are going to be on different scales which our coefficients are going to respond to accrodingly. However, the penalty term is not going react well at all since it will penalize larger coefficients by default. To build on our prior knowledge lets build a scikit learn pipeline that validates \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nK = 5\nkfold = KFold(K,\n                  random_state=0,\n                  shuffle=True)\n\nhitters = pl.read_csv('data/Hitters.csv').with_columns(\n     pl.col('Salary').str.to_integer(strict = False).alias('Salary')\n).drop_nulls()\n\nscaler = StandardScaler()\n\nK = 5\n\nx = hitters.select(pl.exclude('Salary')).to_dummies(cs.string())\n\ny = hitters.select(pl.col('Salary')).to_numpy()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2 )\n\nlambdas = 10**np.linspace(8, -2, 100) / y.std()\n\nridge = ElasticNet(l1_ratio = 0)\n\nparam_grid = {'ridge__alpha': lambdas}\n\npipe = Pipeline(steps = [('scaler', scaler), ('ridge', ridge)])\n\npipe.fit(x_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;ridge&#x27;, ElasticNet(l1_ratio=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;ridge&#x27;, ElasticNet(l1_ratio=0))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ElasticNet</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.ElasticNet.html\">?<span>Documentation for ElasticNet</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ElasticNet(l1_ratio=0)</pre></div> </div></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\nvalidation = ShuffleSplit(n_splits=1,\n                              test_size=0.5,\n                              random_state=0)\n\ngrid = GridSearchCV(pipe, param_grid, cv = validation, scoring='neg_mean_squared_error')\n\ngrid.fit(x, y)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-2 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;ridge&#x27;, ElasticNet(l1_ratio=0))]),\n             param_grid={&#x27;ridge__alpha&#x27;: array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=0, test_size=0.5, train_size=None),\n             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                                       (&#x27;ridge&#x27;, ElasticNet(l1_ratio=0))]),\n             param_grid={&#x27;ridge__alpha&#x27;: array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.9252...\n       4.99443889e-03, 3.95800741e-03, 3.13665318e-03, 2.48574400e-03,\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05])},\n             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;ridge&#x27;,\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ElasticNet</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.ElasticNet.html\">?<span>Documentation for ElasticNet</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ngrid.best_params_['ridge__alpha']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnp.float64(0.0009804286565854396)\n```\n\n\n:::\n\n```{.python .cell-code}\ngrid.best_estimator_\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-3 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-3 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-3 pre {\n  padding: 0;\n}\n\n#sk-container-id-3 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-3 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-3 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-3 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-3 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-3 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-3 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-3 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-3 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-3 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-3 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-3 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-3 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-3 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-3 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-3 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-3 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n#sk-container-id-3 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-3 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-3 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-3 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-3 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-3 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-3 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-3 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-3 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-3 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;ridge&#x27;,\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;ridge&#x27;,\n                 ElasticNet(alpha=np.float64(0.0009804286565854396),\n                            l1_ratio=0))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ElasticNet</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.ElasticNet.html\">?<span>Documentation for ElasticNet</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ElasticNet(alpha=np.float64(0.0009804286565854396), l1_ratio=0)</pre></div> </div></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\nridge_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            -grid.cv_results_['mean_test_score'],\n            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\nax.set_ylim([50000,250000])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(50000.0, 250000.0)\n```\n\n\n:::\n\n```{.python .cell-code}\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n\n\nSo we have it looking at the MSE but we can also look at it with R2\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nridge_cv = ElasticNetCV(alphas = lambdas,\n                        l1_ratio = 0,\n                        cv = kfold)\n\n\npipe_cv = Pipeline(steps =[('scaler', scaler), ('ridge', ridge_cv)])       \n\npipe_cv.fit(x, y)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-4 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-4 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-4 pre {\n  padding: 0;\n}\n\n#sk-container-id-4 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-4 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-4 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-4 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-4 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-4 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-4 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-4 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-4 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-4 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-4 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n#sk-container-id-4 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-4 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-4 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-4 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-4 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-4 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-4 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;ridge&#x27;,\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;ridge&#x27;,\n                 ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03,...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ElasticNetCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.ElasticNetCV.html\">?<span>Documentation for ElasticNetCV</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ElasticNetCV(alphas=array([2.37276310e+05, 1.88037418e+05, 1.49016438e+05, 1.18092979e+05,\n       9.35866661e+04, 7.41658324e+04, 5.87751538e+04, 4.65783042e+04,\n       3.69125096e+04, 2.92525326e+04, 2.31821318e+04, 1.83714430e+04,\n       1.45590544e+04, 1.15378016e+04, 9.14351047e+03, 7.24607567e+03,\n       5.74239105e+03, 4.55074670e+03, 3.60638894e+03, 2.85800156e+03,\n       2.26491736e+03, 1.794908...\n       1.96990961e-03, 1.56111968e-03, 1.23716065e-03, 9.80428657e-04,\n       7.76972943e-04, 6.15737770e-04, 4.87961653e-04, 3.86701265e-04,\n       3.06454139e-04, 2.42859664e-04, 1.92462131e-04, 1.52522947e-04,\n       1.20871827e-04, 9.57888560e-05, 7.59110302e-05, 6.01581933e-05,\n       4.76743393e-05, 3.77810986e-05, 2.99408745e-05, 2.37276310e-05]),\n             cv=KFold(n_splits=5, random_state=0, shuffle=True), l1_ratio=0)</pre></div> </div></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ntuned_ridge = pipe_cv.named_steps['ridge']\nridgeCV_fig, ax = plt.subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            tuned_ridge.mse_path_.mean(1),\n            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_ylim([50000,250000])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(50000.0, 250000.0)\n```\n\n\n:::\n\n```{.python .cell-code}\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20)                   \n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-16-3.png){width=768}\n:::\n:::\n\n\n\n\n\nWe could do the same thing with a lasso regression but this workflow is not great instead we can make a function and then loop over the grids. \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nmodels = {\n    'ridge': ElasticNet(l1_ratio=0),\n    'lasso': ElasticNet(l1_ratio=1)  # Lasso is a special case of ElasticNet\n}\n\n# Prepare for GridSearchCV\n\nparam_grids = {\n    'ridge': {'elasticnet__alpha': lambdas},  # Use 'elasticnet' as the step name\n    'lasso': {'elasticnet__alpha': lambdas}\n}\n\nresults = []\n# Set up ShuffleSplit cross-validation for GridSearchCV\nvalidation = ShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n\n# Function to perform grid search and output results\ndef tune_model(model_name, model, param_grid):\n    # Build pipeline\n    pipe = Pipeline(steps=[('scaler', scaler), ('elasticnet', model)])  # Step name matches model_name\n    \n    # Perform grid search\n    grid = GridSearchCV(pipe, param_grid, cv=validation, scoring='neg_mean_squared_error')\n    grid.fit(x_train, y_train)\n    \n    # Extract best parameters and model\n    best_alpha = grid.best_params_['elasticnet__alpha']\n    best_model = grid.best_estimator_\n    results.append({\n        'model': model_name.capitalize(),  # Store as \"Ridge\" or \"Lasso\"\n        'best_alpha': best_alpha,\n        'best_model': str(best_model),\n        'best_score': grid.best_score_\n    })\n    return best_model\n    \nbest_models = {}\n# Tune each model\nfor model_name, model in models.items():\n    best_models[model_name] = tune_model(model_name, model, param_grids[model_name])\n\n\nresults_df = pl.DataFrame(results)\n```\n:::\n\n\n\n\nSo now we have the best model but we would like to grab the most important features. We may need something to present to stakeholders or to better understand what is going on in our models. \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_variable_importance(best_model, feature_names):\n    # Access the 'elasticnet' step in the pipeline\n    elastic_net_step = best_model.named_steps['elasticnet']\n    coef = elastic_net_step.coef_\n    feature_importance = sorted(zip(feature_names, coef), key=lambda x: abs(x[1]), reverse=True)\n    features, coefficients = zip(*feature_importance)\n    return features, coefficients\n# Prepare side-by-side plots\nfig, axes = plt.subplots(1, len(best_models), figsize=(15, 6), sharey=True)\n\n# Get feature names\nfeature_names = x.columns\n\n# Plot VIP for each model\nfor i, (model_name, best_model) in enumerate(best_models.items()):\n    # Extract variable importance\n    features, coefficients = get_variable_importance(best_model, feature_names)\n    \n    # Determine bar colors based on coefficient sign\n    colors = ['green' if coef > 0 else 'red' for coef in coefficients]\n    \n    # Create subplot\n    axes[i].barh(features, np.abs(coefficients), color=colors)\n    axes[i].set_title(f'Variable Importance: {model_name.capitalize()}')\n    axes[i].set_xlabel('Absolute Coefficient Value')\n    if i == 0:  # Add y-axis label only for the first plot\n        axes[i].set_ylabel('Features')\n    axes[i].invert_yaxis()  # Invert y-axis for descending order\n    \n    # Add a legend\n    axes[i].legend(['Positive Impact', 'Negative Impact'], loc='lower right')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-18-5.png){width=1440}\n:::\n:::\n\n\n\n\n\nWhich is nice we see what is postively impacting value. However, I don't neccessarily like these plots since we are not really showing anything interesting. A better way would be to show marginal effects. In R we would do something to this effect.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidymodels)\nlibrary(marginaleffects)\nlibrary(ISLR)\n\nHitters <- as_tibble(Hitters) |>\n  filter(!is.na(Salary))\n\nHitters_split <- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train <- training(Hitters_split)\nHitters_test <- testing(Hitters_split)\n\nHitters_fold <- vfold_cv(Hitters_train, v = 10)\n\n\nridge_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) |> \n  step_novel(all_nominal_predictors()) |> \n  step_dummy(all_nominal_predictors()) |> \n  step_zv(all_predictors()) |> \n  step_normalize(all_predictors())\n\nridge_spec <- \n  linear_reg(penalty = tune(), mixture = 0) |> \n  set_mode(\"regression\") |> \n  set_engine(\"glmnet\")\n\nridge_workflow <- workflow() |> \n  add_recipe(ridge_recipe) |> \n  add_model(ridge_spec)\n\npenalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\ntune_res <- tune_grid(\n  ridge_workflow,\n  resamples = Hitters_fold, \n  grid = penalty_grid\n)\n\nbest_penalty = select_best(tune_res, metric = 'rsq')\n\nridge_final <- finalize_workflow(ridge_workflow, best_penalty)\n\nridge_final_fit <- fit(ridge_final, data = Hitters_train)\n\navg_predictions(ridge_final_fit, newdata = Hitters_test, by = 'somefactor', newdata = some_grid_of_values)\n```\n:::\n\n\n\n\nIn machine learning parlance we could make a partial dependency plot. Which is really just plotting the predictions when we move values of our variables that we may think are interesting.\n\n\n## Evaluation Metrics \n\nWe have more than a few evaluation metrics for linear regression or regression based tasks to be specific so it is worth going over the most popular ones.  In at @tbl-ols-mets I outline the basics of each measure but will discuss each of these measures in greater detail. \n\n\n\n\n\n::: {#tbl-ols-mets .cell}\n\n```{.python .cell-code  code-fold=\"true\"}\nols_mets = pl.DataFrame({\n    'Metric': [\n        'R-Squared',\n        'Mean Square Error (MSE)',\n        'Root Mean Square Error (RMSE)',\n        'Mean Absolute Error (MAE)',\n        'Mean Absolute Percentage Error (MAPE)',\n        'Symmetric Mean Absolute Percentage Error (SMAPE)'\n    ],\n    'What it Does': [\n        'The proportion of variation explained by the model',\n        'The average error of the model',\n        'The square root of the average error of the model',\n        'Similar to the MSE but we calculate with |y - yhat|',\n        'Similar to the MSE but calculated by |y - yhat| / yi',\n        'Similar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)'\n    ]\n})\n\nGT(ols_mets)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"vujbocfrtg\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#vujbocfrtg table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#vujbocfrtg thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#vujbocfrtg p { margin: 0; padding: 0; }\n #vujbocfrtg .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #vujbocfrtg .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #vujbocfrtg .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #vujbocfrtg .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #vujbocfrtg .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #vujbocfrtg .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #vujbocfrtg .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #vujbocfrtg .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #vujbocfrtg .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #vujbocfrtg .gt_column_spanner_outer:first-child { padding-left: 0; }\n #vujbocfrtg .gt_column_spanner_outer:last-child { padding-right: 0; }\n #vujbocfrtg .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #vujbocfrtg .gt_spanner_row { border-bottom-style: hidden; }\n #vujbocfrtg .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #vujbocfrtg .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #vujbocfrtg .gt_from_md> :first-child { margin-top: 0; }\n #vujbocfrtg .gt_from_md> :last-child { margin-bottom: 0; }\n #vujbocfrtg .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #vujbocfrtg .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #vujbocfrtg .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #vujbocfrtg .gt_row_group_first td { border-top-width: 2px; }\n #vujbocfrtg .gt_row_group_first th { border-top-width: 2px; }\n #vujbocfrtg .gt_striped { background-color: rgba(128,128,128,0.05); }\n #vujbocfrtg .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #vujbocfrtg .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #vujbocfrtg .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #vujbocfrtg .gt_left { text-align: left; }\n #vujbocfrtg .gt_center { text-align: center; }\n #vujbocfrtg .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #vujbocfrtg .gt_font_normal { font-weight: normal; }\n #vujbocfrtg .gt_font_bold { font-weight: bold; }\n #vujbocfrtg .gt_font_italic { font-style: italic; }\n #vujbocfrtg .gt_super { font-size: 65%; }\n #vujbocfrtg .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #vujbocfrtg .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Metric\">Metric</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"What it Does\">What it Does</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_left\">R-Squared</td>\n    <td class=\"gt_row gt_left\">The proportion of variation explained by the model</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Mean Square Error (MSE)</td>\n    <td class=\"gt_row gt_left\">The average error of the model</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Root Mean Square Error (RMSE)</td>\n    <td class=\"gt_row gt_left\">The square root of the average error of the model</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Mean Absolute Error (MAE)</td>\n    <td class=\"gt_row gt_left\">Similar to the MSE but we calculate with |y - yhat|</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Mean Absolute Percentage Error (MAPE)</td>\n    <td class=\"gt_row gt_left\">Similar to the MSE but calculated by |y - yhat| / yi</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Symmetric Mean Absolute Percentage Error (SMAPE)</td>\n    <td class=\"gt_row gt_left\">Similar to the MSE but calculated by |y - yhat| / ((y + yhat) / 2)</td>\n  </tr>\n</tbody>\n\n\n</table>\n\n</div>\n        \n```\n\n:::\n:::\n\n\n\n\n\nBefore we dive into each of these metrics it is worth going over the OLS optimization problem. For an OLS the optimization problem is the sum of the squared residuals. The objective function is given by\n\n$$ \n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - \\hat{y}_{i})^{2}\n$$\n\nFor notational purposes $\\beta$ is a vector of regression coefficients, $y_{i}$ is the observed value of our dependent variable and $\\hat{y}$ is the predicted value from our model. It is a little bit clearer if we rewrite the optimization problem with \n\n$$\n\\arg \\min_{\\beta, \\alpha} \\sum^{n}_{i = 1} (y_{i} - (\\alpha + \\beta \\times x_i))^{2}\n$$\n\n\nOf the optimatization problems in the world this one is kind of beautifully simple. So linear regression is going to draw a line (or hyperplane) through our data that tries to minimize the distance between each of the points. We are going to see how far away the predicted value is from the observed value and then we are going to square them to ensure they aren't negative then we are going to sum them to get the total distance for each point. In effect what is going on is something to this \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nfitted = form_model.fittedvalues\n\nresids = form_model.resid\n\nboston['fitted'] = fitted\n\nm,b = np.polyfit(boston['lstat'], boston['medv'], 1)\n\nfig,ax = plt.subplots(figsize=(15,6))\nplt.plot(boston['lstat'], boston['lstat'] * m + b, color = 'pink')\nplt.scatter(boston['lstat'], fitted, color = 'red',alpha = 0.1)\nplt.scatter(boston['lstat'], boston['medv'], color = 'blue', alpha = 0.5)\nfor i in range(len(boston)):\n    plt.plot([boston['lstat'].iloc[i], boston['lstat'].iloc[i]], \n             [boston['medv'].iloc[i], boston['fitted'].iloc[i]], color='blue', alpha=0.5)\nplt.xlabel('Percent Low Status')\nplt.ylabel('Home Value Median ($1000)')\n\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-21-1.png){width=1440}\n:::\n:::\n\n\n\n\n\nEffectively we are minimizing the distance between the points and our line via this objective function. These metrics are all broadly related to this same idea. \n\n### R-Squared \n\n$R^2$ aka the 'coefficient of determination' is some what straight forward it is $1-\\frac{RSS}{TSS}$. Lets break this down a little further \n\n$$\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum^N_{i = 1} (y_{i} - \\hat{y}_i)^{2}}{\\sum^N_{i = 1} (y_i - \\bar{y})^{2}}\n$$\n\nSo if we look closely at the fraction is the numerator is the sum of the squared residuals or the distance metric we talked about earlier. In the denominator we have the something broadly similar. The total sum of squares is something akin to a more naive predicition error. Instead of $\\hat{y}$ we have $\\bar{y}$ which is just the mean of y. So we are effectively taking the summed distance of the observed value of y and the mean of y. What this gives us is the fraction of the variance explained by our model. In political science and the social sciences more generally this is something that most people don't even report anymore. In many cases we are creating regression tables where we have different dependent variables. One of the primary weaknesses of using $R^{2}$ is that the easiest way to improve your $R^2$ is to simply add more variables. As the model gets more information it is is going to a better job of getting $\\hat{y}$ close to the observed value of Y. \n\nAs a political scientist by training I am always skeptical of the idea of using $R^2$ for anything other than chuckling at people who see it as some sort of measure of distance from the true model. I think @King86 sums some of the criticisms quite well. Decisions about how we process our data are also going to influence this metric because we are going to either increase or decrease the distance. $R^2$ in a machine learning context may fit the description of a goodness of fit test where we are figuring out the amount of variation in the data explained. We are often swapping out variables on the right hand side. $R^2$ we can assess if some combination of these variables reduce the amount of unexplained variation. Generally we would probably want to use the penalized version of $R^2$ so we don't reward our model for just adding more variables. However, I think one of the most interesting points about $R^2$ are things that are already present in our OLS model. This is going to hold less and less as our model gets less interpretable. \n\n\n### Mean Squared Error and its flavors \n\nHeuristically it seems a lot of machine learning evaluation for regression models uses mean squared error or some variation of it to check model performance. Conceptually these metrics are linked because we can get the $R^2$ by doing:\n\n$$\nR^2 = 1 - \\frac{x \\times MSE}{\\sum^n_{i =1} (y_i - \\hat{y})^2}\n$$\n\nHowever, the thing with $R^2$ is that it is not neccessarily that sensistive to extreme values. While in general it is difficult to have wildly divergent measure of $R^2$ and MSE we may actually care about how our models perform in response to outliers. If big errors are costly to our model performance than we are going to want a measure that more directly captures their influence.\n\n$$\nMSE = \\frac{1}{n} \\sum^n_{i =1} (y_i - \\hat{y})^2\n$$\n\nWhere we are effectively getting the average error instead of the total error. One thing that flips with respect to $R^2$ is that we are getting a slightly more interpretable difference metric. Meaning that we are getting the average distance between our predicted values and the observed values. So we are looking for this number to be kind of low! However, whats important to note is that there is no scaling factor. Meaning that outliers are going to really effect this metric because we are taking the difference and then going to square it. Lets imagine a situation where I am trying to increase the distance between a friend and the ball. If I stand right next to them and throw a ball it will land closer to them then if I started from farther away and threw the ball. Since we go through this squaring process what ends up happening is that we don't have a great mental model of how to interpret this distance. What is the squared deviation of the median home value? Kind of not that intuitive because that distance is harder to put into words. To get it back on the original scale we can use the Root mean squared error which is just \n\n$$\nRMSE = \\sqrt{MSE}\n$$\n\nThis puts the MSE back on its original scale. If our model is predicting median home prices and the RMSE is 1000, this means that a typical prediction error is around 1000 dollars, with larger errors being weighted more heavily than smaller ones. \n\n\n## Bias Variance Tradeoff\n\nIn the prior, our model could be good at predicting the data we have. However, when we introduce new data, it might struggle to generalize and predict new observations accurately. This challenge arises from overfitting, where the model is really good at describing the dataset it has already seen. To mitigate this we split our data into training sets, test sets, and validation sets effectively hiding parts of our data from our model. It never has access to every single part of the data. By reducing the amount of data the model sees it can't learn every single strange data point in the model. \n\nThe reason we do this is because we want our model to predict new data but also do a good job of approximating the data generating process. These two goals are inherently conflictual. Bias represents how far away we are from the target while variance represents how far away our guesses are from each other. If we build a model that is good at predicting every single quirk of the dataset, aka reducing variance, then when we introduce new data to the model it is going to be very very brittle. If we reduce the complexity of the model making it more flexible, aka reducing bias, then we risk not being able to catch the patterns in our data.\n\nIn the bias-variance tradeoff, we aim to find a balance: a model that is simple enough to generalize well to new data but complex enough to capture the important patterns. Techniques like cross-validation, regularization, and hyperparameter tuning help us navigate this tradeoff and improve the model’s predictive performance.\n\nMathematically we can think of the bias-variance tradeoff along the lines of mean squared error where \n\n$$\nError = Bias^2 + Variance + Noise\n$$\n\nWhere we have the how far away the predictions are from the real values, how far away the predictions are from each other, and the randomness from the data generating process. We can't really doing anything about the randomness from the DGP. A useful heuristic is that we can reduce how far away our predictions are from our real values by introducing more complexity to the model. This may mean more complex functional forms of variables, introduce interactions, or use more flexible models. We can increase variance by using simple models, use bagging or boosting to leverage \"bad learners\", or use regularization to penalize our models. \n\n\n### Double Descent\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# code taken from https://www.r-bloggers.com/2021/07/double-descent-part-i-sample-wise-non-monotonicity/\n\nf <- function(x){\n  (-0.4 + 1/(x+0.5)) + (0.5*exp(x))\n}\n#The point where the prediction error is minimized\noptimum <- optimize(f, interval=c(0, 1), maximum=FALSE, tol = 1e-8)\ntemp_data <- data.frame(x = optimum$minimum, y=optimum$objective)\n\nf1 <- function(x){\n  ifelse(x<=2,   (-0.4 + 1/(x+0.5)) + (0.5*exp(x)), NA)\n}\nf2 <- function(x){\n  ifelse(x>=2, (0 + 1/(1/(0.5*exp(4/x)))), NA)\n}\n#Prediction variance function (it is piecewise so creating two of them).\nvar_f1 <- function(x){\n  ifelse(x<=2,   (0.5*exp(x)), NA)\n}\nvar_f2 <- function(x){\n  ifelse(x>=2, 1/(1/(0.5*exp(4/x))), NA)\n}\n#Prediction bias function (it is piecewise so creating two of them).\nbias_f1 <- function(x){\n  ifelse(x<=2,-0.4 + 1/(x+0.5),NA )\n}\nbias_f2 <- function(x){\n  ifelse(x>=2,0,NA )\n}\nggplot(data = temp_data, aes(x=x, y=y)) +\n  xlim(0,4) +\n  geom_function(fun = var_f1, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = var_f2, color = \"red\", size = 2, alpha = 0.7) +\n  geom_function(fun = bias_f1, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = bias_f2, color = \"blue\", size = 2, alpha = 0.7)  +\n  geom_function(fun = f1, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_function(fun = f2, color = \"forestgreen\", size = 2, alpha = 0.7) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  geom_point() + \n  theme_minimal() + ylab(\"Error\") + xlab(\"Number of Predictors/Number of observations\") +\n  theme(axis.text=element_blank(),\n        axis.ticks=element_blank()) +\n  annotate(\"text\", x=0.32, y=-0.2+1/(0.2+0.5), label= expression(paste(\"B\", ias^2)), color = \"blue\") +\n  annotate(\"text\", x=0.2, y=-0.2+0.5*exp(0.2), label= \"Variance\", color = \"red\") +\n  annotate(\"text\", x=0.26, y=0.21+(0.5*exp(0.2) + 1/(0.2+0.5)), label= expression(paste(\"Variance + B\", ias^2)), color = \"forestgreen\") +\n  annotate(\"text\", x=2.4, y=-0.2+1/(0.2+0.5), label= \"Interpolation limit\", color = \"black\") \n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-22-3.png){width=672}\n:::\n:::\n\n\n\n\nDouble descent is effectively just the idea that we don't always see degrading performance in the test set when our model is really complex. Once we get past a certain threshold our model will start to perform well again. \n\n\nEffectively at some point the model will pass through every point in our dataset in multidimensional space. Effectively what is happening is that the model will memorize the training set because we let it be super complex and all we are doing is slightly modifying how we draw the line. Thus within some region we call the interpolation threshold our model is going to do poorly within this region. Once we get past this region our model's performance will start to get better again and even outperform the 'simple' model again. \n\nWe can think of double descent as recasting our bias variance tradeoff a bit. Once we reach the interpolation threshold our model gains a somewhat crude approximation of the entirety of the data. In the bias variance framework think of bivariate linear regression. Its predictions are going to be pretty far off in some cases. A similar thing kind of happens in the interpolation region. The interpolations are crude but as we add more and more parameters the interpolations start to get better and better until the test error improves.\n\nThe reason this happens is that when we hit this interpolation region there is effectively only one model that can perfectly interpolate the data and when we add new data this will get thrown off. With to few parameters in the interpolation region we are not giving our model enough shots to be flexible enough to get this specification correct. By adding more and more parameters what we are doing is giving the model more chances to draw the line effectively.\n",
    "supporting": [
      "linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}